{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19796f52-ba37-4c76-a0ea-39dd2c9420b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchmetrics import Precision, Recall\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchmetrics import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5867c809-96e4-44b3-9427-59f1732e7cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile \n",
    "\n",
    "# loading the temp.zip and creating a zip object \n",
    "with ZipFile(\"data/electricity_consump.zip\", 'r') as zObject: \n",
    "  \n",
    "    # Extracting all the members of the zip  \n",
    "    # into a specific location. \n",
    "    zObject.extractall(path=\".\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9b36fc-4f74-433e-af1d-76b8e60120ff",
   "metadata": {},
   "source": [
    "## Generating sequences\n",
    "\n",
    "##### To be able to train neural networks on sequential data, you need to pre-process it first. You'll chunk the data into inputs-target pairs, where the inputs are some number of consecutive data points and the target is the next data point.\n",
    "\n",
    "##### Your task is to define a function to do this called create_sequences(). As inputs, it will receive data stored in a DataFrame, df and seq_length, the length of the inputs. As outputs, it should return two NumPy arrays, one with input sequences and the other one with the corresponding targets.\n",
    "\n",
    "##### As a reminder, here is how the DataFrame df looks like:\n",
    "\n",
    "<table>\n",
    "<tr><th></th>                 <th>timestamp</th>  <th>consumption</th></tr>\n",
    "<tr><td>0</td>      <td>2011-01-01 00:15:00</td>    <td>-0.704319</td></tr>\n",
    "<tr><td>...</td>    <td>                ...</td>    <td>      ...</td></tr>\n",
    "<tr><td>140255</td> <td>2015-01-01 00:00:00</td>    <td>-0.095751</td></tr>\n",
    "</table>\n",
    "\n",
    "### Instructions\n",
    "-    Iterate over the range of the number of data points minus the length of an input sequence.\n",
    "-    Define the inputs x as the slice of df from the ith row to the i + seq_lengthth row and the column at index 1.\n",
    "-    Define the target y as the slice of df at row index i + seq_length and the column at index 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f996ef5-c3b6-4d95-b951-24fdb2143bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_sequences(df, seq_length):\n",
    "    xs, ys = [], []\n",
    "    # Iterate over data indices\n",
    "    for i in range(len(df) - seq_length):\n",
    "      \t# Define inputs\n",
    "        x = df.iloc[i:(i+seq_length), 1]\n",
    "        # Define target\n",
    "        y = df.iloc[i+seq_length, 1]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "    return np.array(xs), np.array(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb9ab9a-bba7-497a-a3fc-a27e49e0aa6e",
   "metadata": {},
   "source": [
    "## Sequential Dataset\n",
    "\n",
    "##### Good job building the create_sequences() function! It's time to use it to create a training dataset for your model.\n",
    "\n",
    "##### Just like tabular and image data, sequential data is easiest passed to a model through a torch Dataset and DataLoader. To build a sequential Dataset, you will call create_sequences() to get the NumPy arrays with inputs and targets, and inspect their shape. Next, you will pass them to a TensorDataset to create a proper torch Dataset, and inspect its length.\n",
    "\n",
    "##### Your implementation of create_sequences() and a DataFrame with the training data called train_data are available.\n",
    "\n",
    "### Instructions\n",
    "-    Call create_sequences(), passing it the training DataFrame and a sequence length of 24*4, assigning the result to X_train, y_train.\n",
    "-    Define dataset_train by calling TensorDataset and passing it two arguments, the inputs and the targets created by create_sequences(), both converted from NumPy arrays to tensors of floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a514c058-e49a-40ba-8235-a12cb70d6a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"electricity_consump/electricity_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "abcb1857-5c48-48e8-862a-71db9e3e4f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105119, 96) (105119,)\n",
      "105119\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Use create_sequences to create inputs and targets\n",
    "X_train, y_train = create_sequences(train_data, 24*4)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# Create TensorDataset\n",
    "dataset_train = TensorDataset(\n",
    "    torch.from_numpy(X_train).float(),\n",
    "    torch.from_numpy(y_train).float(),\n",
    ")\n",
    "print(len(dataset_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b3be73-5807-4f88-85fa-e4ee4d050a4f",
   "metadata": {},
   "source": [
    "## Building a forecasting RNN\n",
    "\n",
    "##### It's time to build your first recurrent network! It will be a sequence-to-vector model consisting of an RNN layer with two layers and a hidden_size of 32. After the RNN layer, a simple linear layer will map the outputs to a single value to be predicted.\n",
    "\n",
    "##### The following imports have already be done for you:\n",
    "\n",
    "<code>\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "</code>\n",
    "\n",
    "### Instructions 1/4\n",
    "-    Define the RNN layer passing it the correct values for input_size, hidden_size, num_layers, and batch_first, and assign it to self.rnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "332b6981-478e-4c41-a0e7-bbd339f7253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=1,\n",
    "            hidden_size = 32,\n",
    "            num_layers = 2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(32, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ea1806-0e21-4fd1-81ca-2a01958fbb33",
   "metadata": {},
   "source": [
    "### Instructions 2/4\n",
    "-    Initialize the first hidden state h0 as a tensor of zeros of the appropriate shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea7bee73-ef6e-449f-8890-f42eb3918c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=1,\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize first hidden state with zeros\n",
    "        h0 = torch.zeros(2, x.size(0), 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecd96d5-c31f-4216-ae65-b2be1aab4551",
   "metadata": {},
   "source": [
    "### Instructions 3/4\n",
    "-    Pass the input x and the first hidden state h0 through recurrent layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c9d0771e-7ce4-4c62-9a72-5e216240b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=1,\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize first hidden state with zeros\n",
    "        h0 = torch.zeros(2, x.size(0), 32)\n",
    "        # Pass x and h0 through recurrent layer\n",
    "        out, _ = self.rnn(x, h0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed45caa9-a2e0-46d6-9727-9747ae19cb4d",
   "metadata": {},
   "source": [
    "### Instructions 4/4\n",
    "-    Pass recurrent layer's last output through the linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d9b518db-9676-4a1e-a852-cb4adebc5b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define RNN layer\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=1,\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize first hidden state with zeros\n",
    "        h0 = torch.zeros(2, x.size(0), 32)\n",
    "        # Pass x and h0 through recurrent layer\n",
    "        out, _ = self.rnn(x, h0)  \n",
    "        # Pass recurrent layer's last output through linear layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d3040f-65c0-405e-8fbf-c4d889955290",
   "metadata": {},
   "source": [
    "## LSTM network\n",
    "\n",
    "##### As you already know, plain RNN cells are not used that much in practice. A more frequently used alternative that ensures a much better handling of long sequences are Long Short-Term Memory cells, or LSTMs. In this exercise, you will be build an LSTM network yourself!\n",
    "\n",
    "##### The most important implementation difference from the RNN network you have built previously comes from the fact that LSTMs have two rather than one hidden states. This means you will need to initialize this additional hidden state and pass it to the LSTM cell.\n",
    "\n",
    "##### torch and torch.nn have already been imported for you, so start coding!\n",
    "\n",
    "### Instructions\n",
    "-    In the .\\_\\_init\\_\\_() method, define an LSTM layer and assign it to self.lstm.\n",
    "-    In the forward() method, initialize the first long-term memory hidden state c0 with zeros.\n",
    "-    In the forward() method, pass all three inputs to the LSTM layer: the current time step's inputs, and a tuple containing the two hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0840b64-c623-4d92-aac0-9db98f1837ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        # Define lstm layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1,\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(2, x.size(0), 32)\n",
    "        # Initialize long-term memory\n",
    "        c0 = torch.zeros(2, x.size(0), 32)\n",
    "        # Pass all inputs to lstm layer\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffca1a35-758c-4307-9f5f-256620b5e184",
   "metadata": {},
   "source": [
    "## GRU network\n",
    "\n",
    "##### Next to LSTMs, another popular recurrent neural network variant is the Gated Recurrent Unit, or GRU. It's appeal is in its simplicity: GRU cells require less computation than LSTM cells while often matching them in performance.\n",
    "\n",
    "##### The code you are provided with is the RNN model definition that you coded previously. Your task is to adapt it such that it produces a GRU network instead. torch and torch.nn as nn have already been imported for you.\n",
    "\n",
    "### Instructions\n",
    "-    Update the RNN model definition in order to obtain a GRU network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "638ebfd5-0053-47ce-87f6-33b0b9ea863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define RNN layer\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=1,\n",
    "            hidden_size=32,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(2, x.size(0), 32)\n",
    "        out, _ = self.gru(x, h0)  \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f048e0-d8ae-4c54-8eb2-7e5006f0b7ca",
   "metadata": {},
   "source": [
    "## RNN training loop\n",
    "\n",
    "###### It's time to train the electricity consumption forecasting model!\n",
    "\n",
    "###### You will use the LSTM network you have defined previously which is available to you as Net, as is the dataloader_train you built before. You will also need to use torch.nn which has already been imported as nn.\n",
    "\n",
    "###### In this exercise, you will train the model for only three epochs to make sure the training progresses as expected. Let's get to it!\n",
    "\n",
    "### Instructions\n",
    "-    Set up the Mean Squared Error loss and assign it to criterion.\n",
    "-    Reshape seqs to (batch size, sequence length, num features), which in our case is (16, 96, 1), and re-assign the result to seqs.\n",
    "-    Pass seqs to the model to get its outputs.\n",
    "-    Based on previously computed quantities, calculate the loss, assigning it to loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e0854d92-08af-4f2a-bf45-73a52c29d5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader based on dataset_train\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13d9250b-9d27-4acd-b931-02e58035b01e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[16, 96, 1]' is invalid for input of size 192",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m seqs, labels \u001b[38;5;129;01min\u001b[39;00m dataloader_train:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;66;03m# Reshape model inputs\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m         seqs \u001b[38;5;241m=\u001b[39m \u001b[43mseqs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;66;03m# Get model outputs\u001b[39;00m\n\u001b[1;32m     13\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m net(seqs)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[16, 96, 1]' is invalid for input of size 192"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "# Set up MSE loss\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(\n",
    "  net.parameters(), lr=0.0001\n",
    ")\n",
    "\n",
    "for epoch in range(3):\n",
    "    for seqs, labels in dataloader_train:\n",
    "        # Reshape model inputs\n",
    "        seqs = seqs.view(16, 96, 1)\n",
    "        # Get model outputs\n",
    "        outputs = net(seqs)\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff28c698-8b37-41f4-a0f8-fa3448e207f3",
   "metadata": {},
   "source": [
    "## Evaluating forecasting models\n",
    "\n",
    "###### It's evaluation time! The same LSTM network that you have trained in the previous exercise has been trained for you for a few more epochs and is available as net.\n",
    "\n",
    "###### Your task is to evaluate it on a test dataset using the Mean Squared Error metric (torchmetrics has already been imported for you). Let's see how well the model is doing!\n",
    "\n",
    "### Instructions\n",
    "-    Define the Mean Squared Error metrics and assign it to mse.\n",
    "-    Pass the input sequence to net, and squeeze the result before you assign it to outputs.\n",
    "-    Compute the final value of the test metric assigning it to test_mse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b3deaf-21ff-4cd1-acbd-7da0fe60e460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MSE metric\n",
    "mse = torchmetrics.MeanSquaredError()\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for seqs, labels in dataloader_test:\n",
    "        seqs = seqs.view(32, 96, 1)\n",
    "        # Pass seqs to net and squeeze the result\n",
    "        outputs = net(seqs).squeeze()\n",
    "        mse(outputs, labels)\n",
    "\n",
    "# Compute final metric value\n",
    "test_mse = mse.compute()\n",
    "print(f\"Test MSE: {test_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c61efa6-f535-425d-9e45-e63f164cdcf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
