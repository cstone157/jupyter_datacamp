{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcac7674-6541-4c06-931b-c677838d77b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.formula.api import ols, glm\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from patsy import dmatrix\n",
    "\n",
    "crab = pd.read_csv(\"../../data/crab.csv\")\n",
    "salary = pd.read_csv('../../data/salary.csv')\n",
    "wells = pd.read_csv(\"../../data/wells.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc9b175-0fd9-4642-a096-b0af180bc40d",
   "metadata": {},
   "source": [
    "## Fit a multivariable logistic regression\n",
    "### Using the knowledge gained in the video you will revisit the crab dataset to fit a multivariate logistic regression model. In chapter 2 you have fitted a logistic regression with width as explanatory variable. In this exercise you will analyze the effects of adding color as additional variable.\n",
    "\n",
    "### The color variable has a natural ordering from medium light, medium, medium dark and dark. As such color is an ordinal variable which in this example you will treat as a quantitative variable.\n",
    "\n",
    "### The crab dataset is preloaded in the workspace. Also note that the only difference in the code from the univariate case is in the formula argument, where now you will add structure to incorporate the new variable.\n",
    "\n",
    "### Instructions\n",
    "-    Import necessary functions from statsmodels library for GLMs.\n",
    "-    Define formula argument where width and color are explanatory variables and y is the response.\n",
    "-    Fit a multivariate logistic regression model using glm() function.\n",
    "-    Print model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d23694f2-fecc-43eb-af4c-cb6d0fa80b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  173\n",
      "Model:                            GLM   Df Residuals:                      170\n",
      "Model Family:                Binomial   Df Model:                            2\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -94.561\n",
      "Date:                Sat, 16 Mar 2024   Deviance:                       189.12\n",
      "Time:                        03:49:24   Pearson chi2:                     170.\n",
      "No. Iterations:                     5   Pseudo R-squ. (CS):             0.1909\n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept    -10.0708      2.807     -3.588      0.000     -15.572      -4.569\n",
      "width          0.4583      0.104      4.406      0.000       0.254       0.662\n",
      "color         -0.5090      0.224     -2.276      0.023      -0.947      -0.071\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Import statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm\n",
    "\n",
    "# Define model formula\n",
    "formula = 'y ~ width + color'\n",
    "\n",
    "# Fit GLM\n",
    "model = glm(formula, data = crab, family = sm.families.Binomial()).fit()\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5fd358-ea1c-4c6f-a8c7-e1e230c5ff99",
   "metadata": {},
   "source": [
    "## The effect of multicollinearity\n",
    "### Using the crab dataset you will analyze the effects of multicollinearity. Recall that multicollinearity can have the following effects:\n",
    "\n",
    "### Coefficient is not significant, but variable is highly correlated with \"y\".\n",
    "-    Adding/removing a variable significantly changes coefficients.\n",
    "-    Not logical sign of the coefficient.\n",
    "-    Variables have high pairwise correlation.\n",
    "\n",
    "### Instructions\n",
    "-    Import necessary functions from statsmodels library for GLMs.\n",
    "-    Fit a multivariate logistic regression model with weight and width as explanatory variables and y as the response.\n",
    "-    View model results using print() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47ce71bd-5bf2-460f-873c-1d6ebccbe86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  173\n",
      "Model:                            GLM   Df Residuals:                      170\n",
      "Model Family:                Binomial   Df Model:                            2\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -96.446\n",
      "Date:                Sat, 16 Mar 2024   Deviance:                       192.89\n",
      "Time:                        03:51:19   Pearson chi2:                     167.\n",
      "No. Iterations:                     5   Pseudo R-squ. (CS):             0.1730\n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -9.3547      3.528     -2.652      0.008     -16.270      -2.440\n",
      "weight         0.8338      0.672      1.241      0.214      -0.483       2.150\n",
      "width          0.3068      0.182      1.686      0.092      -0.050       0.663\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Import statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm\n",
    "\n",
    "# Define model formula\n",
    "formula = 'y ~ weight + width'\n",
    "\n",
    "# Fit GLM\n",
    "model = glm(formula, data = crab, family = sm.families.Binomial()).fit()\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3673ff1-6ad4-4e46-879b-c8972d85038d",
   "metadata": {},
   "source": [
    "## Compute VIF\n",
    "### As you learned in the video one of the most widely used diagnostic for multicollinearity is the variance inflation factor or VIF, which is computed for each explanatory variable.\n",
    "\n",
    "### Recall from the video that the rule of thumb threshold is VIF at the level of 2.5, meaning if the VIF is above 2.5 you should consider there is effect of multicollinearity on your fitted model.\n",
    "\n",
    "### The previously fitted model and crab dataset are preloaded in the workspace.\n",
    "\n",
    "### Instructions\n",
    "-    From statsmodels import variance_inflation_factor.\n",
    "-    From crab dataset choose weight, width and color and save as X. Add Intercept column of ones to X.\n",
    "-    Using pandas function DataFrame() create an empty vif dataframe and add column names of X in column Variables.\n",
    "-    For each variable compute VIF using the variance_inflation_factor()function and save in vif dataframe with VIF column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "866ff20b-fcd2-435a-84bb-82388224d2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   variables         VIF\n",
      "0     weight    4.691018\n",
      "1      width    4.726378\n",
      "2      color    1.076594\n",
      "3  Intercept  414.163343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_364806/25132915.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[\"Intercept\"] = 1\n"
     ]
    }
   ],
   "source": [
    "# Import functions\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Get variables for which to compute VIF and add intercept term\n",
    "X = crab[[\"weight\", \"width\", \"color\"]]\n",
    "X[\"Intercept\"] = 1\n",
    "\n",
    "# Compute and view VIF\n",
    "vif = pd.DataFrame()\n",
    "vif[\"variables\"] = X.columns\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "\n",
    "# View results using print\n",
    "print(vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252a9c65-62ef-4ead-803a-ddf016ee3575",
   "metadata": {},
   "source": [
    "## Checking model fit\n",
    "### In the video you analyzed the example of an improvement in the model fit by adding additional variable on the wells data. Continuing with this data set you will see how further increase in model complexity effects deviance and model fit.\n",
    "\n",
    "### The dataset wells have been preloaded in the workspace.\n",
    "\n",
    "### Instructions\n",
    "-    Fit a logistic regression model with switch as the response and distance100 and arsenic as explanatory variables.\n",
    "-    Compute the difference in deviance of the intercept only model and the model including all the variables.\n",
    "-    Print the computed difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9511107d-5d7a-4eb4-a422-791bc7e6788d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188.76305963384948\n"
     ]
    }
   ],
   "source": [
    "# Import statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm\n",
    "\n",
    "# Define model formula\n",
    "formula = 'switch ~ distance100 + arsenic'\n",
    "\n",
    "# Fit GLM\n",
    "model_dist_ars = glm(formula, data = wells, family = sm.families.Binomial()).fit()\n",
    "\n",
    "# Compare deviance of null and residual model\n",
    "diff_deviance = model_dist_ars.null_deviance - model_dist_ars.deviance\n",
    "\n",
    "# Print the computed difference in deviance\n",
    "print(diff_deviance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a059c0ec-7d6f-4160-a230-d05b3b62cce1",
   "metadata": {},
   "source": [
    "## Compare two models\n",
    "### From previous exercise you have fitted a model with distance100 and arsenic as explanatory variables. In this exercise you will analyze the impact on the model fit for each of the added variables.\n",
    "\n",
    "### Recall that the models you fitted are as follows and have been preloaded in the workspace:\n",
    "-    1.) model_dist = 'switch ~ distance100'\n",
    "-    2.) model_dist_ars = 'switch ~ distance100 + arsenic\n",
    "\n",
    "### The dataset wells has also been preloaded in the workspace.\n",
    "\n",
    "### Instructions\n",
    "-    Compute the difference in deviance when distance100 is added to the null model.\n",
    "-    Compute the difference in deviance when arsenic is added to the model with distance100 variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e02d189f-758f-480e-a367-29a35efac74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dist = glm('switch ~ distance100', data = wells, family = sm.families.Binomial()).fit()\n",
    "model_dist_ars = glm('switch ~ distance100 + arsenic', data = wells, family = sm.families.Binomial()).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "834e7309-5f03-4ca4-a12a-0c3bfad71e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding distance100 to the null model reduces deviance by:  42.726\n",
      "Adding arsenic to the distance model reduced deviance further by:  146.037\n"
     ]
    }
   ],
   "source": [
    "# Compute the difference in adding distance100 variable\n",
    "diff_deviance_distance = model_dist.null_deviance - model_dist.deviance\n",
    "\n",
    "# Print the computed difference in deviance\n",
    "print('Adding distance100 to the null model reduces deviance by: ', \n",
    "      round(diff_deviance_distance,3))\n",
    "\n",
    "# Compute the difference in adding arsenic variable\n",
    "diff_deviance_arsenic = model_dist.deviance - model_dist_ars.deviance\n",
    "\n",
    "# Print the computed difference in deviance\n",
    "print('Adding arsenic to the distance model reduced deviance further by: ', \n",
    "      round(diff_deviance_arsenic,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829231ba-48ea-429c-8b81-1902707b76b4",
   "metadata": {},
   "source": [
    "## Deviance and linear transformation\n",
    "### As you have seen in previous exercises the deviance decreased as you added a variable that improves the model fit. In this exercise you will consider the well switch data example and the model you fitted with distance variable, but you will assess what happens when there is a linear transformation of the variable.\n",
    "\n",
    "### Note that the variable distance100 is the original variable distance divided by 100 to make for more meaningful representation and interpretation of the results. You can inspect the data with wells.head() to view the first 5 rows of data.\n",
    "\n",
    "### The wells dataset and the model 'swicth ~ distance100' has been preloaded as model_dist.\n",
    "\n",
    "### Instructions\n",
    "-    Import statsmodels as sm and the glm() function.\n",
    "-    Fit a logistic regression model with distance as the explanatory variable and switch as the response and save as model_dist_1.\n",
    "-    Check and print the difference in deviance of the current model and the model with distance100 as the explanatory variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eab685-5175-429e-865d-c5cbd53a4304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm\n",
    "\n",
    "# Fit logistic regression model as save as model_dist_1\n",
    "model_dist_1 = glm('switch ~ distance', data = wells, family = sm.families.Binomial()).fit()\n",
    "\n",
    "# Check the difference in deviance of model_dist_1 and model_dist\n",
    "print('Difference in deviance is: ', round(model_dist_1.deviance - model_dist.deviance,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19b6a97-a307-4d65-82d8-642106fc0862",
   "metadata": {},
   "source": [
    "## Model matrix for continuous variables\n",
    "### In the video you learned about the model formula, the under-the-hood workings of the dmatrix() to obtain the model matrix and how it relates to the glm() function. As you have learned the input to dmatrix() is the right hand side of the glm() formula argument. In case the variables are part of the dataframe, then you should also specify the data source via the data argument.  <code>dmatrix('y ~ x1 + x2',  data = my_data)</code>\n",
    "\n",
    "### In this exercise you will analyze and confirm the structure of your model before model fit.\n",
    "\n",
    "### The dataset wells has been preloaded in the workspace.\n",
    "\n",
    "### Instructions 1/2\n",
    "-    Import dmatrix from patsy, use it to construct the model_matrix with 'arsenic', and print the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "174b64fb-54be-4b19-8a79-3d5f68a45f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Intercept  arsenic\n",
      "0        1.0     2.36\n",
      "1        1.0     0.71\n",
      "2        1.0     2.07\n",
      "3        1.0     1.15\n",
      "4        1.0     1.10\n"
     ]
    }
   ],
   "source": [
    "# Import function dmatrix()\n",
    "from patsy import dmatrix\n",
    "\n",
    "# Construct model matrix with arsenic\n",
    "model_matrix = dmatrix('arsenic', data = wells, return_type = 'dataframe')\n",
    "print(model_matrix.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f4d74c-2544-49be-a784-c8e4cb9a3acf",
   "metadata": {},
   "source": [
    "-    Import dmatrix from patsy, construct the model_matrix with 'arsenic' and 'distance100', and print the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32f3133c-4204-435b-8590-77cb27b9083c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Intercept  arsenic  distance100\n",
      "0        1.0     2.36      0.16826\n",
      "1        1.0     0.71      0.47322\n",
      "2        1.0     2.07      0.20967\n",
      "3        1.0     1.15      0.21486\n",
      "4        1.0     1.10      0.40874\n"
     ]
    }
   ],
   "source": [
    "# Import function dmatrix()\n",
    "from patsy import dmatrix\n",
    "\n",
    "# Construct model matrix with arsenic and distance100\n",
    "model_matrix = dmatrix('arsenic + distance100', data = wells, return_type = 'dataframe')\n",
    "print(model_matrix.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75843707-d8df-4139-a900-808597d3518b",
   "metadata": {},
   "source": [
    "## Variable transformation\n",
    "### Continuing with the wells you will practice applying variable transformation directly in the formula and model matrix setting without the need to add the transformed data to the data frame first. You will also revisit the computation of model error or deviance to see if the transformation improved the model fit.\n",
    "\n",
    "### Recall the structure of dmatrix() function is the right hand side of the glm() formula argument in addition to the data argument.  <code>dmatrix('y ~ x1 + x2', data = my_data)</code>\n",
    "\n",
    "### The dataset wells and the model model_ars with arsenic (original variable) have been preloaded in the workspace.\n",
    "\n",
    "### Instructions 1/3\n",
    "-    Import numpy as np, and dmatrix from patsy.\n",
    "-    Construct a model matrix by applying the logarithm transformation on arsenic using numpy log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36970c27-bf3e-46d3-94f2-6b4f755ea1db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "      <th>np.log(arsenic)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.858662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.342490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.727549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.139762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.095310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Intercept  np.log(arsenic)\n",
       "0        1.0         0.858662\n",
       "1        1.0        -0.342490\n",
       "2        1.0         0.727549\n",
       "3        1.0         0.139762\n",
       "4        1.0         0.095310"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import function dmatrix\n",
    "import numpy as np\n",
    "from patsy import dmatrix\n",
    "\n",
    "# Construct model matrix for arsenic with log transformation\n",
    "dmatrix('np.log(arsenic)', data = wells,\n",
    "       return_type = 'dataframe').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d576b6d6-c68e-4988-8073-ed0ff4208557",
   "metadata": {},
   "source": [
    "-    Import statsmodels.api as sm, glmfrom statsmodels.formula.api and numpyas np.\n",
    "-    Fit and print summary of a glm model with the response switch and log transformed arsenic explanatory variable defining the transformation in the formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f881fa6-8299-45c7-aaec-b2a684b7c744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                 switch   No. Observations:                 3010\n",
      "Model:                            GLM   Df Residuals:                     3008\n",
      "Model Family:                Binomial   Df Model:                            1\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -1987.6\n",
      "Date:                Sun, 17 Mar 2024   Deviance:                       3975.3\n",
      "Time:                        03:55:17   Pearson chi2:                 3.01e+03\n",
      "No. Iterations:                     4   Pseudo R-squ. (CS):            0.04187\n",
      "Covariance Type:            nonrobust                                         \n",
      "===================================================================================\n",
      "                      coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "Intercept           0.0966      0.041      2.331      0.020       0.015       0.178\n",
      "np.log(arsenic)     0.7089      0.064     11.046      0.000       0.583       0.835\n",
      "===================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Import statsmodels\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm\n",
    "import numpy as np\n",
    "\n",
    "# Define model formula\n",
    "formula = 'switch ~ np.log(arsenic)'\n",
    "\n",
    "# Fit GLM\n",
    "model_log_ars = glm(formula, data = wells, \n",
    "                     family = sm.families.Binomial()).fit()\n",
    "\n",
    "# Print model summary\n",
    "print(model_log_ars.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0916eaae-6c0a-4bd8-a958-fd55d2efb063",
   "metadata": {},
   "source": [
    "## Coding categorical variables\n",
    "### In previous exercises you practiced creating model matrices for continuous variables and applying variable transformation. During this exercise you will practice the ways of coding a categorical variable.\n",
    "\n",
    "### Categorical data provide a way to analyze and compare relationships given different groups or factors. Hence, choosing a reference group is important and often, depending on the study at hand, you might want to change the reference group, from the default one. One frequently used reason for changing the reference group is that the interpretation of coefficient estimates is more applicable and interesting given the study.\n",
    "\n",
    "### For this exercise you will revisit the crab dataset where colorand spine are categorical variables.\n",
    "\n",
    "### The dataset crab is preloaded in the workspace.\n",
    "\n",
    "### Instructions 1/2\n",
    "-    Import dmatrix from patsy.\n",
    "-    Using dmatrix() construct and print a model matrix with color as categorical variable using C()function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51108f96-3ad5-4f2d-b0c0-51b8512e7899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Intercept  C(color)[T.2]  C(color)[T.3]  C(color)[T.4]\n",
      "0        1.0            1.0            0.0            0.0\n",
      "1        1.0            0.0            1.0            0.0\n",
      "2        1.0            0.0            0.0            0.0\n",
      "3        1.0            0.0            1.0            0.0\n",
      "4        1.0            0.0            1.0            0.0\n"
     ]
    }
   ],
   "source": [
    "# Import function dmatrix\n",
    "from patsy import dmatrix\n",
    "\n",
    "# Construct and print model matrix for color as categorical variable\n",
    "print(dmatrix('C(color)', data = crab,\n",
    "     \t   return_type = 'dataframe').head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26da8959-b078-4951-b720-1bd8fd5a7917",
   "metadata": {},
   "source": [
    "-    Change the reference group to be the medium dark, i.e. group number 3 using the Treatment argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9c93af7-09f2-4100-bc3b-75722208a2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Intercept  C(color, Treatment(3))[T.1]  C(color, Treatment(3))[T.2]  \\\n",
      "0        1.0                          0.0                          1.0   \n",
      "1        1.0                          0.0                          0.0   \n",
      "2        1.0                          1.0                          0.0   \n",
      "3        1.0                          0.0                          0.0   \n",
      "4        1.0                          0.0                          0.0   \n",
      "\n",
      "   C(color, Treatment(3))[T.4]  \n",
      "0                          0.0  \n",
      "1                          0.0  \n",
      "2                          0.0  \n",
      "3                          0.0  \n",
      "4                          0.0  \n"
     ]
    }
   ],
   "source": [
    "# Import function dmatrix\n",
    "from patsy import dmatrix\n",
    "\n",
    "# Construct and print the model matrix for color with reference group 3\n",
    "print(dmatrix('C(color, Treatment(3))', \n",
    "     \t  data = crab,\n",
    "     \t  return_type = 'dataframe').head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b1b64c-0b91-4dd0-8fad-146b78cc0d4a",
   "metadata": {},
   "source": [
    "## Modeling with categorical variable\n",
    "### In previous exercises you have fitted a logistic regression model with color as explanatory variable along with width where you treated the color as quantitative variable. In this exercise you will treat color as a categorical variable which when you construct the model matrix will encode the color into 3 variables with 0/1 encoding.\n",
    "\n",
    "### Recall that the default encoding in dmatrix() uses the first group as a reference group. To view model matrix as a dataframe an additional argument in dmatrix(), namely, return_type will be set to 'dataframe'.\n",
    "\n",
    "### The color variable has a natural ordering as follows:\n",
    "-    1: medium light\n",
    "-    2: medium\n",
    "-    3: medium dark\n",
    "-    4: dark\n",
    "\n",
    "### The crab dataset is preloaded in the workspace.\n",
    "\n",
    "### Instructions 1/4\n",
    "-    Construct a model_matrix with color as a variable. color should be treated as categorical and the reference group set to 4 using Treatment() function.\n",
    "-    Fit and print the results of a logistic model with y as the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55b95aea-debe-4dce-8404-481957098b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Intercept  C(color, Treatment(4))[T.1]  C(color, Treatment(4))[T.2]  \\\n",
      "0        1.0                          0.0                          1.0   \n",
      "1        1.0                          0.0                          0.0   \n",
      "2        1.0                          1.0                          0.0   \n",
      "3        1.0                          0.0                          0.0   \n",
      "4        1.0                          0.0                          0.0   \n",
      "\n",
      "   C(color, Treatment(4))[T.3]  \n",
      "0                          0.0  \n",
      "1                          1.0  \n",
      "2                          0.0  \n",
      "3                          1.0  \n",
      "4                          1.0  \n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  173\n",
      "Model:                            GLM   Df Residuals:                      169\n",
      "Model Family:                Binomial   Df Model:                            3\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -106.03\n",
      "Date:                Sun, 17 Mar 2024   Deviance:                       212.06\n",
      "Time:                        04:19:19   Pearson chi2:                     173.\n",
      "No. Iterations:                     4   Pseudo R-squ. (CS):            0.07612\n",
      "Covariance Type:            nonrobust                                         \n",
      "===============================================================================================\n",
      "                                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Intercept                      -0.7621      0.458     -1.665      0.096      -1.659       0.135\n",
      "C(color, Treatment(4))[T.1]     1.8608      0.809      2.301      0.021       0.276       3.446\n",
      "C(color, Treatment(4))[T.2]     1.7382      0.512      3.393      0.001       0.734       2.742\n",
      "C(color, Treatment(4))[T.3]     1.1299      0.551      2.051      0.040       0.050       2.210\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Construct model matrix\n",
    "model_matrix = dmatrix('C(color, Treatment(4))' , data = crab, \n",
    "                       return_type = 'dataframe')\n",
    "\n",
    "# Print first 5 rows of model matrix dataframe\n",
    "print(model_matrix.head())\n",
    "\n",
    "# Fit and print the results of a glm model with the above model matrix configuration\n",
    "model = glm('y ~ C(color, Treatment(4))', data = crab, \n",
    "            family = sm.families.Binomial()).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dd90f6-b5e6-476b-af58-201861783457",
   "metadata": {},
   "source": [
    "-    To the previous model matrix and logistic regression model add width as additional explanatory variable. View model matrix before fitting the model and then view model results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1e0f65b-25c0-47e0-a75d-a4c884df85be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Intercept  C(color, Treatment(4))[T.1]  C(color, Treatment(4))[T.2]  \\\n",
      "0        1.0                          0.0                          1.0   \n",
      "1        1.0                          0.0                          0.0   \n",
      "2        1.0                          1.0                          0.0   \n",
      "3        1.0                          0.0                          0.0   \n",
      "4        1.0                          0.0                          0.0   \n",
      "\n",
      "   C(color, Treatment(4))[T.3]  width  \n",
      "0                          0.0   28.3  \n",
      "1                          1.0   22.5  \n",
      "2                          0.0   26.0  \n",
      "3                          1.0   24.8  \n",
      "4                          1.0   26.0  \n",
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  173\n",
      "Model:                            GLM   Df Residuals:                      168\n",
      "Model Family:                Binomial   Df Model:                            4\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -93.729\n",
      "Date:                Sun, 17 Mar 2024   Deviance:                       187.46\n",
      "Time:                        04:21:21   Pearson chi2:                     169.\n",
      "No. Iterations:                     5   Pseudo R-squ. (CS):             0.1986\n",
      "Covariance Type:            nonrobust                                         \n",
      "===============================================================================================\n",
      "                                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Intercept                     -12.7151      2.762     -4.604      0.000     -18.128      -7.302\n",
      "C(color, Treatment(4))[T.1]     1.3299      0.853      1.560      0.119      -0.341       3.001\n",
      "C(color, Treatment(4))[T.2]     1.4023      0.548      2.557      0.011       0.327       2.477\n",
      "C(color, Treatment(4))[T.3]     1.1061      0.592      1.868      0.062      -0.054       2.267\n",
      "width                           0.4680      0.106      4.434      0.000       0.261       0.675\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Construct model matrix\n",
    "model_matrix = dmatrix('C(color, Treatment(4)) + width' , data = crab, \n",
    "                       return_type = 'dataframe')\n",
    "\n",
    "# Print first 5 rows of model matrix\n",
    "print(model_matrix.head())\n",
    "\n",
    "# Fit and print the results of a glm model with the above model matrix configuration\n",
    "model = glm('y ~ C(color, Treatment(4)) + width', data = crab, \n",
    "            family = sm.families.Binomial()).fit()\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa56e989-7cf2-425d-94f9-a352aa1967e7",
   "metadata": {},
   "source": [
    "## Interaction terms\n",
    "### In the video you learned how to include interactions in the model structure when there is one continuous and one categorical variable. In this exercise you will analyze the effects of interaction between two continuous variables.\n",
    "\n",
    "### You will use centered variables instead of original values to be able to interpret the coefficient effects more easily, i.e. from the level of the mean values rather than 0 which may not be logical for the study at hand. In other words we don't want to interpret the model by assuming 0 for arsenic or distance100 variables.\n",
    "\n",
    "### The model 'switch ~ distance100 + arsenic' has been preloaded as model_dist_ars in the workspace.\n",
    "### Also wells dataset is preloaded.\n",
    "\n",
    "### Instructions 1/2\n",
    "-    Fit a logistic model with switch as the response and centered distance100,arsenic and the interaction term between distance100 and arsenic as explanatory variables. Use center() to center the variables.\n",
    "Print the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "094f507e-cd66-428c-a896-f2fd5cbf853d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                 switch   No. Observations:                 3010\n",
      "Model:                            GLM   Df Residuals:                     3006\n",
      "Model Family:                Binomial   Df Model:                            3\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -1956.2\n",
      "Date:                Sun, 17 Mar 2024   Deviance:                       3912.4\n",
      "Time:                        04:23:43   Pearson chi2:                 3.08e+03\n",
      "No. Iterations:                     4   Pseudo R-squ. (CS):            0.06168\n",
      "Covariance Type:            nonrobust                                         \n",
      "=======================================================================================================\n",
      "                                          coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "Intercept                               0.3521      0.040      8.813      0.000       0.274       0.430\n",
      "center(distance100)                    -0.8834      0.105     -8.410      0.000      -1.089      -0.678\n",
      "center(arsenic)                         0.4713      0.042     11.168      0.000       0.389       0.554\n",
      "center(distance100):center(arsenic)    -0.1744      0.103     -1.700      0.089      -0.376       0.027\n",
      "=======================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import glm\n",
    "\n",
    "# Fit GLM and print model summary\n",
    "model_int = glm('switch ~ center(distance100) + center(arsenic) + center(distance100):center(arsenic)', \n",
    "                data = wells, family = sm.families.Binomial()).fit()\n",
    "\n",
    "# View model results\n",
    "print(model_int.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8175041c-d0c9-43ef-92d9-8b50bc61354b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
