{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4427413-7bfb-4a70-9c9f-ff5399492aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907b5cd2-87ec-4e6e-be3a-4c65be272de4",
   "metadata": {},
   "source": [
    "## Implementing ReLU\n",
    "\n",
    "#### The rectified linear unit (or ReLU) function is one of the most common activation functions in deep learning.\n",
    "\n",
    "#### It overcomes the training problems linked with the sigmoid function you learned, such as the vanishing gradients problem.\n",
    "\n",
    "#### In this exercise, you'll begin with a ReLU implementation in PyTorch. Next, you'll calculate the gradients of the function.\n",
    "\n",
    "#### The nn module has already been imported for you.\n",
    "\n",
    "## Instructions 1/2\n",
    "-    Create a ReLU function in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e7c0a8f-6bf1-43ec-b0c3-984cea8f5a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ReLU function with PyTorch\n",
    "relu_pytorch = nn.ReLU()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55ddb77-884e-4731-9e91-d5df3f305cd9",
   "metadata": {},
   "source": [
    "## Instructions 2/2\n",
    "-    Calculate the gradient of the ReLU function for x using the relu_pytorch() function you defined, then running a backward pass\n",
    "-    Find the gradient at x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0918fa8a-9a1b-49f3-9e39-04e8de633e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ReLU function with PyTorch\n",
    "relu_pytorch = nn.ReLU()\n",
    "\n",
    "# Apply your ReLU function on x, and calculate gradients\n",
    "x = torch.tensor(-1.0, requires_grad=True)\n",
    "y = relu_pytorch(x)\n",
    "y.backward()\n",
    "\n",
    "# Print the gradient of the ReLU function for x\n",
    "gradient = x.grad\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f21111d-90f3-4d98-80f0-175289f1d143",
   "metadata": {},
   "source": [
    "## Implementing leaky ReLU\n",
    "\n",
    "#### You've learned that ReLU is one of the most used activation functions in deep learning. You will find it in modern architecture. However, it does have the inconvenience of outputting null values for negative inputs and therefore, having null gradients. Once an element of the input is negative, it will be set to zero for the rest of the training. Leaky ReLU overcomes this challenge by using a multiplying factor for negative inputs.\n",
    "\n",
    "#### In this exercise, you will implement the leaky ReLU function in NumPy and PyTorch and practice using it. The numpy as np package, the torch package as well as the torch.nn as nn have already been imported.\n",
    "\n",
    "## Instructions 1/2\n",
    "-    Create a leaky ReLU function in PyTorch with a negative slope of 0.05.\n",
    "-    Call the function on the tensor x, which has already been defined for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcde2398-3bd6-40f1-9092-afdceb4a2940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.1000)\n"
     ]
    }
   ],
   "source": [
    "# Create a leaky relu function in PyTorch\n",
    "leaky_relu_pytorch = nn.LeakyReLU(negative_slope = 0.05)\n",
    "\n",
    "x = torch.tensor(-2.0)\n",
    "# Call the above function on the tensor x\n",
    "output = leaky_relu_pytorch(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df78faf9-0562-4467-84fb-330e398e1f9d",
   "metadata": {},
   "source": [
    "## Counting the number of parameters\n",
    "\n",
    "#### Deep learning models are famous for having a lot of parameters. Recent language models have billions of parameters. With more parameters comes more computational complexity and longer training times, and a deep learning practitioner must know how many parameters their model has.\n",
    "\n",
    "#### In this exercise, you will calculate the number of parameters in your model, first using PyTorch then manually.\n",
    "\n",
    "#### The torch.nn package has been imported as nn.\n",
    "\n",
    "## Instructions 1/2\n",
    "-    Iterate through the model's parameters to update the total variable with the total number of parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99347d11-e6fc-452f-a361-f656184a107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(16, 4),\n",
    "                      nn.Linear(4, 2),\n",
    "                      nn.Linear(2, 1))\n",
    "\n",
    "total = 0\n",
    "\n",
    "# Calculate the number of parameters in the model\n",
    "for parameter in model.parameters():\n",
    "  total += parameter.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21c6b07-eb89-42ac-b62b-7491b4e339a6",
   "metadata": {},
   "source": [
    "## Manipulating the capacity of a network\n",
    "\n",
    "#### In this exercise, you will practice creating neural networks with different capacities. The capacity of a network reflects the number of parameters in said network. To help you, a calculate_capacity() function has been implemented, as follows:\n",
    "\n",
    "<code>\n",
    "def calculate_capacity(model):\n",
    "  total = 0\n",
    "  for p in model.parameters():\n",
    "    total += p.numel()\n",
    "  return total\n",
    "</code>\n",
    "\n",
    "#### This function returns the number of parameters in the your model.\n",
    "\n",
    "#### The dataset you are training this network on has n_features features and n_classes classes. The torch.nn package has been imported as nn.\n",
    "\n",
    "## Instructions 1/2\n",
    "-    Create a neural network with exactly three linear layers and less than 120 parameters, which takes n_features as inputs and outputs n_classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f94cdd9f-8c0f-429f-b436-eae9bd1e9f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_capacity(model):\n",
    "  total = 0\n",
    "  for p in model.parameters():\n",
    "    total += p.numel()\n",
    "  return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42c990c7-27d3-4ffa-b97b-cb999ae786a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n"
     ]
    }
   ],
   "source": [
    "n_features = 8\n",
    "n_classes = 2\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Create a neural network with less than 120 parameters\n",
    "model = nn.Sequential(nn.Linear(n_features, 8),\n",
    "                      nn.Linear(8, 4),\n",
    "                      nn.Linear(4, n_classes))\n",
    "output = model(input_tensor)\n",
    "\n",
    "print(calculate_capacity(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db12531-ef0b-4b4c-9827-55d89c0e737c",
   "metadata": {},
   "source": [
    "## Instructions 2/2\n",
    "-    Create a neural network with exactly four linear layers and more than 120 parameters, which takes n_features as inputs and outputs n_classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39aaaa05-255b-422c-8d56-d2ccbf078422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n"
     ]
    }
   ],
   "source": [
    "n_features = 8\n",
    "n_classes = 2\n",
    "\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Create a neural network with more than 120 parameters\n",
    "model = nn.Sequential(nn.Linear(n_features, 8),\n",
    "                      nn.Linear(8, 4),\n",
    "                      nn.Linear(4, 4),\n",
    "                      nn.Linear(4, n_classes))\n",
    "\n",
    "output = model(input_tensor)\n",
    "\n",
    "print(calculate_capacity(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6dfde9-a279-45bc-b9d0-9f064a7b9152",
   "metadata": {},
   "source": [
    "## # Try a first learning rate value\n",
    "lr0 = .01\n",
    "optimize_and_plot(lr=lr0)Experimenting with learning rate\n",
    "\n",
    "#### In this exercise, your goal is to find the optimal learning rate such that the optimizer can find the minimum of the non-convex function x^4 + x^3 - 5*x^2 in ten steps.\n",
    "\n",
    "#### You will experiment with three different learning rate values. For this problem, try learning rate values between 0.001 to 0.1.\n",
    "\n",
    "#### You are provided with the optimize_and_plot() function that takes the learning rate for the first argument. This function will run 10 steps of the SGD optimizer and display the results.\n",
    "\n",
    "## Instructions 1/3\n",
    "-        Try a small learning rate value such that the optimizer isn't able to get past the first minimum on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62487193-8980-49b3-95e9-2ae9023b1b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_and_plot(lr=0.01, momentum=0.0):\n",
    "  x = torch.tensor(2.0, requires_grad=True)\n",
    "  buffer = torch.zeros_like(x.data)\n",
    "  values = []\n",
    "  for i in range(10):\n",
    "\n",
    "      y = function(x)\n",
    "      values.append((x.clone(), y.clone()))\n",
    "      y.backward()\n",
    "\n",
    "      d_p = x.grad.data\n",
    "      if momentum !=0 :\n",
    "          buffer.mul_(momentum).add_(d_p)\n",
    "          d_p = buffer\n",
    "\n",
    "      x.data.add_(d_p, alpha=-lr)\n",
    "      x.grad.zero_()\n",
    "      \n",
    "  x = np.arange(-3, 2, 0.001)\n",
    "  y = function(x)\n",
    "\n",
    "  plt.figure(figsize=(10, 5))\n",
    "  plt.plot([v[0].detach().numpy() for v in values], [v[1].detach().numpy() for v in values], 'r-X', \n",
    "           linewidth=2, markersize=7)\n",
    "  for i in range(10):\n",
    "      plt.text(values[i][0]+0.1, values[i][1], f'step {i}', fontdict={'color': 'r'})\n",
    "  plt.plot(x, y, linewidth=2)\n",
    "  plt.grid()\n",
    "  plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "  plt.legend(['Optimizer steps', 'Square function'])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d59b968-84ca-46f8-9a6a-119ce3132fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a first learning rate value\n",
    "lr0 = .01\n",
    "optimize_and_plot(lr=lr0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30e6bd5-afd0-41c4-856f-7d03d4f88eaf",
   "metadata": {},
   "source": [
    "## Instructions 2/3\n",
    "-        Try a large learning rate value such that the optimizer skips past the global minimum at -2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d696b5e-c48e-4ac9-bdf8-4fd3fa28f7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a second learning rate value\n",
    "lr1 = .4\n",
    "optimize_and_plot(lr=lr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd9e869-34cb-4492-8a18-babef5604a9c",
   "metadata": {},
   "source": [
    "## Instructions 3/3\n",
    "-        Based on the previous results, try a better learning rate value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c456db-a810-4590-9b6c-41a9b8423ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a third learning rate value\n",
    "lr2 = .083\n",
    "optimize_and_plot(lr=lr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf4b576-f962-443b-953b-6fd23be2dc0a",
   "metadata": {},
   "source": [
    "## Experimenting with momentum\n",
    "\n",
    "#### In this exercise, your goal is to find the optimal momentum such that the optimizer can find the minimum of the following non-convex function x^4 + x^3 - 5*x^2 in 20 steps. You will experiment with two different momentum values. For this problem, the learning rate is fixed at 0.01.\n",
    "\n",
    "#### You are provided with the optimize_and_plot() function that takes the learning rate for the first argument. This function will run 20 steps of the SGD optimizer and display the results.\n",
    "\n",
    "## Instructions 1/2\n",
    "-        Try a first value for the momentum such that the optimizer gets stuck in the first minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec026248-fe2c-4221-93ba-4ed77092ef96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_and_plot(lr=0.01, momentum=0.0):\n",
    "  if lr > 0.05:\n",
    "    raise ValueError('Choose a learning <= 0.05')\n",
    "  x = torch.tensor(2.0, requires_grad=True)\n",
    "  buffer = torch.zeros_like(x.data)\n",
    "  values = []\n",
    "  for i in range(20):\n",
    "\n",
    "      y = function(x)\n",
    "      values.append((x.clone(), y.clone()))\n",
    "      y.backward()\n",
    "\n",
    "      d_p = x.grad.data\n",
    "      if momentum !=0 :\n",
    "          buffer.mul_(momentum).add_(d_p)\n",
    "          d_p = buffer\n",
    "\n",
    "      x.data.add_(d_p, alpha=-lr)\n",
    "      x.grad.zero_()\n",
    "      \n",
    "  x = np.arange(-3, 2, 0.001)\n",
    "  y = function(x)\n",
    "\n",
    "  plt.figure(figsize=(10, 5))\n",
    "  plt.plot([v[0].detach().numpy() for v in values], [v[1].detach().numpy() for v in values], 'r-X', \n",
    "           linewidth=2, markersize=7)\n",
    "  for i in range(20):\n",
    "      plt.text(values[i][0]+0.1, values[i][1], f'step {i}', fontdict={'color': 'r'})\n",
    "  plt.plot(x, y, linewidth=2)\n",
    "  plt.grid()\n",
    "  plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "  plt.legend(['Optimizer steps', 'Square function'])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41136949-d423-4794-be62-3d1c89e18217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a first value for momentum\n",
    "mom0 = .9\n",
    "optimize_and_plot(momentum=mom0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e009e30-204e-4c22-99e3-7e8ec00d6a27",
   "metadata": {},
   "source": [
    "## Instructions 2/2\n",
    "-        Try a second value for the momentum such that the optimizer finds the global optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef68ed9f-c7ad-4c3d-964b-ec53eab7570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a second value for momentum\n",
    "mom1 = .93\n",
    "optimize_and_plot(momentum=mom1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7214f01b-c4a0-487a-9a09-4e3f5c460a9f",
   "metadata": {},
   "source": [
    "## Freeze layers of a model\n",
    "\n",
    "#### You are about to fine-tune a model on a new task after loading pre-trained weights. The model contains three linear layers. However, because your dataset is small, you only want to train the last linear layer of this model and freeze the first two linear layers.\n",
    "\n",
    "#### The model has already been created and exists under the variable model. You will be using the named_parameters method of the model to list the parameters of the model. Each parameter is described by a name. This name is a string with the following naming convention: x.name where x is the index of the layer.\n",
    "\n",
    "#### Remember that a linear layer has two parameters: the weight and the bias.\n",
    "\n",
    "## Instructions\n",
    "-    Use an if statement to determine if the parameter should be frozen or not based on its name.\n",
    "-    Freeze the parameters of the first two layers of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808f7755-f950-4cc5-8c87-53b354abd96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():    \n",
    "  \n",
    "    # Check if the parameters belong to the first layer\n",
    "    if name == '0.weight' or name == '0.bias':\n",
    "      \n",
    "        # Freeze the parameters\n",
    "        param.requires_grad = False\n",
    "  \n",
    "    # Check if the parameters belong to the second layer\n",
    "    if name == '1.weight' or name == '1.bias':\n",
    "      \n",
    "        # Freeze the parameters\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9a6db3-4bbe-4f94-bddb-cdbb8f4d8e37",
   "metadata": {},
   "source": [
    "## Layer initialization\n",
    "\n",
    "#### The initialization of the weights of a neural network has been the focus of researchers for many years. When training a network, the method used to initialize the weights has a direct impact on the final performance of the network.\n",
    "\n",
    "#### As a machine learning practitioner, you should be able to experiment with different initialization strategies. In this exercise, you are creating a small neural network made of two layers and you are deciding to initialize each layer's weights with the uniform method.\n",
    "\n",
    "## Instructions\n",
    "-    For each layer (layer0 and layer1), use the uniform initialization method to initialize the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5931c93f-b831-4d98-af44-b65bd4db281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer0 = nn.Linear(16, 32)\n",
    "layer1 = nn.Linear(32, 64)\n",
    "\n",
    "# Use uniform initialization for layer0 and layer1 weights\n",
    "nn.init.uniform_(layer0.weight)\n",
    "nn.init.uniform_(layer1.weight)\n",
    "\n",
    "model = nn.Sequential(layer0, layer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2400c30e-3935-4975-bd06-59c7b5fabed0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1668de-9b54-4e84-bda5-5b1ae3c7b52d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df66b91d-772a-433f-9644-f047d637cb02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
