{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5adb63bd-9619-4935-b399-2d6576f16d4c",
   "metadata": {},
   "source": [
    "## Reading a CSV file\n",
    "\n",
    "Loading the data is the first step in building a data transformation pipeline. “Comma separated values” (CSV) is a file commonly used file format for data exchange. You’re now going to use Spark to read a CSV file.\n",
    "\n",
    "You’ve seen in the videos how to load landing/prices.csv. Now let’s do the same for landing/ratings.csv, step by step. Remember, the actual data lake is made available to you under ~/workspace/mnt/data_lake.\n",
    "\n",
    "A SparkSession named spark has already been loaded for you.\n",
    "\n",
    "### Instructions\n",
    "    - Create a DataFrameReader object using the spark.read property.\n",
    "    - Make the reader object use the header of the CSV file to name the columns automatically, by passing in the correct keyword arguments to the reader’s .options() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4324c300-0783-43e1-91bb-85d42fc81343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a csv file and set the headers\n",
    "df = (spark.read\n",
    "      .options(header=\"true\")\n",
    "      .csv(\"/home/repl/workspace/mnt/data_lake/landing/ratings.csv\"))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3535270b-3807-4a99-8a76-374595650142",
   "metadata": {},
   "source": [
    "## Defining a schema\n",
    "\n",
    "In the last exercise, you read a CSV file using PySpark. Because you didn’t define a schema, all column values were parsed as strings which can be cumbersome and inefficient to process. You are usually better off defining the data types in a schema yourself.\n",
    "\n",
    "To do this, you use classes from the pyspark.sql.types module. Its StructType() class expects a list of StructField() instances that allow you to add fields to a schema. Various other types, such as ByteType() and IntegerType() are also defined in this module and can be used to specify the data types of each field. In this exercise, all of these classes have been imported for you.\n",
    "\n",
    "In the ratings.csv dataset from the previous exercise, the rating values in the columns “absorption_rate” and “comfort” are expressed on a scale from 1 to 5, like with Amazon’s web store. Because of that, they easily fit into a ByteType(), which can hold values between -128 and 127. The other columns are better left as StringType()s.\n",
    "\n",
    "Feel free to explore the previous Spark DataFrame in the console using df.show() so you can map each column to the correct type.\n",
    "\n",
    "### Instructions\n",
    "    - Define the schema for the spreadsheet that has the columns “brand”, “model”, “absorption_rate” and “comfort”, in that order.\n",
    "    - Pass the predefined schema while loading the CSV file using the .schema() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7397125f-5e4e-4904-991a-7e5278fc047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema\n",
    "schema = StructType([\n",
    "  StructField(\"brand\", StringType(), nullable=False),\n",
    "  StructField(\"model\", StringType(), nullable=False),\n",
    "  StructField(\"absorption_rate\", ByteType(), nullable=True),\n",
    "  StructField(\"comfort\", ByteType(), nullable=True)\n",
    "])\n",
    "\n",
    "better_df = (spark\n",
    "             .read\n",
    "             .options(header=\"true\")\n",
    "             # Pass the predefined schema to the Reader\n",
    "             .schema(schema)\n",
    "             .csv(\"/home/repl/workspace/mnt/data_lake/landing/ratings.csv\"))\n",
    "pprint(better_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a89f6b-ec4a-485c-9f4b-a0a44916472a",
   "metadata": {},
   "source": [
    "## Removing invalid rows\n",
    "\n",
    "Data scientists spend a lot of their time cleaning data, because most data sources they work with are not ready for analytics. Hence, the second step in the data transformation pipeline is cleaning the data.\n",
    "\n",
    "In the previous exercise you have dealt with incorrect data types. In this exercise, you will use Spark to clean a DataFrame, as it contains invalid rows, a common problem with real data.\n",
    "\n",
    "You’ve seen in the videos how to clean landing/prices_with_invalid_rows.csv. Now do the same for landing/ratings_with_invalid_rows.csv, step by step.\n",
    "\n",
    "A SparkSession named spark has already been loaded for you.\n",
    "\n",
    "### Instructions\n",
    "    - Remove any invalid rows by passing the correct keyword (and associated value) to the reader’s .options() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09fbd86-42d8-4bd1-ad33-54fc7a2a2b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the option to drop invalid rows\n",
    "ratings = (spark\n",
    "           .read\n",
    "           .options(header=True, mode=\"DROPMALFORMED\")\n",
    "           .csv(\"/home/repl/workspace/mnt/data_lake/landing/ratings_with_invalid_rows.csv\"))\n",
    "ratings.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f772420-e4a8-4e50-96e7-f0fa5381e48b",
   "metadata": {},
   "source": [
    "## Filling unknown data\n",
    "\n",
    "What if almost all fields in a row are valid, except one or two. What do you do in this case? Sometimes, these fields are not critical and can be left empty, sometimes they can be given a default value. Let’s try to pass a default value.\n",
    "\n",
    "You’ve seen in the videos how to clean landing/prices_with_incomplete_rows.csv. Now let’s do the same for landing/ratings_with_incomplete_rows.csv, step by step.\n",
    "\n",
    "A SparkSession named spark has already been loaded for you, as well as the DataFrame ratings.\n",
    "\n",
    "### Instructions\n",
    "    - Fill the incomplete rows, by supplying the default numeric value of 4 for the comfort column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a204a4c8-6d2b-4602-ae53-28a603a9191a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BEFORE\")\n",
    "ratings.show()\n",
    "\n",
    "print(\"AFTER\")\n",
    "# Replace nulls with arbitrary value on column subset\n",
    "ratings = ratings.fillna(4, subset=[\"comfort\"])\n",
    "ratings.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ce357-0293-4b27-b7dc-74736e843136",
   "metadata": {},
   "source": [
    "## Conditionally replacing values\n",
    "\n",
    "Another common situation is that you have values that you want to replace or that don’t make any sense as we saw in the video. You can select the column to be transformed by using the .withColumn() method, conditionally replace those values using the pyspark.sql.functions.when function when values meet a given condition or leave them unaltered when they don’t with the .otherwise() method.\n",
    "\n",
    "In this exercise, you will do just that by transforming the “comfort” column of the ratings DataFrame (already loaded) by replacing its numeric values with the string \"sufficient\" when the comfort value is bigger than 3, and \"insufficient\" when it is not.\n",
    "\n",
    "### Instructions\n",
    "    - Use the .withColumn() method to relabel the column named “comfort”.\n",
    "    - Use the when() function to replace values of the “comfort” column larger than 3 with the string \"sufficient\".\n",
    "    - Use the .otherwise() method to replace remaining values with \"insufficient\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b118046-2cb4-41ad-9924-438a743bb752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Add/relabel the column\n",
    "categorized_ratings = ratings.withColumn(\n",
    "    \"comfort\",\n",
    "    # Express the condition in terms of column operations\n",
    "    when(col(\"comfort\") > 3, \"sufficient\").otherwise(\"insufficient\"))\n",
    "\n",
    "categorized_ratings.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fa521f-93bd-4825-bd34-6fd1760b4012",
   "metadata": {},
   "source": [
    "## Selecting and renaming columns\n",
    "\n",
    "Transformations are, after ingestion, the next step in data engineering pipelines. Data gets transformed, because certain insights need to be derived. Clear column names help in achieving that goal.\n",
    "\n",
    "You’ve seen in the videos how to select and rename columns of the landing/prices.csv file. Now do the same for landing/ratings.csv, step by step.\n",
    "\n",
    "A SparkSession named spark has already been loaded for you and the CSV file was read in a DataFrame called ratings.\n",
    "\n",
    "### Instructions\n",
    "    - Select the columns “brand”, “model” and “absorption_rate” from the ratings DataFrame.\n",
    "    - Rename the “absorption_rate” column to “absorbency”.\n",
    "    - Show only the distinct values of the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd651ff-56d5-4778-8ae5-02ee9dba85f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Select the columns and rename the \"absorption_rate\" column\n",
    "result = ratings.select([col(\"brand\"),\n",
    "                       col(\"model\"),\n",
    "                       col(\"absorption_rate\").alias(\"absorbency\")])\n",
    "\n",
    "# Show only unique values\n",
    "result.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e966b12-1371-4b24-b582-8b5dfc50624b",
   "metadata": {},
   "source": [
    "## Grouping and aggregating data\n",
    "\n",
    "Often you will want to compute a metric over a set of values that share a common characteristic, like the average price of a house in a certain region. To achieve this, you would need to group the data by region and compute an aggregate metric on that subgroup of data.\n",
    "\n",
    "We’ve already seen in the video a couple of these aggregation metrics, on landingprices.csv_. We’ll inspect a few more now and apply them to _~/workspace/mnt/data_lake/landing/purchased.csv_. In particular, you’ll use the spark.sql aggregation functions avg() to compute the average value of some column in a group, stddev_samp() to compute the standard (sample) deviation and max() (which we alias as sfmax so as not to shadow Python’s built-in max()) to retrieve the largest value of some column in a group.\n",
    "\n",
    "### Instructions\n",
    "    - Use the .groupBy() method to group the data by the “Country” column.\n",
    "    - In these groups, compute the average of the “Salary” column and name the resulting column “average_salary”.\n",
    "    - Compute the standard deviation of the “Salary” column in each group in the same aggregation.\n",
    "    - Retrieve the largest “Salary” in each group, in the same aggregation, and name the resulting column “highest_salary”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160c2070-5010-4ab5-9229-ae5c84d95ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, stddev_samp, max as sfmax\n",
    "\n",
    "aggregated = (purchased\n",
    "              # Group rows by 'Country'\n",
    "              .groupBy(col('Country'))\n",
    "              .agg(\n",
    "                # Calculate the average salary per group and rename\n",
    "                avg('Salary').alias('average_salary'),\n",
    "                # Calculate the standard deviation per group\n",
    "                stddev_samp('Salary'),\n",
    "                # Retain the highest salary per group and rename\n",
    "                sfmax('Salary').alias('highest_salary')\n",
    "              )\n",
    "             )\n",
    "\n",
    "aggregated.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3ca463-0240-4204-9747-da69b924c54f",
   "metadata": {},
   "source": [
    "## Creating a deployable artifact\n",
    "\n",
    "In the video, you saw how to run a PySpark program locally by first zipping your code. This packaging step becomes more important when your code consists of many modules. Packaging in the zip format is done by navigating to the root folder of your pipeline using the cd command and running the following command:\n",
    "\n",
    "```bash\n",
    "zip --recurse-paths zip_file.zip pipeline_folder\n",
    "```\n",
    "\n",
    "In this exercise you will first create a zipped archive, to pass along as dependencies of your Spark job. Then you have one of the prerequisites to running a Spark job locally, which you know now is a good way to get simple issues resolved quickly.\n",
    "\n",
    "### Ide Exercise Instruction\n",
    "    - In the shell, navigate to /home/repl/workspace/spark_pipelines/.\n",
    "    - There, create the pydiaper.zip archive using the bash command zip and its options. The compressed archive should contain all the files in the pydiaper folder (including that folder, because that’s the package imported by various modules), and those in all of the directories below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1bcc06-0942-4a55-b5b7-274feb167b17",
   "metadata": {},
   "source": [
    "- You need to run these commands in the terminal:\n",
    "\n",
    "```bash\n",
    "$ cd spark_pipelines\n",
    "$ zip --recurse-paths pydiaper.zip pydiaper\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cd9586-7712-4d89-b37d-09b71e11bac5",
   "metadata": {},
   "source": [
    "# Submitting your Spark job\n",
    "\n",
    "With the dependencies of a job ready to be distributed across a cluster’s nodes, you can submit a job to a cluster easily. In this exercise, you'll be testing the job locally.\n",
    "\n",
    "To run a PySpark application locally, you need to call:\n",
    "\n",
    "```bash\n",
    "spark-submit --py-files PY_FILES MAIN_PYTHON_FILE\n",
    "```\n",
    "\n",
    "with PY_FILES being either a zipped archive, a Python egg or separate Python files that will be placed on the PYTHONPATH environment variable of your cluster's nodes. The MAIN_PYTHON_FILE should be the entry point of your application.\n",
    "\n",
    "In this particular exercise, the path of the zipped archive is spark_pipelines/pydiaper/pydiaper.zip whereas the path to your application entry point is spark_pipelines/pydiaper/pydiaper/cleaning/clean_ratings.py.\n",
    "\n",
    "### Ide Exercise Instruction\n",
    "    - Submit the Spark job to clean the user ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ea14e2-7c58-40f6-9350-381243f9c3f3",
   "metadata": {},
   "source": [
    "- You need to run this command in the terminal\n",
    "\n",
    "```bash\n",
    "$ spark-submit --py-files spark_pipelines/pydiaper/pydiaper.zip ./spark_pipelines/pydiaper/pydiaper/cleaning/clean_ratings.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
