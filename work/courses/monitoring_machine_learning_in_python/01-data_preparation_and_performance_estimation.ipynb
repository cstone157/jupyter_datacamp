{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cef3449f-3a97-4349-bb55-79483184db55",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "NannyML comes with a set of internal datasets in order to make it easier to demo use cases and test different algorithms. To load the dataset, you only need to use the nannyml.load_us_census_ma_employment_data() function.\n",
    "\n",
    "The function returns three Pandas DataFrame objects: the reference set (the test set), the analysis set (unseen production data), and the ground truth for the analysis set. These data frames should be named according to the convention as reference, analysis, and analysis_gt.\n",
    "\n",
    "In this exercise, you will load the US Census Employment dataset and print the data frames to understand what they look like.\n",
    "\n",
    "### Instructions\n",
    "    - Import the nannyml libary.\n",
    "    - Load the US Census Employment dataset from the nannyml library.\n",
    "    - Print the head of the reference data.\n",
    "    - Print the head of the analysis data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30ee1e5-4258-45a4-aef1-526f628e82b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nannyml\n",
    "import nannyml\n",
    "\n",
    "# Load US Census Employment dataset\n",
    "reference, analysis, analysis_gt = nannyml.load_us_census_ma_employment_data()\n",
    "\n",
    "# Print head of the reference data\n",
    "print(reference.head())\n",
    "\n",
    "# Print head of the analysis data\n",
    "print(analysis.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb9410b-4401-4a1b-85fe-cd9de3d32c53",
   "metadata": {},
   "source": [
    "## Loading and splitting the data\n",
    "\n",
    "To deploy and monitor a model in production, you must first create it. In the last video, you've been introduced to loading and processing data, building the model, and creating reference and analysis sets.\n",
    "\n",
    "In this exercise, you'll follow a similar process, but to simplify matters, you'll use the NYC Green Taxi dataset provided in a csv file that's already been processed.\n",
    "\n",
    "For this exercise, pandas has been imported as pd and is ready for you to use.\n",
    "\n",
    "### Instructions 1/2\n",
    "    - Pass the green_taxi_dataset.csv to the dataset_name variable.\n",
    "    - Use pd.read_csv() to load the dataset.\n",
    "    - Show the head of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848b0ff7-96a3-4072-8731-b0c510229e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset_name = \"green_taxi_dataset.csv\"\n",
    "data = pd.read_csv(dataset_name)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5766d9-2cfe-47f6-87a6-0e9d1caed677",
   "metadata": {},
   "source": [
    "### Instructions 2/2\n",
    "    - Split the dataset into a training set, a test set, and a production(prod) set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549967c3-ca0a-4345-beb0-835972c9d53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset_name = \"green_taxi_dataset.csv\"\n",
    "data = pd.read_csv(dataset_name)\n",
    "features = ['lpep_pickup_datetime', 'PULocationID', 'DOLocationID', 'trip_distance', 'fare_amount', 'pickup_time']\n",
    "target = 'tip_amount'\n",
    "\n",
    "# Split the training data\n",
    "X_train = data.loc[data['partition'] == 'train', features]\n",
    "y_train = data.loc[data['partition'] == 'train', target]\n",
    "\n",
    "# Split the test data\n",
    "X_test = data.loc[data['partition'] == 'test', features]\n",
    "y_test = data.loc[data['partition'] == 'test', target]\n",
    "\n",
    "# Split the prod data\n",
    "X_prod = data.loc[data['partition'] == 'prod', features]\n",
    "y_prod = data.loc[data['partition'] == 'prod', target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254bfdd1-e03a-41c3-988d-01c235dfe50f",
   "metadata": {},
   "source": [
    "## Creating reference and analysis set\n",
    "\n",
    "After your data is split into train, test, and production sets, you can build and deploy your model. The testing and production data will later be used to create the reference and analysis set.\n",
    "\n",
    "In this exercise, you will go through this process. You have all of your X_train/test/prod, and y_train/test/prod datasets created in the previous exercise already loaded here.\n",
    "\n",
    "For this exercise, pandas has been imported as pd and is ready for use.\n",
    "\n",
    "### Instructions 1/2\n",
    "    - Train the model using fit method and pass X_train and y_train sets.\n",
    "    - Make predictions on train and test sets.\n",
    "    - Deploy the model by making predictions for production data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8795f7-98c7-43b6-915d-0824a5f730b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Fit the model\n",
    "model = LGBMRegressor(random_state=111, n_estimators=50, n_jobs=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Deploy the model\n",
    "y_pred_prod = model.predict(X_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c766d1ba-b15e-4d1e-a796-5351d5c9c355",
   "metadata": {},
   "source": [
    "### Instructions 2/2\n",
    "    - Add a y_pred(models predictions) column for the reference and analysis sets, assigning the model's predictions from the test and production sets, respectively.\n",
    "    - Add a tip_amount column to reference set set and assign values from y_test(labels) to it.\n",
    "    - Join the lpep_pickup_datetime timestamp column for reference and analysis set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b09e44-0933-4851-ad7c-37cf6be64a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Fit the model\n",
    "model = LGBMRegressor(random_state=111, n_estimators=50, n_jobs=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get model's prediction on train, test, and production set\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "y_pred_prod = model.predict(X_prod)\n",
    "\n",
    "# Create reference and analysis set\n",
    "reference = X_test.copy() # Copy test set features\n",
    "reference['y_pred'] = y_pred_test # Add models predictions on test set\n",
    "reference['tip_amount'] = y_test # Add labels(ground truth)\n",
    "reference = reference.join(data['lpep_pickup_datetime']) # Add timestamp column\n",
    "\n",
    "analysis = X_prod.copy() # Add production set features\n",
    "analysis['y_pred'] = y_pred_prod # Add models predictions on production set\n",
    "analysis = analysis.join(data['lpep_pickup_datetime']) # Add timestamp column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9409dcb-9390-4243-848f-03698c4c9359",
   "metadata": {},
   "source": [
    "## Performance estimation for tip prediction\n",
    "\n",
    "In the previous exercises, you prepared a reference and analysis set for the NYC Green Taxi dataset. In this one, you will use that data to estimate the model's performance in production.\n",
    "\n",
    "First, you must initialize the DLE algorithm with the provided parameters and then plot the results.\n",
    "\n",
    "The reference and analysis set is already loaded and saved in the reference and analysis variables. Additionally, nannyml is also already imported.\n",
    "\n",
    "### Instructions\n",
    "    - Initiate the DLE algorithm with daily chunk period, tip_amount as a y_true , and MSE metric.\n",
    "    - Fit reference set to the DLE estimator, estimate performance for analysis set and store the output in the results variable.\n",
    "    - Visualize the results using plot() and show() methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b9ef05-7c1f-4fbb-bbea-9e5d5294422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = nannyml.DLE(y_pred='y_pred',\n",
    "    timestamp_column_name='lpep_pickup_datetime',\n",
    "    feature_column_names=features,\n",
    "    chunk_period='d',\n",
    "    y_true='tip_amount',\n",
    "    metrics=['mse'])\n",
    "\n",
    "# Fit the reference data to the DLE algorithm\n",
    "estimator.fit(reference)\n",
    "\n",
    "# Estimate the performance on the analysis data\n",
    "results = estimator.estimate(analysis)\n",
    "\n",
    "# Plot and show the results\n",
    "results.plot().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
