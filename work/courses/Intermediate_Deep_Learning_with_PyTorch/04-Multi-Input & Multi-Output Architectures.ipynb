{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "429bb128-afb4-43e8-b57c-392dff859295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchmetrics import Precision, Recall\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9467b98-2623-4e0b-b780-df5c3379dfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile \n",
    "\n",
    "# loading the temp.zip and creating a zip object \n",
    "with ZipFile(\"data/omniglot_test.zip\", 'r') as zObject: \n",
    "    # Extracting all the members of the zip  \n",
    "    # into a specific location. \n",
    "    zObject.extractall(path=\".\") \n",
    "\n",
    "with ZipFile(\"data/omniglot_train.zip\", 'r') as zObject: \n",
    "    # Extracting all the members of the zip  \n",
    "    # into a specific location. \n",
    "    zObject.extractall(path=\".\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e553ea-4b70-4fec-ad1e-79cb83a72acc",
   "metadata": {},
   "source": [
    "## Two-input dataset\n",
    "\n",
    "###### Building a multi-input model starts with crafting a custom dataset that can supply all the inputs to the model. In this exercise, you will build the Omniglot dataset that serves triplets consisting of:\n",
    "\n",
    "###### -    The image of a character to be classified,\n",
    "###### -    The one-hot encoded alphabet vector of length 30, with zeros everywhere but for a single one denoting the ID of the alphabet the character comes from,\n",
    "###### -    The target label, an integer between 0 and 963.\n",
    "\n",
    "###### You are provided with train_samples, a list of 3-tuples comprising an image's file path, its alphabet vector, and the target label. Also, the following imports have already been done for you, so let's get to it!\n",
    "\n",
    "<code>\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "</code>\n",
    "\n",
    "### Instructions 1/4\n",
    "-    Assign transform and samples to class attributes with the same names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "029ca679-d890-40ca-8ee5-f6d720db7d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmniglotDataset(Dataset):\n",
    "    def __init__(self, transform, samples):\n",
    "        # Assign transform and samples to class attributes\n",
    "        self.transform = transform\n",
    "        self.samples = samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88ff7ad-7b81-4111-a186-cfcf31724103",
   "metadata": {},
   "source": [
    "### Instructions 2/4\n",
    "-    Implement the .__len()__ method such that it return the number of samples stored in the class' samples attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83814f68-28c0-4efb-ac03-88916308b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmniglotDataset(Dataset):\n",
    "    def __init__(self, transform, samples):\n",
    "\t\t# Assign transform and samples to class attributes\n",
    "        self.transform = transform\n",
    "        self.samples = samples\n",
    "                    \n",
    "    def __len__(self):\n",
    "        # Return number of samples\n",
    "        return len(self.samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839bd9c9-27b2-42a6-aab5-e15a1d71be1e",
   "metadata": {},
   "source": [
    "### Instructions 3/4\n",
    "-    Unpack the sample at index idx assigning its contents to img_path, alphabet, and label.\n",
    "-    Transform the loaded image with self.transform() and assign it to img_transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8175646-8997-4856-9085-8af9bb15bace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OmniglotDataset(Dataset):\n",
    "    def __init__(self, transform, samples):\n",
    "\t\t# Assign transform and samples to class attributes\n",
    "        self.transform = transform\n",
    "        self.samples = samples\n",
    "                    \n",
    "    def __len__(self):\n",
    "\t\t# Return number of samples\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      \t# Unpack the sample at index idx\n",
    "        img_path, alphabet, label = self.samples[idx]\n",
    "        img = Image.open(img_path).convert('L')\n",
    "        # Transform the image \n",
    "        img_transformed = self.transform(img)\n",
    "        return img_transformed, alphabet, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca662a3-efcf-416d-8027-a6ca1bea6e5a",
   "metadata": {},
   "source": [
    "## Two-input model\n",
    "\n",
    "##### With the data ready, it's time to build the two-input model architecture! To do so, you will set up a model class with the following methods:\n",
    "\n",
    "-    .\\_\\_init\\_\\_(), in which you will define sub-networks by grouping layers; this is where you define the two layers for processing the two inputs, and the classifier that returns a classification score for each class.\n",
    "-    forward(), in which you will pass both inputs through corresponding pre-defined sub-networks, concatenate the outputs, and pass them to the classifier.\n",
    "\n",
    "##### torch.nn is already imported for you as nn. Let's do it!\n",
    "\n",
    "### Instructions 1/3\n",
    "-    Define image, alphabet and classifier sub-networks as sequential models, assigning them to self.image_layer, self.alphabet_layer and self.classifier, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce45cd2f-4860-41de-9fea-10147c6cdbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Define sub-networks as sequential models\n",
    "        self.image_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ELU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*32*32, 128)\n",
    "        )\n",
    "        self.alphabet_layer = nn.Sequential(\n",
    "            nn.Linear(30, 8),\n",
    "            nn.ELU(), \n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 + 8, 964), \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c03d9c-75c3-4e09-b073-b98ac62119d5",
   "metadata": {},
   "source": [
    "### Instructions 2/3\n",
    "-    Pass the image and alphabet through the appropriate model layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a652128-391d-4d50-940b-b5d27ba1b7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Define sub-networks as sequential models\n",
    "        self.image_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ELU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*32*32, 128)\n",
    "        )\n",
    "        self.alphabet_layer = nn.Sequential(\n",
    "            nn.Linear(30, 8),\n",
    "            nn.ELU(), \n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 + 8, 964), \n",
    "        )\n",
    "        \n",
    "    def forward(self, x_image, x_alphabet):\n",
    "        # Pass the x_image and x_alphabet through appropriate layers\n",
    "        x_image = self.image_layer(x_image)\n",
    "        x_alphabet = self.alphabet_layer(x_alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4618795-9037-4401-bc3c-246ba4fdf16e",
   "metadata": {},
   "source": [
    "### Instructions 3/3\n",
    "-    Concatenate the outputs from image and alphabet layers and assign the result to x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "997ea789-a364-4872-a65c-967fc02dce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Define sub-networks as sequential models\n",
    "        self.image_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ELU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*32*32, 128)\n",
    "        )\n",
    "        self.alphabet_layer = nn.Sequential(\n",
    "            nn.Linear(30, 8),\n",
    "            nn.ELU(), \n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 + 8, 964), \n",
    "        )\n",
    "        \n",
    "    def forward(self, x_image, x_alphabet):\n",
    "\t\t# Pass the x_image and x_alphabet through appropriate layers\n",
    "        x_image = self.image_layer(x_image)\n",
    "        x_alphabet = self.alphabet_layer(x_alphabet)\n",
    "        # Concatenate x_image and x_alphabet\n",
    "        x = torch.cat((x_image, x_alphabet), dim=1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b08be66-117d-4850-9c16-c313f496604f",
   "metadata": {},
   "source": [
    "## Two-output Dataset and DataLoader\n",
    "\n",
    "##### In this and the following exercises, you will build a two-output model to predict both the character and the alphabet it comes from based on the character's image. As always, you will start with getting the data ready.\n",
    "\n",
    "##### The OmniglotDataset class you have created before is available for you to use along with updated samples. Let's use it to build the Dataset and the DataLoader.\n",
    "\n",
    "##### The following imports have already been done for you:\n",
    "\n",
    "<code>\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "</code>\n",
    "\n",
    "### Instructions 1/3\n",
    "-    Print the element of samples at index 100 and examine its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "112d500f-ecb4-43fc-9986-042eb9413bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('./omniglot_train/Gujarati/character22/0439_20.png', 0, 22),\n",
       " ('./omniglot_train/Gujarati/character22/0439_09.png', 0, 22),\n",
       " ('./omniglot_train/Gujarati/character22/0439_18.png', 0, 22),\n",
       " ('./omniglot_train/Gujarati/character22/0439_06.png', 0, 22),\n",
       " ('./omniglot_train/Gujarati/character22/0439_01.png', 0, 22),\n",
       " ('./omniglot_train/Gujarati/character22/0439_08.png', 0, 22),\n",
       " ('./omniglot_train/Gujarati/character22/0439_05.png', 0, 22),\n",
       " ('./omniglot_train/Gujarati/character22/0439_10.png', 0, 22),\n",
       " ('./omniglot_train/Gujarati/character22/0439_19.png', 0, 22),\n",
       " ('./omniglot_train/Gujarati/character22/0439_14.png', 0, 22)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WHAT samples should look like\n",
    "# ('omniglot_train/Gujarati/character42/0459_02.png', 0, 0),\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "mypath = \"./omniglot_train/\"\n",
    "language = 0\n",
    "character = 0\n",
    "\n",
    "characters = []\n",
    "for f in listdir(mypath):\n",
    "    if not f.startswith(\".\"):\n",
    "        tmp_path = mypath + f + \"/\"\n",
    "\n",
    "        #lang_chars = [(tmp_path + g, language, int(g[9:])) for g in listdir(tmp_path) if not g.startswith(\".\")]\n",
    "        \n",
    "        for g in listdir(tmp_path): \n",
    "            if not g.startswith(\".\"):\n",
    "                imgs = [(tmp_path + g + \"/\" + c, language, int(g[9:])) for c in listdir(tmp_path + g) if not c.startswith(\".\")]\n",
    "                characters.append(imgs)\n",
    "        \n",
    "        language += 1\n",
    "\n",
    "samples = [item for row in characters for item in row]\n",
    "\n",
    "#samples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a863c58-38ec-4d28-959a-20500d3a87d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('./omniglot_train/Gujarati/character28/0445_19.png', 0, 28)\n"
     ]
    }
   ],
   "source": [
    "# Print the sample at index 100\n",
    "print(samples[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885f25ee-a257-4c28-ab3c-99382b9986b0",
   "metadata": {},
   "source": [
    "### Instructions 2/3\n",
    "-    Use your OmniglotDataset to create dataset_train, passing the two image transforms you have used before: parse the image to a tensor and resize it to size (64, 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eaa9e987-d87b-40e1-a7e9-a62dd8cd9078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('./omniglot_train/Gujarati/character28/0445_19.png', 0, 28)\n"
     ]
    }
   ],
   "source": [
    "# Print the sample at index 100\n",
    "print(samples[100])\n",
    "\n",
    "# Create dataset_train\n",
    "dataset_train = OmniglotDataset(\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((128, 128)),\n",
    "    ]),\n",
    "  samples=samples,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f4f7e8-6631-446a-8775-8260de220034",
   "metadata": {},
   "source": [
    "### Instructions 3/3\n",
    "-    Create dataloader_train from dataset_train; shuffle the training images and set batch size to 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bc8c4a2e-556e-4c25-a66d-8e4dd23bd03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('./omniglot_train/Gujarati/character28/0445_19.png', 0, 28)\n"
     ]
    }
   ],
   "source": [
    "# Print the sample at index 100\n",
    "print(samples[100])\n",
    "\n",
    "# Create dataset_train\n",
    "dataset_train = OmniglotDataset(\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "      \ttransforms.Resize((64, 64)),\n",
    "    ]),\n",
    "    samples=samples,\n",
    ")\n",
    "\n",
    "# Create dataloader_train\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train, batch_size=2, shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcbfc4f-d56a-40c6-a0ac-c0fa82ed0402",
   "metadata": {},
   "source": [
    "## Two-output model architecture\n",
    "\n",
    "##### In this exercise, you will construct a multi-output neural network architecture capable of predicting the character and the alphabet.\n",
    "\n",
    "##### Recall the general structure: in the .__init__() method, you define layers to be used in the forward pass later. In the forward() method, you will first pass the input image through a couple of layers to obtain its embedding, which in turn is fed into two separate classifier layers, one for each output.\n",
    "\n",
    "##### torch.nn is already imported under its usual alias, so let's build a model!\n",
    "\n",
    "### Instructions 1/2\n",
    "-    Define self.classifier_alpha and self.classifier_char as linear layers with input shapes matching the output of image_layer, and output shapes corresponding to the number of alphabets (30) and the number of characters (964), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "baff3f91-b7c1-42a1-99df-f7cebbe67161",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.image_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ELU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*32*32, 128)\n",
    "        )\n",
    "\n",
    "        # Define the two classifier layers\n",
    "        self.classifier_alpha = nn.Linear(128, 30)\n",
    "        self.classifier_char = nn.Linear(128, 964)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f5f185-7138-4253-a8a0-fbe7dd9e831f",
   "metadata": {},
   "source": [
    "### Instructions 2/2\n",
    "-    Pass the image embedding x_image separately through each of the classifiers, assigning the results to output_alpha and output_char, respectively, and return them in this order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d4bdac2b-9412-400c-9337-ea859ce9b438",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.image_layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.ELU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16*32*32, 128)\n",
    "        )\n",
    "        # Define the two classifier layers\n",
    "        self.classifier_alpha = nn.Linear(128, 30)\n",
    "        self.classifier_char = nn.Linear(128, 964)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_image = self.image_layer(x)\n",
    "        # Pass x_image through the classifiers and return both results\n",
    "        output_alpha = self.classifier_alpha(x_image)\n",
    "        output_char = self.classifier_char(x_image)\n",
    "        return output_alpha, output_char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6ea1c7-e0a5-41fe-b660-6f3bef5b4284",
   "metadata": {},
   "source": [
    "## Training multi-output models\n",
    "\n",
    "##### When training models with multiple outputs, it is crucial to ensure that the loss function is defined correctly.\n",
    "\n",
    "##### In this case, the model produces two outputs: predictions for the alphabet and the character. For each of these, there are corresponding ground truth labels, which will allow you to calculate two separate losses: one incurred from incorrect alphabet classifications, and the other from incorrect character classification. Since in both cases you are dealing with a multi-label classification task, the Cross-Entropy loss can be applied each time.\n",
    "\n",
    "##### Gradient descent can optimize only one loss function, however. You will thus define the total loss as the sum of alphabet and character losses.\n",
    "\n",
    "### Instructions\n",
    "-    Calculate the alphabet classification loss and assign it to loss_alpha.\n",
    "-    Calculate the character classification loss and assign it to loss_char.\n",
    "-    Compute the total loss as the sum of the two partial losses and assign it to loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a8f1c59-53f4-4d4f-a9ea-271b6161dde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.05)\n",
    "\n",
    "for epoch in range(1):\n",
    "    for images, labels_alpha, labels_char in dataloader_train:\n",
    "        optimizer.zero_grad()\n",
    "        outputs_alpha, outputs_char = net(images)\n",
    "        # Compute alphabet classification loss\n",
    "        loss_alpha = criterion(outputs_alpha, labels_alpha)\n",
    "        # Compute character classification loss\n",
    "        loss_char = criterion(outputs_char, labels_char)\n",
    "        # Compute total loss\n",
    "        loss = loss_alpha + loss_char\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abd78fe-3c1d-460b-9c3f-73aed870d4cb",
   "metadata": {},
   "source": [
    "## Multi-output model evaluation\n",
    "\n",
    "##### In this exercise, you will practice model evaluation for multi-output models. Your task is to write a function called evaluate_model() that takes an alphabet-and-character-predicting model as input, runs the evaluation loop, and prints the model's accuracy in the two tasks.\n",
    "\n",
    "##### You can assume that the function will have access to dataloader_test. The following imports have already been run for you:\n",
    "\n",
    "<code>\n",
    "import torch\n",
    "from torchmetrics import Accuracy\n",
    "</code>\n",
    "\n",
    "##### Once you have implemented evaluate_model(), you will use it in the following exercise!\n",
    "\n",
    "### Instructions 1/3\n",
    "-    Define acc_alpha and acc_char as multi-class Accuracy() metrics for the two outputs, alphabets and characters, with the appropriate number of classes each (there are 30 alphabets and 964 characters in the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "107b88c4-487e-46b8-bdef-91f597485338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    # Define accuracy metrics\n",
    "    acc_alpha = Accuracy(task=\"multiclass\", num_classes=30)\n",
    "    acc_char = Accuracy(task=\"multiclass\", num_classes=964)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e21d25e-0f9b-42b8-bfb6-50d2667dae64",
   "metadata": {},
   "source": [
    "### Instructions 2/3\n",
    "-    Define the evaluation loop by iterating over test images, labels_alpha, and labels_char.\n",
    "-    Inside the for-loop, obtain model results for the test data batch and assign them to outputs_alpha, outputs_char."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2174de80-edc3-42db-95fb-fac4c3c93dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    # Define accuracy metrics\n",
    "    acc_alpha = Accuracy(task=\"multiclass\", num_classes=30)\n",
    "    acc_char = Accuracy(task=\"multiclass\", num_classes=964)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Iterate over test data batches\n",
    "        for images, labels_alpha, labels_char in dataloader_test:\n",
    "            # Obtain model outputs\n",
    "            outputs_alpha, outputs_char = model(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585dfa2f-7587-45e7-99d0-a025bf2dcecf",
   "metadata": {},
   "source": [
    "### Instructions 3/3\n",
    "-    Update the two accuracy metrics with the current batch's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40f5099e-209e-48c3-ac27-0e3ca737ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    # Define accuracy metrics\n",
    "    acc_alpha = Accuracy(task=\"multiclass\", num_classes=30)\n",
    "    acc_char = Accuracy(task=\"multiclass\", num_classes=964)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels_alpha, labels_char in dataloader_test:\n",
    "            # Obtain model outputs\n",
    "            outputs_alpha, outputs_char = model(images)\n",
    "            _, pred_alpha = torch.max(outputs_alpha, 1)\n",
    "            _, pred_char = torch.max(outputs_char, 1)\n",
    "\t\t\t# Update both accuracy metrics\n",
    "            acc_alpha(pred_alpha, labels_alpha)\n",
    "            acc_char(pred_char, labels_char)\n",
    "    \n",
    "    print(f\"Alphabet: {acc_alpha.compute()}\")\n",
    "    print(f\"Character: {acc_char.compute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfb041d-95d0-4cc7-a960-ab510d828d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
