{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec43cd80-f402-4073-a71a-78fa751b2f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost graphviz\n",
    "\n",
    "import graphviz\n",
    "\n",
    "# Import plotting modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bdcd00-fa5a-4210-8882-c07550182419",
   "metadata": {},
   "source": [
    "## Tuning the number of boosting rounds\n",
    "\n",
    "### Let's start with parameter tuning by seeing how the number of boosting rounds (number of trees you build) impacts the out-of-sample performance of your XGBoost model. You'll use xgb.cv() inside a for loop and build one model per num_boost_round parameter.\n",
    "\n",
    "### Here, you'll continue working with the Ames housing dataset. The features are available in the array X, and the target vector is contained in y.\n",
    "\n",
    "-    Create a DMatrix called housing_dmatrix from X and y.\n",
    "-    Create a parameter dictionary called params, passing in the appropriate \"objective\" (\"reg:linear\") and \"max_depth\" (set it to 3).\n",
    "-    Iterate over num_rounds inside a for loop and perform 3-fold cross-validation. In each iteration of the loop, pass in the current number of boosting rounds (curr_num_rounds) to xgb.cv() as the argument to num_boost_round.\n",
    "-    Append the final boosting round RMSE for each cross-validated XGBoost model to the final_rmse_per_round list.\n",
    "-    num_rounds and final_rmse_per_round have been zipped and converted into a DataFrame so you can easily see how the model performs with each boosting round. Hit 'Submit Answer' to see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98866c74-689b-48a5-8b53-3106421d0e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = pd.read_csv(\"../../data/ames_housing_trimmed_processed.csv\")\n",
    "X,y = housing_data[housing_data.columns.tolist()[:-1]],housing_data[housing_data.columns.tolist()[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd170e2a-92cb-414f-b309-e286873099e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_boosting_rounds          rmse\n",
      "0                    5  40350.042785\n",
      "1                   10  34222.544068\n",
      "2                   15  32537.190260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/xgboost/core.py:160: UserWarning: [02:34:01] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create the DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params \n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
    "\n",
    "# Create list of number of boosting rounds\n",
    "num_rounds = [5, 10, 15]\n",
    "\n",
    "# Empty list to store final round rmse per XGBoost model\n",
    "final_rmse_per_round = []\n",
    "\n",
    "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
    "for curr_num_rounds in num_rounds:\n",
    "\n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append final round RMSE\n",
    "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
    "print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fdad14-3750-4982-b716-28d4771d1dc3",
   "metadata": {},
   "source": [
    "## Automated boosting round selection using early_stopping\n",
    "\n",
    "### Now, instead of attempting to cherry pick the best possible number of boosting rounds, you can very easily have XGBoost automatically select the number of boosting rounds for you within xgb.cv(). This is done using a technique called early stopping.\n",
    "\n",
    "### Early stopping works by testing the XGBoost model after every boosting round against a hold-out dataset and stopping the creation of additional boosting rounds (thereby finishing training of the model early) if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds. Here you will use the early_stopping_rounds parameter in xgb.cv() with a large possible number of boosting rounds (50). Bear in mind that if the holdout metric continuously improves up through when num_boost_rounds is reached, then early stopping does not occur.\n",
    "\n",
    "### Here, the DMatrix and parameter dictionary have been created for you. Your task is to use cross-validation with early stopping. Go for it!\n",
    "\n",
    "-    Perform 3-fold cross-validation with early stopping and \"rmse\" as your metric. Use 10 early stopping rounds and 50 boosting rounds. Specify a seed of 123 and make sure the output is a pandas DataFrame. Remember to specify the other parameters such as dtrain, params, and metrics.\n",
    "-    Print cv_results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621586ea-69ef-47ca-b4af-a7c0532cda7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4eec07e8-ebac-49d8-9730-4bc4b66115e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
      "0      61708.384075      789.992240    63420.791562    2613.472325\n",
      "1      49522.546344      802.013790    53085.413747    2315.613545\n",
      "2      41169.937732      688.760982    46216.723045    2275.292419\n",
      "3      35291.530581      733.718357    41684.086762    2912.197343\n",
      "4      31046.377888      516.077566    38496.489124    2817.930645\n",
      "5      27696.206636      582.478240    36362.599330    2671.301364\n",
      "6      25408.301944      330.625199    35302.558898    2856.045380\n",
      "7      23639.812549      423.028610    34090.701206    2644.890618\n",
      "8      22255.018210      457.633545    33549.535045    2224.370383\n",
      "9      21042.204957      457.772868    33339.326024    2026.857986\n",
      "10     20190.735243      444.843073    33050.944991    1820.593939\n",
      "11     19405.436763      422.811600    32872.225752    1607.504842\n",
      "12     18825.729259      345.778797    32827.872247    1598.434460\n",
      "13     18276.857540      286.175363    32595.274062    1458.332167\n",
      "14     17817.708433      307.733726    32329.066786    1293.312181\n",
      "15     17490.368770      338.811939    32300.401008    1147.370213\n",
      "16     17127.710300      386.543502    32187.557605     959.532692\n",
      "17     16861.225088      315.905389    32085.832663     970.091560\n",
      "18     16624.823157      318.228209    32000.496006     998.552628\n",
      "19     16412.391603      306.676130    31901.414772     989.243563\n",
      "20     16101.948141      237.196507    31971.114015     928.485715\n",
      "21     15814.184923      251.737428    31811.880746    1020.025121\n",
      "22     15625.574779      237.074613    31867.686128     894.651876\n",
      "23     15454.503309      268.378017    31847.025148     789.994885\n",
      "24     15213.782639      375.634984    31833.367882     844.815287\n",
      "25     15000.887049      344.992477    31898.537101     776.634551\n",
      "26     14719.630105      308.994114    31833.534822     842.622938\n",
      "27     14492.574617      305.273386    31748.151662     752.174013\n",
      "28     14257.448180      255.950031    31699.670484     773.830394\n",
      "29     14079.358257      281.210658    31593.259747     803.802619\n",
      "30     13820.200418      329.160789    31546.480476     684.532413\n",
      "31     13623.444957      254.709185    31588.373867     654.924473\n",
      "32     13471.137892      279.473837    31567.277273     621.092081\n",
      "33     13323.024714      355.910437    31580.753850     611.629871\n",
      "34     13103.960905      324.883935    31638.756287     558.231457\n",
      "35     12915.041484      268.128395    31634.096144     567.469538\n",
      "36     12742.468103      253.082098    31551.363291     557.487521\n",
      "37     12630.599978      246.660357    31446.563558     574.462762\n",
      "38     12464.692821      273.681766    31440.780824     562.311438\n",
      "39     12315.755221      247.227348    31424.436429     600.757620\n",
      "40     12177.586271      239.645863    31419.597032     575.000375\n",
      "41     12073.000223      231.186596    31400.979313     640.855704\n",
      "42     11966.176359      205.111434    31318.818821     613.094368\n",
      "43     11785.968073      209.779764    31266.122759     602.314809\n",
      "44     11619.969729      191.148745    31252.644482     616.547269\n",
      "45     11502.363588      190.019842    31211.004245     568.956299\n",
      "46     11382.568566      268.818755    31174.535682     568.182228\n",
      "47     11236.729993      300.513827    31115.356617     542.338428\n",
      "48     11115.605347      307.209013    31106.022347     542.387109\n",
      "49     10993.824420      337.553551    31096.557556     544.377973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/xgboost/core.py:160: UserWarning: [02:34:02] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree: params\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
    "\n",
    "# Perform cross-validation with early stopping: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, early_stopping_rounds=10, num_boost_round=50, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80f6588-7615-4f4b-bf06-aafc644a8eac",
   "metadata": {},
   "source": [
    "## Tuning eta\n",
    "\n",
    "### It's time to practice tuning other XGBoost hyperparameters in earnest and observing their effect on model performance! You'll begin by tuning the \"eta\", also known as the learning rate.\n",
    "\n",
    "### The learning rate in XGBoost is a parameter that can range between 0 and 1, with higher values of \"eta\" penalizing feature weights more strongly, causing much stronger regularization.\n",
    "\n",
    "-    Create a list called eta_vals to store the following \"eta\" values: 0.001, 0.01, and 0.1.\n",
    "-    Iterate over your eta_vals list using a for loop.\n",
    "-    In each iteration of the for loop, set the \"eta\" key of params to be equal to curr_val. Then, perform 3-fold cross-validation with early stopping (5 rounds), 10 boosting rounds, a metric of \"rmse\", and a seed of 123. Ensure the output is a DataFrame.\n",
    "-    Append the final round RMSE to the best_rmse list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15dabe7-d292-4729-a69a-d931e38be178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a2dc341-d04b-43e2-87cd-ab14037bec3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     eta     best_rmse\n",
      "0  0.001  78903.745397\n",
      "1  0.010  74293.709019\n",
      "2  0.100  47136.241898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/xgboost/core.py:160: UserWarning: [02:34:03] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree (boosting round)\n",
    "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
    "\n",
    "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
    "eta_vals = [0.001, 0.01, 0.1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the eta \n",
    "for curr_val in eta_vals:\n",
    "\n",
    "    params[\"eta\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, early_stopping_rounds=5, num_boost_round=10, metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64292201-4d5e-4912-88b8-4be61f2cf1c3",
   "metadata": {},
   "source": [
    "## Tuning max_depth\n",
    "\n",
    "### In this exercise, your job is to tune max_depth, which is the parameter that dictates the maximum depth that each tree in a boosting round can grow to. Smaller values will lead to shallower trees, and larger values to deeper trees.\n",
    "\n",
    "-    Create a list called max_depths to store the following \"max_depth\" values: 2, 5, 10, and 20.\n",
    "-    Iterate over your max_depths list using a for loop.\n",
    "-    Systematically vary \"max_depth\" in each iteration of the for loop and perform 2-fold cross-validation with early stopping (5 rounds), 10 boosting rounds, a metric of \"rmse\", and a seed of 123. Ensure the output is a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f96e69b-ee22-407f-984b-9a3fe425cac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bdcf80ee-7e57-4627-9fa7-5b46ece21aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/xgboost/core.py:160: UserWarning: [02:34:04] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   max_depth     best_rmse\n",
      "0          2  37140.345524\n",
      "1          5  34301.329122\n",
      "2         10  35733.363167\n",
      "3         20  35937.767774\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params = {\"objective\":\"reg:linear\"}\n",
    "\n",
    "# Create list of max_depth values\n",
    "max_depths = [2, 5, 10, 20]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the max_depth\n",
    "for curr_val in max_depths:\n",
    "\n",
    "    params[\"max_depth\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, \n",
    "            nfold=2, early_stopping_rounds=5, num_boost_round=10, \n",
    "            metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[\"max_depth\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d555f9b-6e44-4f62-97fb-f2fc5a72335c",
   "metadata": {},
   "source": [
    "## Tuning colsample_bytree\n",
    "\n",
    "### Now, it's time to tune \"colsample_bytree\". You've already seen this if you've ever worked with scikit-learn's RandomForestClassifier or RandomForestRegressor, where it just was called max_features. In both xgboost and sklearn, this parameter (although named differently) simply specifies the fraction of features to choose from at every split in a given tree. In xgboost, colsample_bytree must be specified as a float between 0 and 1.\n",
    "\n",
    "-    Create a list called colsample_bytree_vals to store the values 0.1, 0.5, 0.8, and 1.\n",
    "-    Systematically vary \"colsample_bytree\" and perform cross-validation, exactly as you did with max_depth and eta previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b65fc3-cae9-45a5-90b4-be5a1187f836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5caa0cbb-0729-456f-a6a8-3cd9926b836e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   colsample_bytree     best_rmse\n",
      "0               0.1  47958.123027\n",
      "1               0.5  35066.230191\n",
      "2               0.8  35199.444917\n",
      "3               1.0  34786.230629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/xgboost/core.py:160: UserWarning: [02:34:05] WARNING: /workspace/src/objective/regression_obj.cu:209: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)\n",
    "\n",
    "# Create the parameter dictionary\n",
    "params={\"objective\":\"reg:linear\",\"max_depth\":3}\n",
    "\n",
    "# Create list of hyperparameter values: colsample_bytree_vals\n",
    "colsample_bytree_vals = [0.1, 0.5, 0.8, 1]\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the hyperparameter value \n",
    "for curr_val in colsample_bytree_vals:\n",
    "\n",
    "    params[\"colsample_bytree\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
    "                 num_boost_round=10, early_stopping_rounds=5,\n",
    "                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "    \n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0496d28-acdc-4afc-b7ae-3882db7940c8",
   "metadata": {},
   "source": [
    "## Grid search with XGBoost\n",
    "\n",
    "### Now that you've learned how to tune parameters individually with XGBoost, let's take your parameter tuning to the next level by using scikit-learn's GridSearch and RandomizedSearch capabilities with internal cross-validation using the GridSearchCV and RandomizedSearchCV functions. You will use these to find the best model exhaustively from a collection of possible parameter values across multiple parameters simultaneously. Let's get to work, starting with GridSearchCV!\n",
    "\n",
    "-    Create a parameter grid called gbm_param_grid that contains a list of \"colsample_bytree\" values (0.3, 0.7), a list with a single value for \"n_estimators\" (50), and a list of 2 \"max_depth\" (2, 5) values.\n",
    "-    Instantiate an XGBRegressor object called gbm.\n",
    "-    Create a GridSearchCV object called grid_mse, passing in: the parameter grid to param_grid, the XGBRegressor to estimator, \"neg_mean_squared_error\" to scoring, and 4 to cv. Also specify verbose=1 so you can better understand the output.\n",
    "-    Fit the GridSearchCV object to X and y.\n",
    "-    Print the best parameter values and lowest RMSE, using the .best_params_ and .best_score_ attributes, respectively, of grid_mse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f028f2b-b18e-4334-9ec6-b5c364838048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e23b9f0-0915-45ad-964e-7033d4a8143b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n",
      "Best parameters found:  {'colsample_bytree': 0.7, 'max_depth': 2, 'n_estimators': 50}\n",
      "Lowest RMSE found:  30948.247344949472\n"
     ]
    }
   ],
   "source": [
    "# Create the parameter grid: gbm_param_grid\n",
    "gbm_param_grid = {\n",
    "    'colsample_bytree': [0.3, 0.7],\n",
    "    'n_estimators': [50],\n",
    "    'max_depth': [2, 5]\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor()\n",
    "\n",
    "# Perform grid search: grid_mse\n",
    "grid_mse = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid,\n",
    "            scoring=\"neg_mean_squared_error\", cv=4, verbose=1)\n",
    "\n",
    "\n",
    "# Fit grid_mse to the data\n",
    "grid_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cafa135-76cb-4c6d-a968-c8d00db68068",
   "metadata": {},
   "source": [
    "## Random search with XGBoost\n",
    "\n",
    "### Often, GridSearchCV can be really time consuming, so in practice, you may want to use RandomizedSearchCV instead, as you will do in this exercise. The good news is you only have to make a few modifications to your GridSearchCV code to do RandomizedSearchCV. The key difference is you have to specify a param_distributions parameter instead of a param_grid parameter.\n",
    "\n",
    "-    Create a parameter grid called gbm_param_grid that contains a list with a single value for 'n_estimators' (25), and a list of 'max_depth' values between 2 and 11 for 'max_depth' - use range(2, 12) for this.\n",
    "-    Create a RandomizedSearchCV object called randomized_mse, passing in: the parameter grid to param_distributions, the XGBRegressor to estimator, \"neg_mean_squared_error\" to scoring, 5 to n_iter, and 4 to cv. Also specify verbose=1 so you can better understand the output.\n",
    "-    Fit the RandomizedSearchCV object to X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8a4f12b-c658-4b26-8ce6-6f34ac28d71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 5 candidates, totalling 20 fits\n",
      "Best parameters found:  {'n_estimators': 25, 'max_depth': 4}\n",
      "Lowest RMSE found:  31628.849360549713\n"
     ]
    }
   ],
   "source": [
    "# Create the parameter grid: gbm_param_grid \n",
    "gbm_param_grid = {\n",
    "    'n_estimators': [25],\n",
    "    'max_depth': range(2, 12)\n",
    "}\n",
    "\n",
    "# Instantiate the regressor: gbm\n",
    "gbm = xgb.XGBRegressor(n_estimators=10)\n",
    "\n",
    "# Perform random search: grid_mse\n",
    "randomized_mse = RandomizedSearchCV(estimator=gbm,\n",
    "            param_distributions=gbm_param_grid,\n",
    "            n_iter=5, scoring='neg_mean_squared_error', \n",
    "            cv=4, verbose=1)\n",
    "\n",
    "# Fit randomized_mse to the data\n",
    "randomized_mse.fit(X, y)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", randomized_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebd5ac9-a007-4037-a8dd-354ff2d5fa49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
