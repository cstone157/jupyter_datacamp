{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b02d3dc0-afe0-4548-be30-9ab62a0ddab6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39muse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mggplot\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "#!pip install xgboost\n",
    "\n",
    "# Import plotting modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b61993b-0cc3-4f6b-b2b8-2789fe2ae926",
   "metadata": {},
   "source": [
    "## First question, data-camp couldn't be bothered to include their damn data-set as usual.  Thanks guys good work.\n",
    "## Xgboost fit/predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bd6232d-e869-45c6-9aaf-769e2b7a5f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_data = pd.read_csv(\"../../data/churn_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "590c2a40-6ddb-4371-b7b5-04809a06151e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.759400\n"
     ]
    }
   ],
   "source": [
    "# Import xgboost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create arrays for the features and the target: X, y\n",
    "X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Instantiate the XGBClassifier: xg_cl\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xg_cl.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: preds\n",
    "preds = xg_cl.predict(X_test)\n",
    "\n",
    "# Compute the accuracy: accuracy\n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "print(\"accuracy: %f\" % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e539e50-70ea-4dff-b63c-756b6b51b0e2",
   "metadata": {},
   "source": [
    "## Decision trees\n",
    "\n",
    "### Your task in this exercise is to make a simple decision tree using scikit-learn's DecisionTreeClassifier on the breast cancer dataset that comes pre-loaded with scikit-learn.\n",
    "\n",
    "### This dataset contains numeric measurements of various dimensions of individual tumors (such as perimeter and texture) from breast biopsies and a single outcome value (the tumor is either malignant, or benign).\n",
    "\n",
    "### We've preloaded the dataset of samples (measurements) into X and the target values per tumor into y. Now, you have to split the complete dataset into training and testing sets, and then train a DecisionTreeClassifier. You'll specify a parameter called max_depth. Many other parameters can be modified within this model, and you can check all of them out here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0252ba5-5e12-4591-a3cb-454a5fe303d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((569, 30), (569,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision trees\n",
    "\n",
    "# Import the necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import datasets\n",
    "\n",
    "bc = datasets.load_breast_cancer()\n",
    "X = bc.data\n",
    "y = bc.target\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0510ea20-a54f-4aa2-a201-8a3b8c3b4f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Create the training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, random_state=123)\n",
    "\n",
    "# Instantiate the classifier: dt_clf_4\n",
    "dt_clf_4 = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "dt_clf_4.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred_4\n",
    "y_pred_4 = dt_clf_4.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the predictions: accuracy\n",
    "accuracy = float(np.sum(y_pred_4==y_test))/y_test.shape[0]\n",
    "print(\"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df781b1-6a47-469a-833f-f557b8ff39ae",
   "metadata": {},
   "source": [
    "## Measuring accuracy\n",
    "\n",
    "### You'll now practice using XGBoost's learning API through its baked in cross-validation capabilities. As Sergey discussed in the previous video, XGBoost gets its lauded performance and efficiency gains by utilizing its own optimized data structure for datasets called a DMatrix.\n",
    "\n",
    "### In the previous exercise, the input datasets were converted into DMatrix data on the fly, but when you use the xgboost cv object, you have to first explicitly convert your data into a DMatrix. So, that's what you will do here before running cross-validation on churn_data.\n",
    "\n",
    "-    Create a DMatrix called churn_dmatrix from churn_data using xgb.DMatrix(). The features are available in X and the labels in y.\n",
    "-    Perform 3-fold cross-validation by calling xgb.cv(). dtrain is your churn_dmatrix, params is your parameter dictionary, nfold is the number of cross-validation folds (3), num_boost_round is the number of trees we want to build (5), metrics is the metric you want to compute (this will be \"error\", which we will convert to an accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bb0de14-d546-4944-8865-e6a056f3059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Found the churn_data, elsewhere\n",
    "churn_data = pd.read_csv(\"../../data/churn_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8b93969-7d66-45ed-a00a-f82427e69b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-error-mean  train-error-std  test-error-mean  test-error-std\n",
      "0           0.29625         0.017060          0.29986        0.016645\n",
      "1           0.26915         0.002776          0.27164        0.002463\n",
      "2           0.25611         0.002730          0.25720        0.003823\n",
      "3           0.25160         0.002481          0.25546        0.001942\n",
      "4           0.24764         0.002331          0.24940        0.000435\n",
      "0.750600006600612\n"
     ]
    }
   ],
   "source": [
    "# Create arrays for the features and the target: X, y\n",
    "X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
    "\n",
    "# Create the DMatrix from X and y: churn_dmatrix\n",
    "churn_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary: params\n",
    "params = {\"objective\":\"reg:logistic\", \"max_depth\":3} \n",
    "\n",
    "# Perform cross-validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, \n",
    "                  nfold=3, num_boost_round=5, \n",
    "                  metrics=\"error\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the accuracy\n",
    "print(((1-cv_results[\"test-error-mean\"]).iloc[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec9c148-d75d-40b5-bd20-e63814462c87",
   "metadata": {},
   "source": [
    "## Measuring AUC\n",
    "\n",
    "### Now that you've used cross-validation to compute average out-of-sample accuracy (after converting from an error), it's very easy to compute any other metric you might be interested in. All you have to do is pass it (or a list of metrics) in as an argument to the metrics parameter of xgb.cv().\n",
    "\n",
    "### Your job in this exercise is to compute another common metric used in binary classification - the area under the curve (\"auc\"). As before, churn_data is available in your workspace, along with the DMatrix churn_dmatrix and parameter dictionary params.\n",
    "\n",
    "-    Perform 3-fold cross-validation with 5 boosting rounds and \"auc\" as your metric.\n",
    "-    Print the \"test-auc-mean\" column of cv_results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1f623b4-19cf-469f-b093-d9ec09a3ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_data = pd.read_csv(\"../../data/churn_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adff56ea-3644-449f-9b30-d96f34c0af79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n",
      "0        0.768844       0.001476       0.767757      0.002737\n",
      "1        0.790677       0.006640       0.788928      0.006727\n",
      "2        0.815537       0.003857       0.814085      0.005832\n",
      "3        0.822564       0.001682       0.821239      0.003626\n",
      "4        0.827401       0.000886       0.826085      0.002024\n",
      "0.8260849495138259\n"
     ]
    }
   ],
   "source": [
    "# Perform cross_validation: cv_results\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params, \n",
    "                  nfold=3, num_boost_round=5, \n",
    "                  metrics=\"auc\", as_pandas=True, seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the AUC\n",
    "print((cv_results[\"test-auc-mean\"]).iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a9cd97-8f8a-4a2b-859c-918e3b51316f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
