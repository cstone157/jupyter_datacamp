{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00af1d21-55db-4d23-9042-944b4a579c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, PowerTransformer, StandardScaler\n",
    "\n",
    "so_survey_csv = 'https://assets.datacamp.com/production/repositories/3752/datasets/19699a2441073ad6459bf5e3e17690e2cae86cf1/Combined_DS_v10.csv'\n",
    "so_survey_df = pd.read_csv(so_survey_csv)\n",
    "\n",
    "speech_df = pd.read_csv(\"../../data/inaugural_speeches.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2979b89f-fc2f-4500-9e30-4970c1380515",
   "metadata": {},
   "source": [
    "## Cleaning up your text\n",
    "### Unstructured text data cannot be directly used in most analyses. Multiple steps need to be taken to go from a long free form string to a set of numeric columns in the right format that can be ingested by a machine learning model. The first step of this process is to standardize the data and eliminate any characters that could cause problems later on in your analytic pipeline.\n",
    "\n",
    "### In this chapter you will be working with a new dataset containing the inaugural speeches of the presidents of the United States loaded as speech_df, with the speeches stored in the text column.\n",
    "\n",
    "### Instructions 1/2\n",
    "-    Print the first 5 rows of the text column to see the free text fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1248abaf-8780-440c-9468-424b5d1bf813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Fellow-Citizens of the Senate and of the House...\n",
      "1    Fellow Citizens:  I AM again called upon by th...\n",
      "2    WHEN it was first perceived, in early times, t...\n",
      "3    Friends and Fellow-Citizens:  CALLED upon to u...\n",
      "4    PROCEEDING, fellow-citizens, to that qualifica...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Print the first 5 rows of the text column\n",
    "print(speech_df[\"text\"].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfec14c-5683-42e0-8edc-403b25eb90f4",
   "metadata": {},
   "source": [
    "### Instructions 2/2\n",
    "-    Replace all non letter characters in the text column with a whitespace.\n",
    "-    Make all characters in the newly created text_clean column lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81022a62-13ca-4e15-9291-94faea990eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    fellow-citizens of the senate and of the house...\n",
      "1    fellow citizens:  i am again called upon by th...\n",
      "2    when it was first perceived, in early times, t...\n",
      "3    friends and fellow-citizens:  called upon to u...\n",
      "4    proceeding, fellow-citizens, to that qualifica...\n",
      "Name: text_clean, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Replace all non letter characters with a whitespace\n",
    "speech_df['text_clean'] = speech_df['text'].str.replace('[^a-zA-Z]', ' ')\n",
    "\n",
    "# Change to lower case\n",
    "speech_df['text_clean'] = speech_df['text_clean'].str.lower()\n",
    "\n",
    "# Print the first 5 rows of the text_clean column\n",
    "print(speech_df['text_clean'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9291ff-9829-4b70-8008-3eae7bad0a3b",
   "metadata": {},
   "source": [
    "## High level text features\n",
    "### Once the text has been cleaned and standardized you can begin creating features from the data. The most fundamental information you can calculate about free form text is its size, such as its length and number of words. In this exercise (and the rest of this chapter), you will focus on the cleaned/transformed text column (text_clean) you created in the last exercise.\n",
    "\n",
    "### Instructions\n",
    "-    Record the character length of each speech in the char_count column.\n",
    "-    Record the word count of each speech in the word_count column.\n",
    "-    Record the average word length of each speech in the avg_word_length column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c42e4d41-9012-4f48-a949-6267d061770d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           text_clean  char_cnt  word_cnt  \\\n",
      "0   fellow-citizens of the senate and of the house...      8616      1427   \n",
      "1   fellow citizens:  i am again called upon by th...       787       135   \n",
      "2   when it was first perceived, in early times, t...     13871      2317   \n",
      "3   friends and fellow-citizens:  called upon to u...     10144      1717   \n",
      "4   proceeding, fellow-citizens, to that qualifica...     12902      2157   \n",
      "5   unwilling to depart from examples of the most ...      7003      1173   \n",
      "6   about to add the solemnity of an oath to the o...      7148      1210   \n",
      "7   i should be destitute of feeling if i was not ...     19894      3367   \n",
      "8   fellow-citizens:  i shall not attempt to descr...     26322      4462   \n",
      "9   in compliance with an usage coeval with the ex...     17753      2907   \n",
      "10  fellow-citizens:  about to undertake the arduo...      6818      1124   \n",
      "11  fellow-citizens:  the will of the american peo...      7061      1171   \n",
      "12  fellow-citizens: the practice of all my predec...     23527      3884   \n",
      "13  called from a retirement which i had supposed ...     32706      5542   \n",
      "14  fellow-citizens:  without solicitation on my p...     28739      4792   \n",
      "15  elected by the american people to the highest ...      6599      1087   \n",
      "16  my countrymen:  it a relief to feel that no he...     20089      3328   \n",
      "17  fellow-citizens:  i appear before you this day...     16820      2820   \n",
      "18  fellow-citizens of the united states:  in comp...     21032      3625   \n",
      "19  fellow-countrymen:    at this second appearing...      3934       698   \n",
      "20  citizens of the united states:  your suffrages...      6521      1121   \n",
      "21  fellow-citizens:  under providence i have been...      7736      1334   \n",
      "22  fellow-citizens:  we have assembled to repeat ...     14969      2472   \n",
      "23  fellow-citizens:  we stand to-day upon an emin...     17774      2973   \n",
      "24  fellow-citizens:  in the presence of this vast...     10155      1677   \n",
      "25  fellow-citizens:  there is no constitutional o...     26175      4384   \n",
      "26  my fellow-citizens:  in obedience of the manda...     12340      2013   \n",
      "27  fellow-citizens:  in obedience to the will of ...     23691      3957   \n",
      "28  my fellow-citizens:  when we assembled here on...     13426      2216   \n",
      "29  my fellow-citizens, no people on earth have mo...      5565       983   \n",
      "30  my fellow-citizens:  anyone who has taken the ...     32160      5430   \n",
      "31  there has been a change of government. it bega...      9554      1699   \n",
      "32  my fellow citizens:  the four years which have...      8402      1525   \n",
      "33  my countrymen:  when one surveys the world abo...     20294      3325   \n",
      "34  my countrymen:  no one can contemplate current...     23937      4054   \n",
      "35  my countrymen:  this occasion is not alone the...     22961      3745   \n",
      "36  i am certain that my fellow americans expect t...     10910      1875   \n",
      "37  when four years ago we met to inaugurate a pre...     10629      1799   \n",
      "38  on each national day of inauguration since    ...      7674      1316   \n",
      "39  mr. chief justice, mr. vice president, my frie...      3086       543   \n",
      "40  mr. vice president, mr. chief justice, and fel...     13707      2262   \n",
      "41  my friends, before i begin the expression of t...     14003      2449   \n",
      "42  the price of peace mr. chairman, mr. vice pres...      9277      1635   \n",
      "43  vice president johnson, mr. speaker, mr. chief...      7706      1339   \n",
      "44  my fellow countrymen, on this occasion, the oa...      8242      1470   \n",
      "45  senator dirksen, mr. chief justice, mr. vice p...     11701      2102   \n",
      "46  mr. vice president, mr. speaker, mr. chief jus...     10048      1783   \n",
      "47  for myself and for our nation, i want to thank...      6934      1216   \n",
      "48  senator hatfield, mr. chief justice, mr. presi...     13787      2422   \n",
      "49  senator mathias, chief justice burger, vice pr...     14601      2545   \n",
      "50  mr. chief justice, mr. president, vice preside...     12536      2306   \n",
      "51  my fellow citizens:today we celebrate the myst...      9119      1568   \n",
      "52  my fellow citizens:at this last presidential i...     12374      2143   \n",
      "53  president clinton, distinguished guests and my...      9084      1581   \n",
      "54  vice president cheney, mr. chief justice, pres...     12199      2056   \n",
      "55  my fellow citizens:    i stand here today humb...     13637      2371   \n",
      "56  vice president biden, mr. chief justice, membe...     12174      2072   \n",
      "57  chief justice roberts, president carter, presi...      8555      1452   \n",
      "\n",
      "    avg_word_length  \n",
      "0          6.037842  \n",
      "1          5.829630  \n",
      "2          5.986621  \n",
      "3          5.907979  \n",
      "4          5.981456  \n",
      "5          5.970162  \n",
      "6          5.907438  \n",
      "7          5.908524  \n",
      "8          5.899148  \n",
      "9          6.106983  \n",
      "10         6.065836  \n",
      "11         6.029889  \n",
      "12         6.057415  \n",
      "13         5.901480  \n",
      "14         5.997287  \n",
      "15         6.070837  \n",
      "16         6.036358  \n",
      "17         5.964539  \n",
      "18         5.801931  \n",
      "19         5.636103  \n",
      "20         5.817128  \n",
      "21         5.799100  \n",
      "22         6.055421  \n",
      "23         5.978473  \n",
      "24         6.055456  \n",
      "25         5.970575  \n",
      "26         6.130154  \n",
      "27         5.987111  \n",
      "28         6.058664  \n",
      "29         5.661241  \n",
      "30         5.922652  \n",
      "31         5.623308  \n",
      "32         5.509508  \n",
      "33         6.103459  \n",
      "34         5.904539  \n",
      "35         6.131108  \n",
      "36         5.818667  \n",
      "37         5.908282  \n",
      "38         5.831307  \n",
      "39         5.683241  \n",
      "40         6.059682  \n",
      "41         5.717844  \n",
      "42         5.674006  \n",
      "43         5.755041  \n",
      "44         5.606803  \n",
      "45         5.566603  \n",
      "46         5.635446  \n",
      "47         5.702303  \n",
      "48         5.692403  \n",
      "49         5.737132  \n",
      "50         5.436253  \n",
      "51         5.815689  \n",
      "52         5.774148  \n",
      "53         5.745731  \n",
      "54         5.933366  \n",
      "55         5.751582  \n",
      "56         5.875483  \n",
      "57         5.891873  \n"
     ]
    }
   ],
   "source": [
    "# Find the length of each text\n",
    "speech_df['char_cnt'] = speech_df['text_clean'].str.len()\n",
    "\n",
    "# Count the number of words in each text\n",
    "speech_df['word_cnt'] = speech_df['text_clean'].str.split().str.len()\n",
    "\n",
    "# Find the average length of word\n",
    "speech_df['avg_word_length'] = speech_df['char_cnt'] / speech_df['word_cnt']\n",
    "\n",
    "# Print the first 5 rows of these columns\n",
    "print(speech_df[['text_clean', 'char_cnt', 'word_cnt', 'avg_word_length']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03491df1-587b-46c7-b5e4-8c11e90c1bf8",
   "metadata": {},
   "source": [
    "## Counting words (I)\n",
    "### Once high level information has been recorded you can begin creating features based on the actual content of each text. One way to do this is to approach it in a similar way to how you worked with categorical variables in the earlier lessons.\n",
    "\n",
    "### For each unique word in the dataset a column is created.\n",
    "### For each entry, the number of times this word occurs is counted and the count value is entered into the respective column.\n",
    "### These \"count\" columns can then be used to train machine learning models.\n",
    "\n",
    "### Instructions\n",
    "-    Import CountVectorizer from sklearn.feature_extraction.text.\n",
    "-    Instantiate CountVectorizer and assign it to cv.\n",
    "-    Fit the vectorizer to the text_clean column.\n",
    "-    Print the feature names generated by the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b726f9b4-ad8c-4e3f-b02e-56fdf9526054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0085' '0092' '0093' ... 'zealous' 'zealously' 'zone']\n"
     ]
    }
   ],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer\n",
    "cv.fit(speech_df['text_clean'])\n",
    "\n",
    "# Print feature names\n",
    "#print(cv.get_feature_names()) ## Function doesn't seem to be there\n",
    "print(cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc5b95c-125e-457e-b728-7503681f8e94",
   "metadata": {},
   "source": [
    "## Counting words (II)\n",
    "### Once the vectorizer has been fit to the data, it can be used to transform the text to an array representing the word counts. This array will have a row per block of text and a column for each of the features generated by the vectorizer that you observed in the last exercise.\n",
    "\n",
    "### The vectorizer to you fit in the last exercise (cv) is available in your workspace.\n",
    "\n",
    "### Instructions 1/2\n",
    "-    Apply the vectorizer to the text_clean column.\n",
    "-    Convert this transformed (sparse) array into a numpy array with counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c603720a-6874-48ba-b4d0-66c6dfa0e0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " ...\n",
      " [ 3 12  1 ...  0  0  0]\n",
      " [ 0 12  1 ...  0  0  0]\n",
      " [ 0 10  0 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# Apply the vectorizer\n",
    "cv_transformed = cv.transform(speech_df['text_clean'])\n",
    "\n",
    "# Print the full array\n",
    "cv_array = cv_transformed.toarray()\n",
    "print(cv_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33d832d-cc19-4680-b659-5ed7f57201db",
   "metadata": {},
   "source": [
    "### Instructions 2/2\n",
    "-    Print the dimensions of this numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ce1ff6f-90bf-4fa4-aef1-1c97fb2ced4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58, 9048)\n"
     ]
    }
   ],
   "source": [
    "# Apply the vectorizer\n",
    "cv_transformed = cv.transform(speech_df['text_clean'])\n",
    "\n",
    "# Print the full array\n",
    "cv_array = cv_transformed.toarray()\n",
    "\n",
    "# Print the shape of cv_array\n",
    "print(cv_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e37174-9757-4ea9-950c-9ba76a6ada5e",
   "metadata": {},
   "source": [
    "## Limiting your features\n",
    "### As you have seen, using the CountVectorizer with its default settings creates a feature for every single word in your corpus. This can create far too many features, often including ones that will provide very little analytical value.\n",
    "\n",
    "### For this purpose CountVectorizer has parameters that you can set to reduce the number of features:\n",
    "\n",
    "### -    min_df : Use only words that occur in more than this percentage of documents. This can be used to remove outlier words that will not generalize across texts.\n",
    "### -    max_df : Use only words that occur in less than this percentage of documents. This is useful to eliminate very common words that occur in every corpus without adding value such as \"and\" or \"the\".\n",
    "### Instructions\n",
    "-    Limit the number of features in the CountVectorizer by setting the minimum number of documents a word can appear to 20% and the maximum to 80%.\n",
    "-    Fit and apply the vectorizer on text_clean column in one step.\n",
    "-    Convert this transformed (sparse) array into a numpy array with counts.\n",
    "-    Print the dimensions of the new reduced array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8e2f2c1-7c48-40cd-a394-9c74b2eb049d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(58, 818)\n"
     ]
    }
   ],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Specify arguements to limit the number of features generated\n",
    "cv = CountVectorizer(min_df=0.2, max_df=0.8)\n",
    "\n",
    "# Fit, transform, and convert into array\n",
    "cv_transformed = cv.fit_transform(speech_df['text_clean'])\n",
    "cv_array = cv_transformed.toarray()\n",
    "\n",
    "# Print the array shape\n",
    "print(cv_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c771989-8a4d-4c25-a947-65ac0b3a448c",
   "metadata": {},
   "source": [
    "## Text to DataFrame\n",
    "### Now that you have generated these count based features in an array you will need to reformat them so that they can be combined with the rest of the dataset. This can be achieved by converting the array into a pandas DataFrame, with the feature names you found earlier as the column names, and then concatenate it with the original DataFrame.\n",
    "\n",
    "### The numpy array (cv_array) and the vectorizer (cv) you fit in the last exercise are available in your workspace.\n",
    "\n",
    "### Instructions\n",
    "-    Create a DataFrame cv_df containing the cv_array as the values and the feature names as the column names.\n",
    "-    Add the prefix Counts_ to the column names for ease of identification.\n",
    "-    Concatenate this DataFrame (cv_df) to the original DataFrame (speech_df) column wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00fd9cf8-706a-45d1-8c7d-3d87e465dedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Name         Inaugural Address                      Date  \\\n",
      "0  George Washington   First Inaugural Address  Thursday, April 30, 1789   \n",
      "1  George Washington  Second Inaugural Address     Monday, March 4, 1793   \n",
      "2         John Adams         Inaugural Address   Saturday, March 4, 1797   \n",
      "3   Thomas Jefferson   First Inaugural Address  Wednesday, March 4, 1801   \n",
      "4   Thomas Jefferson  Second Inaugural Address     Monday, March 4, 1805   \n",
      "\n",
      "                                                text  \\\n",
      "0  Fellow-Citizens of the Senate and of the House...   \n",
      "1  Fellow Citizens:  I AM again called upon by th...   \n",
      "2  WHEN it was first perceived, in early times, t...   \n",
      "3  Friends and Fellow-Citizens:  CALLED upon to u...   \n",
      "4  PROCEEDING, fellow-citizens, to that qualifica...   \n",
      "\n",
      "                                          text_clean  char_cnt  word_cnt  \\\n",
      "0  fellow-citizens of the senate and of the house...      8616      1427   \n",
      "1  fellow citizens:  i am again called upon by th...       787       135   \n",
      "2  when it was first perceived, in early times, t...     13871      2317   \n",
      "3  friends and fellow-citizens:  called upon to u...     10144      1717   \n",
      "4  proceeding, fellow-citizens, to that qualifica...     12902      2157   \n",
      "\n",
      "   avg_word_length  Counts_abiding  Counts_ability  ...  Counts_women  \\\n",
      "0         6.037842               0               0  ...             0   \n",
      "1         5.829630               0               0  ...             0   \n",
      "2         5.986621               0               0  ...             0   \n",
      "3         5.907979               0               0  ...             0   \n",
      "4         5.981456               0               0  ...             0   \n",
      "\n",
      "   Counts_words  Counts_work  Counts_wrong  Counts_year  Counts_years  \\\n",
      "0             0            0             0            0             1   \n",
      "1             0            0             0            0             0   \n",
      "2             0            0             0            2             3   \n",
      "3             0            1             2            0             0   \n",
      "4             0            0             0            2             2   \n",
      "\n",
      "   Counts_yet  Counts_you  Counts_young  Counts_your  \n",
      "0           0           5             0            9  \n",
      "1           0           0             0            1  \n",
      "2           0           0             0            1  \n",
      "3           2           7             0            7  \n",
      "4           2           4             0            4  \n",
      "\n",
      "[5 rows x 826 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with these features\n",
    "#cv_df = pd.DataFrame(cv_array, \n",
    "#                     columns=cv.get_feature_names()).add_prefix('Counts_')\n",
    "cv_df = pd.DataFrame(cv_array, \n",
    "                     columns=cv.get_feature_names_out()).add_prefix('Counts_')\n",
    "\n",
    "# Add the new columns to the original DataFrame\n",
    "speech_df_new = pd.concat([speech_df, cv_df], axis=1, sort=False)\n",
    "print(speech_df_new.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44275444-e56e-4205-988d-57143354da10",
   "metadata": {},
   "source": [
    "## Tf-idf\n",
    "### While counts of occurrences of words can be useful to build models, words that occur many times may skew the results undesirably. To limit these common words from overpowering your model a form of normalization can be used. In this lesson you will be using Term frequency-inverse document frequency (Tf-idf) as was discussed in the video. Tf-idf has the effect of reducing the value of common words, while increasing the weight of words that do not occur in many documents.\n",
    "\n",
    "### Instructions\n",
    "-    Import TfidfVectorizer from sklearn.feature_extraction.text.\n",
    "-    Instantiate TfidfVectorizer while limiting the number of features to 100 and removing English stop words.\n",
    "-    Fit and apply the vectorizer on text_clean column in one step.\n",
    "-    Create a DataFrame tv_df containing the weights of the words and the feature names as the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27a094c6-2d3c-4d27-a1a3-4af6497da85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TFIDF_0092  TFIDF_0097  TFIDF_action  TFIDF_administration  TFIDF_america  \\\n",
      "0         0.0    0.047468      0.000000              0.133265       0.000000   \n",
      "1         0.0    0.000000      0.000000              0.261016       0.266097   \n",
      "2         0.0    0.021955      0.000000              0.092456       0.157092   \n",
      "3         0.0    0.131111      0.000000              0.092023       0.000000   \n",
      "4         0.0    0.028455      0.041523              0.039943       0.000000   \n",
      "\n",
      "   TFIDF_american  TFIDF_americans  TFIDF_believe  TFIDF_best  TFIDF_better  \\\n",
      "0        0.105269              0.0       0.000000    0.000000      0.000000   \n",
      "1        0.000000              0.0       0.000000    0.000000      0.000000   \n",
      "2        0.073033              0.0       0.000000    0.026118      0.060473   \n",
      "3        0.000000              0.0       0.090286    0.116980      0.045143   \n",
      "4        0.031552              0.0       0.000000    0.067701      0.039189   \n",
      "\n",
      "   ...  TFIDF_things  TFIDF_time  TFIDF_today  TFIDF_union  TFIDF_united  \\\n",
      "0  ...      0.000000    0.045877          0.0     0.135859      0.203364   \n",
      "1  ...      0.000000    0.000000          0.0     0.000000      0.199157   \n",
      "2  ...      0.032037    0.021219          0.0     0.062837      0.070544   \n",
      "3  ...      0.047831    0.000000          0.0     0.093814      0.000000   \n",
      "4  ...      0.083046    0.165008          0.0     0.122162      0.030477   \n",
      "\n",
      "   TFIDF_war  TFIDF_way  TFIDF_work  TFIDF_world  TFIDF_years  \n",
      "0   0.000000   0.060687    0.000000     0.045877     0.052635  \n",
      "1   0.000000   0.000000    0.000000     0.000000     0.000000  \n",
      "2   0.024344   0.000000    0.000000     0.063657     0.073033  \n",
      "3   0.036346   0.000000    0.038993     0.095038     0.000000  \n",
      "4   0.094657   0.000000    0.000000     0.055003     0.063105  \n",
      "\n",
      "[5 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instantiate TfidfVectorizer\n",
    "tv = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "\n",
    "# Fit the vectroizer and transform the data\n",
    "tv_transformed = tv.fit_transform(speech_df['text_clean'])\n",
    "\n",
    "# Create a DataFrame with these features\n",
    "#tv_df = pd.DataFrame(tv_transformed.toarray(), \n",
    "#                     columns=tv.get_feature_names()).add_prefix('TFIDF_')\n",
    "tv_df = pd.DataFrame(tv_transformed.toarray(), \n",
    "                     columns=tv.get_feature_names_out()).add_prefix('TFIDF_')\n",
    "print(tv_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcdf87e-bc50-4966-9dfe-ba53edd67fe6",
   "metadata": {},
   "source": [
    "## Inspecting Tf-idf values\n",
    "### After creating Tf-idf features you will often want to understand what are the most highest scored words for each corpus. This can be achieved by isolating the row you want to examine and then sorting the the scores from high to low.\n",
    "\n",
    "### The DataFrame from the last exercise (tv_df) is available in your workspace.\n",
    "\n",
    "### Instructions\n",
    "-    Assign the first row of tv_df to sample_row.\n",
    "-    sample_row is now a series of weights assigned to words. Sort these values to print the top 5 highest-rated words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d56b6cf-faf1-48a4-bcd2-70d54e3c9bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF_government    0.367016\n",
      "TFIDF_public        0.332862\n",
      "TFIDF_present       0.314827\n",
      "TFIDF_duty          0.238368\n",
      "TFIDF_country       0.229385\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Isolate the row to be examined\n",
    "sample_row = tv_df.iloc[0]\n",
    "\n",
    "# Print the top 5 words of the sorted output\n",
    "print(sample_row.sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42564548-323f-4bef-97cf-ec0dd5397287",
   "metadata": {},
   "source": [
    "## Transforming unseen data\n",
    "### When creating vectors from text, any transformations that you perform before training a machine learning model, you also need to apply on the new unseen (test) data. To achieve this follow the same approach from the last chapter: fit the vectorizer only on the training data, and apply it to the test data.\n",
    "\n",
    "### For this exercise the speech_df DataFrame has been split in two:\n",
    "\n",
    "### -    train_speech_df: The training set consisting of the first 45 speeches.\n",
    "### -    test_speech_df: The test set consisting of the remaining speeches.\n",
    "### Instructions\n",
    "-    Instantiate TfidfVectorizer.\n",
    "-    Fit the vectorizer and apply it to the text_clean column.\n",
    "-    Apply the same vectorizer on the text_clean column of the test data.\n",
    "-    Create a DataFrame of these new features from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d4177aa-6d26-4f52-9963-e2ddff549db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_speech_df = speech_df_new[:45]\n",
    "test_speech_df = speech_df_new[45:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a311252-886f-4f91-ad11-6c0682780aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TFIDF_0097  TFIDF_action  TFIDF_administration  TFIDF_america  \\\n",
      "0    0.451720      0.000000              0.026354       0.208724   \n",
      "1    0.264367      0.000000              0.000000       0.528140   \n",
      "2    0.426195      0.000000              0.000000       0.114876   \n",
      "3    0.289710      0.035503              0.064536       0.255561   \n",
      "4    0.210826      0.000000              0.000000       0.216971   \n",
      "\n",
      "   TFIDF_american  TFIDF_authority  TFIDF_best  TFIDF_business  \\\n",
      "0        0.073784         0.000000    0.000000        0.000000   \n",
      "1        0.035562         0.000000    0.034765        0.000000   \n",
      "2        0.121826         0.000000    0.119096        0.000000   \n",
      "3        0.030114         0.038275    0.058878        0.047937   \n",
      "4        0.153399         0.027853    0.085692        0.000000   \n",
      "\n",
      "   TFIDF_citizens  TFIDF_commerce  ...  TFIDF_subject  TFIDF_support  \\\n",
      "0        0.020142             0.0  ...            0.0       0.000000   \n",
      "1        0.014562             0.0  ...            0.0       0.018615   \n",
      "2        0.000000             0.0  ...            0.0       0.000000   \n",
      "3        0.073986             0.0  ...            0.0       0.094581   \n",
      "4        0.107681             0.0  ...            0.0       0.022943   \n",
      "\n",
      "   TFIDF_time  TFIDF_union  TFIDF_united  TFIDF_war  TFIDF_way  TFIDF_work  \\\n",
      "0    0.102936      0.00000      0.021990   0.070525   0.029721    0.000000   \n",
      "1    0.089301      0.00000      0.000000   0.050986   0.064459    0.076212   \n",
      "2    0.067983      0.00000      0.072616   0.038815   0.049072    0.087028   \n",
      "3    0.201654      0.00000      0.053849   0.028784   0.036390    0.225877   \n",
      "4    0.183433      0.12918      0.039187   0.020946   0.079444    0.117410   \n",
      "\n",
      "   TFIDF_world  TFIDF_years  \n",
      "0     0.267633     0.120218  \n",
      "1     0.267902     0.121676  \n",
      "2     0.203950     0.039699  \n",
      "3     0.226861     0.058878  \n",
      "4     0.293493     0.149961  \n",
      "\n",
      "[5 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate TfidfVectorizer\n",
    "tv = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "\n",
    "# Fit the vectroizer and transform the data\n",
    "tv_transformed = tv.fit_transform(train_speech_df['text_clean'])\n",
    "\n",
    "# Transform test data\n",
    "test_tv_transformed = tv.transform(test_speech_df['text_clean'])\n",
    "\n",
    "# Create new features for the test set\n",
    "#test_tv_df = pd.DataFrame(test_tv_transformed.toarray(), \n",
    "#                          columns=tv.get_feature_names()).add_prefix('TFIDF_')\n",
    "test_tv_df = pd.DataFrame(test_tv_transformed.toarray(), \n",
    "                          columns=tv.get_feature_names_out()).add_prefix('TFIDF_')\n",
    "print(test_tv_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d90aa-d0f8-4cec-b087-d00702ef6663",
   "metadata": {},
   "source": [
    "## Using longer n-grams\n",
    "### So far you have created features based on individual words in each of the texts. This can be quite powerful when used in a machine learning model but you may be concerned that by looking at words individually a lot of the context is being ignored. To deal with this when creating models you can use n-grams which are sequence of n words grouped together. For example:\n",
    "\n",
    "### -    bigrams: Sequences of two consecutive words\n",
    "### -    trigrams: Sequences of two consecutive words\n",
    "### These can be automatically created in your dataset by specifying the ngram_range argument as a tuple (n1, n2) where all n-grams in the n1 to n2 range are included.\n",
    "\n",
    "### Instructions\n",
    "-    Import CountVectorizer from sklearn.feature_extraction.text.\n",
    "-    Instantiate CountVectorizer while considering only trigrams.\n",
    "-    Fit the vectorizer and apply it to the text_clean column in one step.\n",
    "-    Print the feature names generated by the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03f71754-b5b2-4cb7-9db7-63d3fc9037cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0092 ideal freedom' 'ability preserve protect'\n",
      " 'agriculture commerce manufactures' 'america 0092 ideal'\n",
      " 'best ability preserve' 'best interests country' 'bless god bless'\n",
      " 'bless united states' 'chief justice mr' 'children children children'\n",
      " 'citizens united states' 'civil religious liberty'\n",
      " 'commerce united states' 'concern thank god' 'confidence fellow citizens'\n",
      " 'congress extraordinary session' 'constitution does expressly'\n",
      " 'constitution united states' 'coordinate branches government'\n",
      " 'day task people' 'defend constitution united' 'desire determined work'\n",
      " 'distinguished guests fellow' 'does expressly say' 'equal exact justice'\n",
      " 'era good feeling' 'executive branch government'\n",
      " 'faithfully execute office' 'fellow citizens assembled'\n",
      " 'fellow citizens called' 'fellow citizens large' 'fellow citizens world'\n",
      " 'form perfect union' 'general welfare secure' 'god bless america'\n",
      " 'god bless god' 'good greatest number' 'government united states'\n",
      " 'granted federal government' 'great body people' 'great mass people'\n",
      " 'great political parties' 'greatest good greatest'\n",
      " 'guests fellow citizens' 'interests united states' 'land new promise'\n",
      " 'laws faithfully executed' 'letter spirit constitution'\n",
      " 'liberty pursuit happiness' 'life liberty pursuit'\n",
      " 'local self government' 'make hard choices' 'men women children'\n",
      " 'mr chief justice' 'mr vice president' 'nation like person'\n",
      " 'new breeze blowing' 'new jobs new' 'new level respect'\n",
      " 'new order things' 'new retreat responsibilities' 'new states admitted'\n",
      " 'north south east' 'oath prescribed constitution'\n",
      " 'office president united' 'peace shall strive' 'peace unless america'\n",
      " 'peace world peace' 'peaceful settlement disputes' 'people united states'\n",
      " 'policy united states' 'power general government'\n",
      " 'preserve protect defend' 'president united states'\n",
      " 'president vice president' 'promote general welfare'\n",
      " 'proof confidence fellow' 'protect defend constitution'\n",
      " 'protection great interests' 'reform civil service'\n",
      " 'republican model government' 'require thee justly'\n",
      " 'reserved states people' 'secure blessings liberty'\n",
      " 'soldiers sailors widows' 'south east west'\n",
      " 'sovereignty general government' 'states admitted union'\n",
      " 'taken oath office' 'territories united states' 'thank god bless'\n",
      " 'turning away old' 'united states 0097' 'united states america'\n",
      " 'united states best' 'united states government' 'united states great'\n",
      " 'united states maintain' 'vice president mr' 'welfare secure blessings']\n"
     ]
    }
   ],
   "source": [
    "# Import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Instantiate a trigram vectorizer\n",
    "cv_trigram_vec = CountVectorizer(max_features=100, \n",
    "                                 stop_words='english', \n",
    "                                 ngram_range = (3,3))\n",
    "\n",
    "# Fit and apply trigram vectorizer\n",
    "cv_trigram = cv_trigram_vec.fit_transform(speech_df['text_clean'])\n",
    "\n",
    "# Print the trigram features\n",
    "#print(cv_trigram_vec.get_feature_names())\n",
    "print(cv_trigram_vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0ddc57-9506-42d5-9cca-0ea1d49f3125",
   "metadata": {},
   "source": [
    "## Finding the most common words\n",
    "### Its always advisable once you have created your features to inspect them to ensure that they are as you would expect. This will allow you to catch errors early, and perhaps influence what further feature engineering you will need to do.\n",
    "\n",
    "### The vectorizer (cv) you fit in the last exercise and the sparse array consisting of word counts (cv_trigram) is available in your workspace.\n",
    "\n",
    "### Instructions\n",
    "-    Create a DataFrame of the features (word counts).\n",
    "-    Add the counts of word occurrences and print the top 5 most occurring words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a5fa8ca-457a-4cba-a0cb-091b4fe7d5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts_constitution united states    20\n",
      "Counts_people united states          13\n",
      "Counts_mr chief justice              10\n",
      "Counts_preserve protect defend       10\n",
      "Counts_president united states        8\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame of the features\n",
    "#cv_tri_df = pd.DataFrame(cv_trigram.toarray(), \n",
    "#                 columns=cv_trigram_vec.get_feature_names()).add_prefix('Counts_')\n",
    "cv_tri_df = pd.DataFrame(cv_trigram.toarray(), \n",
    "                 columns=cv_trigram_vec.get_feature_names_out()).add_prefix('Counts_')\n",
    "\n",
    "# Print the top 5 words in the sorted output\n",
    "print(cv_tri_df.sum().sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a957ce-6546-41f9-a90a-05851d6e01e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
