{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa8d4e15-332b-471e-b77d-63e1e87a16a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from itertools import product\n",
    "import random\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../../data/credit-card-full.csv\")\n",
    "X = df[['LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3',\n",
    "       'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']]\n",
    "y = df[['SEX']].values.ravel()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b22956a-bdc8-4537-9644-5e282c373ad1",
   "metadata": {},
   "source": [
    "## Visualizing Coarse to Fine\n",
    "### You're going to undertake the first part of a Coarse to Fine search. This involves analyzing the results of an initial random search that took place over a large search space, then deciding what would be the next logical step to make your hyperparameter search finer.\n",
    "\n",
    "### You have available:\n",
    "\n",
    "##### - combinations_list - a list of the possible hyperparameter combinations the random search was undertaken on.\n",
    "##### - results_df - a DataFrame that has each hyperparameter combination and the resulting accuracy of all 500 trials. Each hyperparameter is a column, with the header the hyperparameter name.\n",
    "##### - visualize_hyperparameter() - a function that takes in a column of the DataFrame (as a string) and produces a scatter plot of this column's values compared to the accuracy scores. An example call of the function would be visualize_hyperparameter('accuracy')\n",
    "##### - If you wish to view the visualize_hyperparameter() function definition, you can run this code:\n",
    "\n",
    "<code>\n",
    "import inspect\n",
    "print(inspect.getsource(visualize_hyperparameter))\n",
    "</code>\n",
    "\n",
    "### Instructions\n",
    "-    Confirm (by printing out) the size of the combinations_list, justifying the need to start with a random search.\n",
    "-    Sort the results_df by accuracy values and print the top 10 rows. Are there clear insights? Beware a small sample size!\n",
    "-    Confirm (by printing out) which hyperparameters were used in this search. These are the column names in results_df.\n",
    "-    Call visualize_hyperparameter() with each hyperparameter in turn (max_depth, min_samples_leaf, learn_rate). Are there any trends?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f4ac3c0-0795-48f8-b3c1-9d634d161ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Once again Data-Camp can't be bothered to share their code, quality work guys.\n",
    "\n",
    "# Confirm the size of the combinations_list\n",
    "#print(len(combinations_list))\n",
    "\n",
    "# Sort the results_df by accuracy and print the top 10 rows\n",
    "#print(results_df.sort_values(by='accuracy', ascending=False).head(10))\n",
    "\n",
    "# Confirm which hyperparameters were used in this search\n",
    "#print(results_df.columns)\n",
    "\n",
    "# Call visualize_hyperparameter() with each hyperparameter in turn\n",
    "#visualize_hyperparameter('max_depth')\n",
    "#visualize_hyperparameter('min_samples_leaf')\n",
    "#visualize_hyperparameter('learn_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957a43d0-1443-40da-b567-d5bb9bf48a31",
   "metadata": {},
   "source": [
    "## Coarse to Fine Iterations\n",
    "### You will now visualize the first random search undertaken, construct a tighter grid and check the results. You will have available:\n",
    "\n",
    "##### - results_df - a DataFrame that has the hyperparameter combination and the resulting accuracy of all 500 trials. Only the hyperparameters that had the strongest visualizations from the previous exercise are included (max_depth and learn_rate)\n",
    "##### - visualize_first() - This function takes no arguments but will visualize each of your hyperparameters against accuracy for your first random search.\n",
    "### If you wish to view the visualize_first() (or the visualize_second()) function definition, you can run this code:\n",
    "\n",
    "<code>\n",
    "import inspect\n",
    "print(inspect.getsource(visualize_first))\n",
    "</code>\n",
    "\n",
    "### Instructions 1/3\n",
    "-    Use the visualize_first() function to check the values of max_depth and learn_rate that tend to perform better. A convenient red line will be added to make this explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c2e5844-3656-4272-b933-3e51751e8c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the provided function to visualize the first results\n",
    "#visualize_first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9d4c2c-fff1-41ca-be63-4c9433a1f93a",
   "metadata": {},
   "source": [
    "### Instructions 2/3\n",
    "-    Now create a more narrow grid search, testing for max_depth values between 1 and 20 and for 50 learning rates between 0.001 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f3b8834-5f67-47a3-a126-776c919df8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the provided function to visualize the first results\n",
    "# visualize_first()\n",
    "\n",
    "# Create some combinations lists & combine\n",
    "#max_depth_list = list(range(1, 21))\n",
    "#learn_rate_list = np.linspace(0.001, 1, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31ed77e-e80d-4b94-a22f-7ec995e0c266",
   "metadata": {},
   "source": [
    "### Instructions 3/3\n",
    "-    We ran the 1,000 model grid search in the background based on those new combinations. Now use the visualize_second() function to visualize the second iteration (grid search) and see if there is any improved results. This function takes no arguments, just run it in-place to generate the plots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd24a6dd-17d4-4d1e-b9e0-88a321aa41d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the provided function to visualize the first results\n",
    "# visualize_first()\n",
    "\n",
    "# Create some combinations lists & combine:\n",
    "#max_depth_list = list(range(1,21))\n",
    "#learn_rate_list = np.linspace(0.001,1,50)\n",
    "\n",
    "# Call the function to visualize the second results\n",
    "#visualize_second()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69274ffe-5bf4-4912-b5e0-326d3f39c414",
   "metadata": {},
   "source": [
    "## Bayes Rule in Python\n",
    "### In this exercise you will undertake a practical example of setting up Bayes formula, obtaining new evidence and updating your 'beliefs' in order to get a more accurate result. The example will relate to the likelihood that someone will close their account for your online software product.\n",
    "\n",
    "### These are the probabilities we know:\n",
    "\n",
    "##### - 7% (0.07) of people are likely to close their account next month\n",
    "##### - 15% (0.15) of people with accounts are unhappy with your product (you don't know who though!)\n",
    "##### - 35% (0.35) of people who are likely to close their account are unhappy with your product\n",
    "### Instructions 1/3\n",
    "-    Assign the different probabilities (as decimals) to variables. p_unhappy is the likelihood someone is unhappy, p_unhappy_close is the probability that someone is unhappy with the product, given they are going to close their account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5db8809-edc3-4f09-bde9-459d96879e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign probabilities to variables \n",
    "p_unhappy = 0.15\n",
    "p_unhappy_close = 0.35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f80dc-b90c-4e43-b437-001de9136f01",
   "metadata": {},
   "source": [
    "### Instructions 2/3\n",
    "-    Assign the probability that someone will close their account next month to the variable p_close as a decimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13aa8324-0f1c-4691-907a-a266e0757b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign probabilities to variables \n",
    "p_unhappy = 0.15\n",
    "p_unhappy_close = 0.35\n",
    "\n",
    "# Probabiliy someone will close\n",
    "p_close = 0.07"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8050a117-69c1-426a-9598-9ddff00570e7",
   "metadata": {},
   "source": [
    "### Instructions 3/3\n",
    "-    You interview one of your customers and discover they are unhappy. What is the probability they will close their account, now that you know this evidence? Assign the result to p_close_unhappy and print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9a36268-e50f-48c2-94d1-324338022f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16333333333333336\n"
     ]
    }
   ],
   "source": [
    "# Assign probabilities to variables \n",
    "p_unhappy = 0.15\n",
    "p_unhappy_close = 0.35\n",
    "\n",
    "# Probabiliy someone will close\n",
    "p_close = 0.07\n",
    "\n",
    "# Probability unhappy person will close\n",
    "p_close_unhappy = (p_close * p_unhappy_close) / p_unhappy\n",
    "print(p_close_unhappy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9310997-282d-49b8-8f82-63774ce39151",
   "metadata": {},
   "source": [
    "## Bayesian Hyperparameter tuning with Hyperopt\n",
    "### In this example you will set up and run a Bayesian hyperparameter optimization process using the package Hyperopt (already imported as hp for you). You will set up the domain (which is similar to setting up the grid for a grid search), then set up the objective function. Finally, you will run the optimizer over 20 iterations.\n",
    "\n",
    "### You will need to set up the domain using values:\n",
    "\n",
    "##### - max_depth using quniform distribution (between 2 and 10, increasing by 2)\n",
    "##### - learning_rate using uniform distribution (0.001 to 0.9)\n",
    "### Note that for the purpose of this exercise, this process was reduced in data sample size and hyperopt & GBM iterations. If you are trying out this method by yourself on your own machine, try a larger search space, more trials, more cvs and a larger dataset size to really see this in action!\n",
    "\n",
    "### Instructions\n",
    "-    Set up a space dictionary using the domain mentioned above.\n",
    "-    Set up the objective function using a gradient boosting classifier.\n",
    "-    Run the algorithm for 20 evaluations (just use the default, suggested algorithm from the slides)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dfce2b7-057c-472f-8ccc-a56af9c87869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install hyperopt\n",
    "from hyperopt import fmin, tpe, hp\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "630ebc2f-03ed-4c4c-85e7-dc44167e2cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:12<00:00,  6.61s/trial, best loss: 0.37626865671641796]\n",
      "{'learning_rate': 0.06722126188002117, 'max_depth': 4.0}\n"
     ]
    }
   ],
   "source": [
    "# Set up space dictionary with specified hyperparameters\n",
    "space = {'max_depth': hp.quniform('max_depth', 2, 10, 2),'learning_rate': hp.uniform('learning_rate', 0.001,0.9)}\n",
    "\n",
    "# Set up objective function\n",
    "def objective(params):\n",
    "    params = {'max_depth': int(params['max_depth']),'learning_rate': params['learning_rate']}\n",
    "    gbm_clf = GradientBoostingClassifier(n_estimators=100, **params) \n",
    "    best_score = cross_val_score(gbm_clf, X_train, y_train, scoring='accuracy', cv=2, n_jobs=4).mean()\n",
    "    loss = 1 - best_score\n",
    "    return loss\n",
    "\n",
    "# Run the algorithm\n",
    "best = fmin(fn=objective, space=space, max_evals=20, rstate=np.random.default_rng(42), algo=tpe.suggest)\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d3e998-8c19-448a-9768-c555984fd601",
   "metadata": {},
   "source": [
    "## Genetic Hyperparameter Tuning with TPOT\n",
    "### You're going to undertake a simple example of genetic hyperparameter tuning. TPOT is a very powerful library that has a lot of features. You're just scratching the surface in this lesson, but you are highly encouraged to explore in your own time.\n",
    "\n",
    "### This is a very small example. In real life, TPOT is designed to be run for many hours to find the best model. You would have a much larger population and offspring size as well as hundreds more generations to find a good model.\n",
    "\n",
    "### You will create the estimator, fit the estimator to the training data and then score this on the test data.\n",
    "\n",
    "### For this example we wish to use:\n",
    "\n",
    "##### - 3 generations\n",
    "##### - 4 in the population size\n",
    "##### - 3 offspring in each generation\n",
    "##### - accuracy for scoring\n",
    "### A random_state of 2 has been set for consistency of results.\n",
    "\n",
    "### Instructions\n",
    "-    Assign the values outlined in the context to the inputs for tpot_clf.\n",
    "-    Create the tpot_clf classifier with the correct inputs.\n",
    "-    Fit the classifier to the training data (X_train & y_train are available in your workspace).\n",
    "-    Use the fitted classifier to score on the test set (X_test & y_test are available in your workspace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df6ea87f-92ed-4e6d-928c-e3ce31e51076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tpot/builtins/__init__.py:36: UserWarning: Warning: optional dependency `torch` is not available. - skipping import of NN models.\n",
      "  warnings.warn(\"Warning: optional dependency `torch` is not available. - skipping import of NN models.\")\n"
     ]
    }
   ],
   "source": [
    "# pip install tpot\n",
    "from tpot import TPOTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "476a72f0-69bd-47b9-87ff-da312e387dc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/13 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.6142786069651742\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.6142786069651742\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.6142786069651742\n",
      "\n",
      "Best pipeline: DecisionTreeClassifier(input_matrix, criterion=gini, max_depth=3, min_samples_leaf=10, min_samples_split=9)\n",
      "0.6171717171717171\n"
     ]
    }
   ],
   "source": [
    "# Assign the values outlined to the inputs\n",
    "number_generations = 3\n",
    "population_size = 4\n",
    "offspring_size = 3\n",
    "scoring_function = 'accuracy'\n",
    "\n",
    "# Create the tpot classifier\n",
    "tpot_clf = TPOTClassifier(generations=number_generations, population_size=population_size,\n",
    "                          offspring_size=offspring_size, scoring=scoring_function,\n",
    "                          verbosity=2, random_state=2, cv=2)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "tpot_clf.fit(X_train, y_train)\n",
    "\n",
    "# Score on the test set\n",
    "print(tpot_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb1e657-58e2-420b-abf6-3ff07b90b13c",
   "metadata": {},
   "source": [
    "## Analysing TPOT's stability\n",
    "### You will now see the random nature of TPOT by constructing the classifier with different random states and seeing what model is found to be best by the algorithm. This assists to see that TPOT is quite unstable when not run for a reasonable amount of time.\n",
    "\n",
    "### Instructions 1/3\n",
    "-    Create the TPOT classifier, fit to the data and score using a random_state of 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bb86389-7060-433c-a98e-8af83fbe1443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/10 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.62\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.62\n",
      "\n",
      "Best pipeline: RandomForestClassifier(MinMaxScaler(input_matrix), bootstrap=True, criterion=gini, max_features=0.2, min_samples_leaf=8, min_samples_split=4, n_estimators=100)\n",
      "0.61989898989899\n"
     ]
    }
   ],
   "source": [
    "# Create the tpot classifier \n",
    "tpot_clf = TPOTClassifier(generations=2, population_size=4, offspring_size=3, scoring='accuracy', cv=2,\n",
    "                          verbosity=2, random_state=42)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "tpot_clf.fit(X_train, y_train)\n",
    "\n",
    "# Score on the test set\n",
    "print(tpot_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f8c81e-589e-4180-9921-dbb2ae24e9e0",
   "metadata": {},
   "source": [
    "### Instructions 2/3\n",
    "-    Now try using a random_state of 122. The numbers don't mean anything special, but should produce different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe7a233b-400e-41be-9edf-039ac710b76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/10 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.607363184079602\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.607363184079602\n",
      "\n",
      "Best pipeline: LogisticRegression(RobustScaler(input_matrix), C=0.5, dual=False, penalty=l2)\n",
      "0.5959595959595959\n"
     ]
    }
   ],
   "source": [
    "# Create the tpot classifier \n",
    "tpot_clf = TPOTClassifier(generations=2, population_size=4, offspring_size=3, scoring='accuracy', cv=2,\n",
    "                          verbosity=2, random_state=122)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "tpot_clf.fit(X_train, y_train)\n",
    "\n",
    "# Score on the test set\n",
    "print(tpot_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9384fa62-ea5a-4064-87ad-136f4212259f",
   "metadata": {},
   "source": [
    "### Instructions 3/3\n",
    "-    Finally try using the random_state of 99. See how there is a different result again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1654875c-b5be-47bd-98ac-6b91a44c99b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/10 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.6116417910447761\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.6171641791044775\n",
      "\n",
      "Best pipeline: RandomForestClassifier(input_matrix, bootstrap=False, criterion=entropy, max_features=0.15000000000000002, min_samples_leaf=6, min_samples_split=6, n_estimators=100)\n",
      "0.6153535353535353\n"
     ]
    }
   ],
   "source": [
    "# Create the tpot classifier \n",
    "tpot_clf = TPOTClassifier(generations=2, population_size=4, offspring_size=3, scoring='accuracy', cv=2,\n",
    "                          verbosity=2, random_state=99)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "tpot_clf.fit(X_train, y_train)\n",
    "\n",
    "# Score on the test set\n",
    "print(tpot_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa8a7dd-fcbb-408d-a897-a55c44e12791",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
