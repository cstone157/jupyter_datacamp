{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0adc712a-7581-41de-a1f6-ef22edcd2e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## As usual Datacamp didn't include how they loaded their data, because f' me right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff410b82-e470-4d96-af10-b1878c867485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create and fit the model\n",
    "knn = KNeighborsClassifier()\n",
    "#knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test features, print the results\n",
    "#pred = knn.predict(X_test)[0]\n",
    "#print(\"Prediction for test example 0:\", pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a16b474-b1c2-46e1-81c5-cab8e83f2348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42ef8a4d-dc50-4a24-8e9b-9e653c4c0e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00168001-9272-4623-87b8-0a23c530c76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9733333333333334\n",
      "0.9977728285077951\n",
      "0.9844444444444445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target)\n",
    "\n",
    "# Apply logistic regression and print scores\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(lr.score(X_train, y_train))\n",
    "print(lr.score(X_test, y_test))\n",
    "\n",
    "# Apply SVM and print scores\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "print(svm.score(X_train, y_train))\n",
    "print(svm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3869adb4-e7d9-4cac-9128-fbc9498213a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: LOVED IT! This movie was amazing. Top 10 this year.\n",
      "Review: Total junk! I'll never watch a film by that director again, no matter how good the reviews.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate logistic regression and train\n",
    "lr = LogisticRegression()\n",
    "#lr.fit(X, y)\n",
    "\n",
    "# Predict sentiment for a glowing review\n",
    "review1 = \"LOVED IT! This movie was amazing. Top 10 this year.\"\n",
    "#review1_features = get_features(review1)\n",
    "print(\"Review:\", review1)\n",
    "#print(\"Probability of positive review:\", lr.predict_proba(review1_features)[0,1])\n",
    "\n",
    "# Predict sentiment for a poor review\n",
    "review2 = \"Total junk! I'll never watch a film by that director again, no matter how good the reviews.\"\n",
    "#review2_features = get_features(review2)\n",
    "print(\"Review:\", review2)\n",
    "#print(\"Probability of positive review:\", lr.predict_proba(review2_features)[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124f20c4-99ae-4efd-ab6c-8bf4abd67fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7f323da-3ea7-4c86-97af-bad596da2514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "wine = datasets.load_wine()\n",
    "X=wine.data\n",
    "y=wine.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d1d7783-37f8-4e02-b2ed-2dadc0536a00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "def plot_4_classifiers(X, y, classfiers):\n",
    "    # Set-up 2x2 grid for plotting.\n",
    "    fig, sub = plt.subplots(2, 2)\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "    \n",
    "    X0, X1 = X[:, 0], X[:, 1]\n",
    "    \n",
    "    for c, ax in zip(classfiers, sub.flatten()):\n",
    "        disp = DecisionBoundaryDisplay.from_estimator(\n",
    "            c,\n",
    "            X,\n",
    "            response_method=\"predict\",\n",
    "            cmap=plt.cm.coolwarm,\n",
    "            alpha=0.8,\n",
    "            ax=ax\n",
    "        )\n",
    "        ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors=\"k\")\n",
    "\n",
    "# Retrieved the code by using function_name??\n",
    "def plot_contours(ax, clf, xx, yy, proba=False, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    if proba:\n",
    "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,-1]\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        out = ax.imshow(Z,extent=(np.min(xx), np.max(xx), np.min(yy), np.max(yy)), origin='lower', vmin=0, vmax=1, **params)\n",
    "        ax.contour(xx, yy, Z, levels=[0.5])\n",
    "    else:\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "def make_meshgrid(x, y, h=.02, lims=None):\n",
    "    \"\"\"Create a mesh of points to plot in\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: data to base x-axis meshgrid on\n",
    "    y: data to base y-axis meshgrid on\n",
    "    h: stepsize for meshgrid, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    if lims is None:\n",
    "        x_min, x_max = x.min() - 1, x.max() + 1\n",
    "        y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    else:\n",
    "        x_min, x_max, y_min, y_max = lims\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "def plot_classifier(X, y, clf, ax=None, ticks=False, proba=False, lims=None): # assumes classifier \"clf\" is already fit\n",
    "    X0, X1 = X[:, 0], X[:, 1]\n",
    "    xx, yy = make_meshgrid(X0, X1, lims=lims)\n",
    "\n",
    "    if ax is None:\n",
    "        plt.figure()\n",
    "        ax = plt.gca()\n",
    "        show = True\n",
    "    else:\n",
    "        show = False\n",
    "\n",
    "    # can abstract some of this into a higher-level function for learners to call\n",
    "    cs = plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8, proba=proba)\n",
    "    if proba:\n",
    "        cbar = plt.colorbar(cs)\n",
    "        cbar.ax.set_ylabel('probability of red $\\Delta$ class', fontsize=20, rotation=270, labelpad=30)\n",
    "        cbar.ax.tick_params(labelsize=14)\n",
    "    #ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=30, edgecolors='k', linewidth=1)\n",
    "    labels = np.unique(y)\n",
    "    if len(labels) == 2:\n",
    "        ax.scatter(X0[y==labels[0]], X1[y==labels[0]], cmap=plt.cm.coolwarm, s=60, c='b', marker='o', edgecolors='k')\n",
    "        ax.scatter(X0[y==labels[1]], X1[y==labels[1]], cmap=plt.cm.coolwarm, s=60, c='r', marker='^', edgecolors='k')\n",
    "    else:\n",
    "        ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=50, edgecolors='k', linewidth=1)\n",
    "\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "#     ax.set_xlabel(data.feature_names[0])\n",
    "#     ax.set_ylabel(data.feature_names[1])\n",
    "    if ticks:\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "#     ax.set_title(title)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        return ax\n",
    "\n",
    "def plot_4_classifiers(X, y, clfs):\n",
    "\n",
    "    # Set-up 2x2 grid for plotting.\n",
    "    fig, sub = plt.subplots(2, 2)\n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "\n",
    "    for clf, ax, title in zip(clfs, sub.flatten(), (\"(1)\", \"(2)\", \"(3)\", \"(4)\")):\n",
    "        # clf.fit(X, y)\n",
    "        plot_classifier(X, y, clf, ax, ticks=True)\n",
    "        ax.set_title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ca4d99b-83c6-4b43-8e1f-fd1f4f23019e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Visualizing decision boundaries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Define the classifiers\n",
    "classifiers = [LogisticRegression(), LinearSVC(),\n",
    "               SVC(), KNeighborsClassifier()]\n",
    "\n",
    "# Fit the classifiers\n",
    "for c in classifiers:\n",
    "    c.fit(X, y)\n",
    "\n",
    "# As usual, datacamp can't be bothered to share their code/clean up.  Thanks guys, really great work.\n",
    "    \n",
    "# Plot the classifiers - plot is a series of previously defined functions\n",
    "#plot_4_classifiers(X, y, classifiers)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb11c4f9-1e8b-40a6-8e8a-a6d525fdc1ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
