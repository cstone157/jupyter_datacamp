{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8366de-d43e-4c7f-8bc6-300ebde81a4d",
   "metadata": {},
   "source": [
    "## Develop a classification model\n",
    "\n",
    "In this exercise, you'll work with the weather dataset and develop a training code to predict rainfall for the next day. The preprocess_dataset.py contains helper functions to pre-process the dataset. Your task is to finish the scaffolded train.py to formulate a high-level model training flow.\n",
    "\n",
    "Feel free to explore the Python files to see the complete implementation of the workflow.\n",
    "\n",
    "NOTE: Use python3 instead of python to run Python scripts.\n",
    "\n",
    "### Ide Exercise Instruction\n",
    "    - Split the pre-processed dataset in train.py file using the train_test_split method from scikit-learn.\n",
    "    - Train the model using the training set, by specifying the correct arguments to the train_model method.\n",
    "    - Calculate test set metrics using the test set, by specifying the correct arguments to the evaluate_model method.\n",
    "    - Save the metrics dictionary into a JSON file using the save_metrics method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5782fe57-edb9-476c-b5cd-d08160009aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from metrics_and_plots import plot_confusion_matrix, save_metrics\n",
    "from model import evaluate_model, train_model\n",
    "from utils_and_constants import PROCESSED_DATASET, TARGET_COLUMN\n",
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    X = data.drop(TARGET_COLUMN, axis=1)\n",
    "    y = data[TARGET_COLUMN]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def main():\n",
    "    X, y = load_data(PROCESSED_DATASET)\n",
    "    \n",
    "    # Split data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1993)\n",
    "\n",
    "    # Train the model using the training set\n",
    "    model = train_model(X_train, y_train)\n",
    "    \n",
    "    # Calculate test set metrics\n",
    "    metrics = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "    print(\"====================Test Set Metrics==================\")\n",
    "    print(json.dumps(metrics, indent=2))\n",
    "    print(\"======================================================\")\n",
    "\n",
    "    # Save metrics into json file\n",
    "    save_metrics(metrics)\n",
    "    plot_confusion_matrix(model, X_test, y_test)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70146a7e-7861-4949-9f0b-530562fe3b3f",
   "metadata": {},
   "source": [
    "## Setup model training using CML\n",
    "\n",
    "In this exercise, you will use CML GitHub Action to train a Random Forest Classifier to predict rainfall. CML is a GitHub Action that abstracts generating reports for ML experiments.\n",
    "\n",
    "The training will trigger when you open a PR against the main branch. You'll continue working with the weather dataset; the preprocess_dataset.py file contains helper functions to pre-process the dataset as before.\n",
    "\n",
    "The output from running train.py is a metrics.json file containing model metrics, and confusion_matrix.png file containing a plot of the confusion matrix.\n",
    "\n",
    "Your task is to finish the scaffolded .github/workflows/train_cml.yaml to formulate a high-level model training flow.\n",
    "\n",
    "NOTE: Use python3 instead of python to run Python scripts.\n",
    "\n",
    "### Ide Exercise Instruction\n",
    "    - Setup CML GitHub Action iterative/setup-cml@v1.\n",
    "    - Add evaluation metrics data, metrics.json, to the markdown report in the Write CML report step.\n",
    "    - Add confusion matrix plot, confusion_matrix.png , to the markdown report in the Write CML report step.\n",
    "    - Write the correct cml comment subcommand to create a comment in the PR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4bbf5b-3c20-4c42-8823-e7f163f750b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "name: model-training\n",
    "\n",
    "on:\n",
    "  pull_request:\n",
    "    branches: main\n",
    "\n",
    "permissions: write-all\n",
    "\n",
    "jobs:\n",
    "  train_and_report_eval_performance:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - name: Checkout \n",
    "        uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Setup Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: 3.9\n",
    "\n",
    "      # Setup CML GitHub Action\n",
    "      - name: Setup CML\n",
    "        uses: iterative/setup-cml@v1\n",
    "          \n",
    "      - name: Train model\n",
    "        run: |\n",
    "          python3 preprocess_dataset.py\n",
    "          python3 train.py\n",
    "\n",
    "      - name: Write CML report\n",
    "        env:\n",
    "          REPO_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n",
    "        run: |\n",
    "          # Add metrics data to markdown\n",
    "          cat metrics.json >> model_eval_report.md\n",
    "          \n",
    "          # Add confusion matrix plot to markdown\n",
    "          echo \"![confusion matrix plot](./confusion_matrix.png)\" >> model_eval_report.md\n",
    "\n",
    "          # Create comment from markdown report\n",
    "          cml comment create model_eval_report.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03d169c-7bd4-4ad4-b8bf-aee2696e3f04",
   "metadata": {},
   "source": [
    "## Data versioning in action\n",
    "\n",
    "Data Version Control (DVC) provides a systematic approach to versioning data, a critical aspect often overlooked. With DVC, you can precisely track changes in your datasets, ensuring reproducibility, collaboration, and troubleshooting ease. It's your safeguard against data-related challenges, fostering trust and efficiency in your data-driven projects.\n",
    "\n",
    "In this exercise, you will practice initializing a DVC project and versioning a dataset. Git has already been initialized for this project.\n",
    "\n",
    "### Ide Exercise Instruction\n",
    "    - Initialize DVC in the workspace.\n",
    "    - Verify that .dvcignore file and .dvc folder are present.\n",
    "    - Add dataset.csv to DVC and examine the contents of dataset.csv.dvc by opening it in the file editor.\n",
    "    - Verify that DVC cache is populated by running find .dvc/cache -type f command in terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbb633e-a0f0-4f3c-aaba-f0b6c3b11e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repl:~/workspace$ dvc init\n",
    "# repl:~/workspace$ dvc add dataset.csv\n",
    "# repl:~/workspace$ find .dvc/cache -type "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3b546d-41a1-48f2-91d4-f50c3a4daaaf",
   "metadata": {},
   "source": [
    "## DVC remotes in action\n",
    "\n",
    "In this exercise, you'll learn how to set up and use DVC remotes to store and share your datasets securely. Whether it's a colleague across the globe or your future self working on the project, DVC remotes ensure that your data is readily accessible and up-to-date. This exercise already has DVC initialized and the dataset added to DVC cache. We will be limiting ourselves to DVC remotes set up on a local filesystem.\n",
    "\n",
    "The syntax for adding a default DVC remote is\n",
    "\n",
    "dvc remote add -d --local <remote_name> </path/to/folder>\n",
    "\n",
    "where -d indicates the default DVC remote, and --local indicates that the DVC remote is pointed locally.\n",
    "\n",
    "### Ide Exercise Instruction\n",
    "    - Set up a local DVC remote named myremote pointed at /tmp/dvc/localremote and verify it is empty by examining output of ls /tmp/dvc/localremote.\n",
    "    - Examine the contents of .dvc/config.local, is the default set correctly to myremote?\n",
    "    - Run dvc push and verify that the local remote now contains the file.\n",
    "    - Run dvc pull and verify that \"Everything up to date\" appears as shell output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502fd074-77c4-4635-8159-c68cb5447277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#$ dvc remote add -d --local myremote /tmp/dvc/localremote\n",
    "#$ dvc push\n",
    "#$ dvc pull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76071653-b5f5-4f24-b3f3-7232c4d32f86",
   "metadata": {},
   "source": [
    "## Creating a DVC pipeline\n",
    "\n",
    "Imagine a simple example of a workflow where a document is printed and then scanned to create a signed PDF document, with DVC managing the dependencies and outputs of each stage.\n",
    "\n",
    "The print stage depends on printing instructions outlined in print.sh and produces the pages output. The scan stage depends on instructions in scan.sh and pages (output of printer) and produces a signed.pdf output.\n",
    "\n",
    "Your task is to design a DVC pipeline outlining the workflow using the dvc stage add command. Its syntax is\n",
    "\n",
    "dvc stage add -n <stage_name> -d <dependency> -o <output> <command>\n",
    "\n",
    "You can add multiple dependencies and outputs with repeated use of -d and -o flags, respectively.\n",
    "\n",
    "NOTE: DVC has already been initialized in the exercise setup. There is no need to run dvc init again.\n",
    "\n",
    "### Ide Exercise Instruction\n",
    "    - Design the print stage with print.sh as a dependency, pages as output, and ./print.sh as command.\n",
    "    - Design the scan stage with scan.sh and pages as dependencies, signed.pdf as output, and ./scan.sh as command.\n",
    "    - Verify dvc.yaml is written correctly.\n",
    "    - Visualize the pipeline with dvc dag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a50231-cebc-4dc7-99a1-291bb33b1f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dvc.yaml\n",
    "stages:\n",
    "  print:\n",
    "    cmd: ./print.sh\n",
    "    deps:\n",
    "    - print.sh\n",
    "    outs:\n",
    "    - pages\n",
    "  scan:\n",
    "    cmd: ./scan.sh\n",
    "    deps:\n",
    "    - pages\n",
    "    - scan.sh\n",
    "    outs:\n",
    "    - signed.pdf\n",
    "#$ dvc stage add -n print -d print.sh -o pages ./print.sh\n",
    "#$ dvc stage add -n scan -d scan.sh -d pages -o signed.pdf ./scan.sh\n",
    "#$ dvc dag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc22a88b-ee31-4ef5-97f8-9a32ac7ea41d",
   "metadata": {},
   "source": [
    "## Train ML models with DVC\n",
    "\n",
    "Ensuring the reproducibility of your experiments and maintaining a clean project structure can be challenging.\n",
    "\n",
    "This exercise will guide you through a scenario where you have to train a machine-learning model using a structured approach. You'll leverage DVC for managing data versioning and reproducibility in machine learning projects. Your task is to complete the scaffolded dvc.yaml file provides instructions for preprocessing data and training a model, ensuring that you can confidently reproduce your experiments and collaborate effectively with your team.\n",
    "\n",
    "NOTE: Use python3 instead of python to run Python scripts.\n",
    "\n",
    "### Ide Exercise Instruction\n",
    "    - Run the data preprocessing script in the preprocess stage.\n",
    "    - Run the model training script in the train stage.\n",
    "    - Specify the preprocessed dataset as a dependency in the train stage.\n",
    "    - Run the dvc repro command on the terminal to run the DVC pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98311270-8651-4f6e-a0f8-b5b83c3b8c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvc.yml\n",
    "stages:\n",
    "  preprocess:\n",
    "    # Run the data preprocessing script\n",
    "    cmd: python3 preprocess_dataset.py\n",
    "    deps:\n",
    "    - preprocess_dataset.py\n",
    "    - raw_dataset/weather.csv\n",
    "    - utils_and_constants.py\n",
    "    outs:\n",
    "    - processed_dataset/weather.csv\n",
    "  train:\n",
    "    # Run the model training script\n",
    "    cmd: python3 train.py\n",
    "    deps:\n",
    "    - metrics_and_plots.py\n",
    "    - model.py\n",
    "    # Specify the preprocessed dataset as a dependency\n",
    "    - processed_dataset/weather.csv\n",
    "    - train.py\n",
    "    - utils_and_constants.py\n",
    "    outs:\n",
    "    - metrics.json\n",
    "    - confusion_matrix.png\n",
    "\n",
    "\n",
    "#$ dvc repro"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
