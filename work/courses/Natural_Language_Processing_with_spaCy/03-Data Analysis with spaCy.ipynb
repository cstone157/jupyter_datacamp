{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "447620aa-ee4d-4321-a520-aa3c75b3d305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#!python3 -m spacy download en_core_web_sm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878caee1-a62e-4e50-9afd-c9a0ccb2eac8",
   "metadata": {},
   "source": [
    "## Adding pipes in spaCy\n",
    "\n",
    "### You often use an existing spaCy model for different NLP tasks. However, in some cases, an off-the-shelf pipeline component such as sentence segmentation will take long times to produce expected results. In this exercise, you'll practice adding a pipeline component to a spaCy model (text processing pipeline).\n",
    "\n",
    "### You will use the first five reviews from the Amazon Fine Food Reviews dataset for this exercise. You can access these reviews by using the texts string.\n",
    "\n",
    "### The spaCy package is already imported for you to use.\n",
    "\n",
    "### Instructions\n",
    "-    Load a blank spaCy English model and add a sentencizer component to the model.\n",
    "-    Create a Doc container for the texts, create a list to store sentences of the given document and print its number of sentences.\n",
    "-    Print the list of tokens in the second sentence from the sentences list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f5e7ad8-b7ff-4e12-8cca-7810f53b77e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = 'I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most. Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\". This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis\\' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch. If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal. Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4456008c-8f04-46d5-aef7-c97e4162b74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:  19 \n",
      "\n",
      "Second sentence tokens:  [The, product, looks, more, like, a, stew, than, a, processed, meat, and, it, smells, better, .]\n"
     ]
    }
   ],
   "source": [
    "# Load a blank spaCy English model and add a sentencizer component\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Create Doc containers, store sentences and print its number of sentences\n",
    "doc = nlp(texts)\n",
    "sentences = [s for s in doc.sents]\n",
    "print(\"Number of sentences: \", len(sentences), \"\\n\")\n",
    "\n",
    "# Print the list of tokens in the second sentence\n",
    "print(\"Second sentence tokens: \", [token for token in sentences[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a612ea8-b172-40cb-b90f-d75bb9a5e059",
   "metadata": {},
   "source": [
    "### Analyzing pipelines in spaCy\n",
    "\n",
    "### spaCy allows you to analyze a spaCy pipeline to check whether any required attributes are not set. In this exercise, you'll practice analyzing a spaCy pipeline. Earlier in the video, an existing en_core_web_sm pipeline was analyzed and the result was No problems found., in this instance, you will analyze a blank spaCy English model with few added components and observe results of the analysis.\n",
    "\n",
    "### The spaCy package is already imported for you to use.\n",
    "\n",
    "### Instructions 1/2\n",
    "-    Load a blank spaCy English model as nlp.\n",
    "-    Add tagger and entity_linker pipeline components to the blank model.\n",
    "-    Analyze the nlp pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d6fa55e-5a84-4a4a-822c-586acdfc2789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "============================= Pipeline Overview =============================\u001b[0m\n",
      "\n",
      "#   Component       Assigns           Requires         Scores        Retokenizes\n",
      "-   -------------   ---------------   --------------   -----------   -----------\n",
      "0   tagger          token.tag                          tag_acc       False      \n",
      "                                                                                \n",
      "1   entity_linker   token.ent_kb_id   doc.ents         nel_micro_f   False      \n",
      "                                      doc.sents        nel_micro_r              \n",
      "                                      token.ent_iob    nel_micro_p              \n",
      "                                      token.ent_type                            \n",
      "\n",
      "\u001b[1m\n",
      "================================ Problems (4) ================================\u001b[0m\n",
      "\u001b[38;5;3mâš  'entity_linker' requirements not met: doc.ents, doc.sents,\n",
      "token.ent_iob, token.ent_type\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load a blank spaCy English model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Add tagger and entity_linker pipeline components\n",
    "nlp.add_pipe(\"tagger\")\n",
    "nlp.add_pipe(\"entity_linker\")\n",
    "\n",
    "# Analyze the pipeline\n",
    "analysis = nlp.analyze_pipes(pretty=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e464fca6-7e51-4204-8cdd-7c79d581c669",
   "metadata": {},
   "source": [
    "## EntityRuler with blank spaCy model\n",
    "\n",
    "### EntityRuler lets you to add entities to doc.ents. It can be combined with EntityRecognizer, a spaCy pipeline component for named-entity recognition, to boost accuracy, or used on its own to implement a purely rule-based entity recognition system. In this exercise, you will practice adding an EntityRuler component to a blank spaCy English model and classify named entities of the given text using purely rule-based named-entity recognition.\n",
    "\n",
    "### The spaCy package is already imported and a blank spaCy English model is ready for your use as nlp. A list of patterns to classify lower cased OpenAI and Microsoft as ORG is already created for your use.\n",
    "\n",
    "### Instructions\n",
    "-    Create and add an EntityRuler component to the pipeline.\n",
    "-    Add given patterns to the EntityRuler component.\n",
    "-    Run the model on the given text and create its corresponding Doc container.\n",
    "-    Print a tuple of (entities text and types) for all entities in the Doc container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c44af354-0620-48ad-b855-d1b8b854df0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('OpenAI', 'ORG'), ('Microsoft', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"openai\"}]},\n",
    "            {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"microsoft\"}]}]\n",
    "text = \"OpenAI has joined forces with Microsoft.\"\n",
    "\n",
    "# Add EntityRuler component to the model\n",
    "entity_ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "# Add given patterns to the EntityRuler component\n",
    "entity_ruler.add_patterns(patterns)\n",
    "\n",
    "# Run the model on a given text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print entities text and type for all entities in the Doc container\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80ed209-537e-46b7-b0e4-9d2ec4d4882c",
   "metadata": {},
   "source": [
    "## EntityRuler for NER\n",
    "\n",
    "### EntityRuler can be combined with EntityRecognizer of an existing model to boost its accuracy. In this exercise, you will practice combining an EntityRuler component and an existing NER component of the en_core_web_sm model. The model is already loaded as nlp.\n",
    "\n",
    "### When EntityRuler is added before NER component, the entity recognizer will respect the existing entity spans and adjust its predictions based on patterns added to the EntityRuler to improve accuracy of named entity recognition task.\n",
    "\n",
    "### Instructions\n",
    "-    Add an EntityRuler to the nlp before ner component.\n",
    "-    Define a token entity pattern to classify lower cased new york group as ORG.\n",
    "-    Add the patterns to the EntityRuler component.\n",
    "-    Run the model and print the tuple of entities text and type for the Doc container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1f3bdac-2e8a-41b5-ba05-87e1785f2339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('New York Group', 'ORG'), ('1987', 'DATE')]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"New York Group was built in 1987.\"\n",
    "\n",
    "# Add an EntityRuler to the nlp before NER component\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "\n",
    "# Define a pattern to classify lower cased new york group as ORG\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": [{\"lower\": \"new york group\"}]}]\n",
    "\n",
    "# Add the patterns to the EntityRuler component\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Run the model and print entities text and type for all the entities\n",
    "doc = nlp(text)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d277108e-a435-4406-ab2a-c7ddd122298b",
   "metadata": {},
   "source": [
    "## EntityRuler with multi-patterns in spaCy\n",
    "\n",
    "### EntityRuler lets you to add entities to doc.ents and boost its named entity recognition performance. In this exercise, you will practice adding an EntityRuler component to an existing nlp pipeline to ensure multiple entities are correctly being classified.\n",
    "\n",
    "### The en_core_web_sm model is already loaded and is available for your use as nlp. You can access an example text in example_text and use nlp and doc to access an spaCy model and Doc container of example_text respectively.\n",
    "\n",
    "### Instructions\n",
    "-    Print a list of tuples of entities text and types in the example_text with the nlp model.\n",
    "-    Define multiple patterns to match lower cased brother and sisters to PERSON label.\n",
    "-    Add an EntityRuler component to the nlp pipeline and add the patterns to the EntityRuler.\n",
    "-    Print a tuple of text and type of entities for the example_text with the nlp model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccd89080-7436-47fc-b661-b85686add1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = 'This is a confection. In this case Filberts. And it is cut into tiny squares. This is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bb01610-9e65-4a2d-82be-b8605c7c9716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before EntityRuler:  [('Filberts', 'PERSON'), ('Edmund', 'PERSON'), ('Sisters', 'ORG')] \n",
      "\n",
      "After EntityRuler:  [('Filberts', 'PERSON'), ('Edmund', 'PERSON'), ('Brother', 'PERSON'), ('Sisters', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Print a list of tuples of entities text and types in the example_text\n",
    "print(\"Before EntityRuler: \", [(ent.text, ent.label_) for ent in nlp(example_text).ents], \"\\n\")\n",
    "\n",
    "# Define pattern to add a label PERSON for lower cased sisters and brother entities\n",
    "patterns = [{\"label\": \"PERSON\", \"pattern\": [{\"lower\": \"brother\"}]},\n",
    "            {\"label\": \"PERSON\", \"pattern\": [{\"lower\": \"sisters\"}]}]\n",
    "\n",
    "# Add an EntityRuler component and add the patterns to the ruler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Print a list of tuples of entities text and types\n",
    "print(\"After EntityRuler: \", [(ent.text, ent.label_) for ent in nlp(example_text).ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e17509-01c3-47d2-be22-22452e899958",
   "metadata": {},
   "source": [
    "## RegEx in Python\n",
    "\n",
    "### Rule-based information extraction is useful for many NLP tasks. Certain types of entities, such as dates or phone numbers have distinct formats that can be recognized by a set of rules without needing to train any model. In this exercise, you will practice using re package for RegEx. The goal is to find phone numbers in a given text.\n",
    "\n",
    "### re package is already imported for your use. You can use \\d to match string patterns representative of a metacharacter that matches any digit from 0 to 9.\n",
    "\n",
    "### Instructions\n",
    "-    Define a pattern to match phone numbers of the form (111)-111-1111.\n",
    "-    Find all the matching patterns using re.finditer() method.\n",
    "-    For each match, print start and end characters and matching section of the given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09badbe8-04fd-4616-a9d5-b5d3132e80ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2acc2e23-f40b-410d-b920-145a38920f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start character:  20 | End character:  34 | Matching text:  (425)-123-4567\n"
     ]
    }
   ],
   "source": [
    "text = \"Our phone number is (425)-123-4567.\"\n",
    "\n",
    "# Define a pattern to match phone numbers\n",
    "pattern = r\"\\((\\d){3}\\)-(\\d){3}-(\\d){4}\"\n",
    "\n",
    "# Find all the matching patterns in the text\n",
    "phones = re.finditer(pattern, text)\n",
    "\n",
    "# Print start and end characters and matching section of the text\n",
    "for match in phones:\n",
    "    start_char = match.start()\n",
    "    end_char = match.end()\n",
    "    print(\"Start character: \", start_char, \"| End character: \", end_char, \"| Matching text: \", text[start_char:end_char])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91f2f61-166a-4f51-8d1b-048b32ada71b",
   "metadata": {},
   "source": [
    "## RegEx with EntityRuler in spaCy\n",
    "\n",
    "### Regular expressions, or RegEx, are used for rule-based information extraction with complex string matching patterns. RegEx can be used to retrieve patterns or replace matching patterns in a string with some other patterns. In this exercise, you will practice using EntityRuler in spaCy to find email addresses in a given text.\n",
    "\n",
    "### spaCy package is already imported for your use. You can use \\d to match string patterns representative of a metacharacter that matches any digit from 0 to 9.\n",
    "\n",
    "### A spaCy pattern can use REGEX as an attribute. In this case, a pattern will be of shape [{\"TEXT\": {\"REGEX\": \"<a given pattern>\"}}].\n",
    "\n",
    "### Instructions\n",
    "-    Define a pattern to match phone numbers of the form 8888888888 to be used by the EntityRuler.\n",
    "-    Load a blank spaCy English model and add an EntityRuler component to the pipeline.\n",
    "-    Add the compiled pattern to the EntityRuler component.\n",
    "-    Run the model and print the tuple of text and type of entities for the given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d722d04-45d1-4deb-8ada-dde98e15ecc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('4251234567', 'PHONE_NUMBERS')]\n"
     ]
    }
   ],
   "source": [
    "text = \"Our phone number is 4251234567.\"\n",
    "\n",
    "# Define a pattern to match phone numbers\n",
    "patterns = [{\"label\": \"PHONE_NUMBERS\", \"pattern\": [{\"TEXT\": {\"REGEX\": \"(\\d){10}\"}}]}]\n",
    "\n",
    "# Load a blank model and add an EntityRuler\n",
    "nlp = spacy.blank(\"en\")\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "# Add the compiled patterns to the EntityRuler\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Print the tuple of entities texts and types for the given text\n",
    "doc = nlp(text)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd42e16-ff84-4751-8a9d-8f7ca2f985e0",
   "metadata": {},
   "source": [
    "## Matching a single term in spaCy\n",
    "\n",
    "### RegEx patterns are not trivial to read, write and debug. But you are not at a loss, spaCy provides a readable and production-level alternative, the Matcher class. The Matcher class can match predefined rules to a sequence of tokens in a given Doc container. In this exercise, you will practice using Matcher to find a single word.\n",
    "\n",
    "### You can access the corresponding text in example_text and use nlp and doc to access an spaCy model and Doc container of example_text respectively.\n",
    "\n",
    "### Instructions\n",
    "-    Initialize a Matcher class.\n",
    "-    Define a pattern to match lower cased witch in the example_text.\n",
    "-    Add the patterns to the Matcher class and find matches.\n",
    "-    Iterate through matches and print start and end token indices and span of the matched text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7abe0a97-63bd-4571-a363-19738634d2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "example_text = 'I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis\\' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5b53133-fdb8-432d-b3e6-6ac4baa8a4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start token:  24  | End token:  25 | Matched text:  Witch\n",
      "Start token:  47  | End token:  48 | Matched text:  Witch\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(example_text)\n",
    "\n",
    "# Initialize a Matcher object\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Define a pattern to match lower cased word witch\n",
    "pattern = [{\"lower\" : \"witch\"}]\n",
    "\n",
    "# Add the pattern to matcher object and find matches\n",
    "matcher.add(\"CustomMatcher\", [pattern])\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Print start and end token indices and span of the matched text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Start token: \", start, \" | End token: \", end, \"| Matched text: \", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c352f718-c883-4f25-abbc-5d4b20ea4cbc",
   "metadata": {},
   "source": [
    "## PhraseMatcher in spaCy\n",
    "\n",
    "### While processing unstructured text, you often have long lists and dictionaries that you want to scan and match in given texts. The Matcher patterns are handcrafted and each token needs to be coded individually. If you have a long list of phrases, Matcher is no longer the best option. In this instance, PhraseMatcher class helps us match long dictionaries. In this exercise, you will practice to retrieve patterns with matching shapes to multiple terms using PhraseMatcher class.\n",
    "\n",
    "### en_core_web_sm model is already loaded and ready for you to use as nlp. PhraseMatcher class is imported. A text string and a list of terms are available for your use.\n",
    "\n",
    "### Instructions\n",
    "-    Initialize a PhraseMatcher class with an attr to match to shape of given terms.\n",
    "-    Create patterns to add to the PhraseMatcher object.\n",
    "-    Find matches to the given patterns and print start and end token indices and matching section of the given text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef6f288d-9cf4-4d2d-8205-913873e8fb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a556d64d-dee5-423a-adf8-031a23a783a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start token:  12  | End token:  13 | Matched text:  127.100.0.1\n",
      "Start token:  17  | End token:  18 | Matched text:  123.4.1.0\n"
     ]
    }
   ],
   "source": [
    "text = \"There are only a few acceptable IP addresse: (1) 127.100.0.1, (2) 123.4.1.0.\"\n",
    "terms = [\"110.0.0.0\", \"101.243.0.0\"]\n",
    "\n",
    "# Initialize a PhraseMatcher class to match to shapes of given terms\n",
    "matcher = PhraseMatcher(nlp.vocab, attr = \"SHAPE\")\n",
    "\n",
    "# Create patterns to add to the PhraseMatcher object\n",
    "patterns = [nlp.make_doc(term) for term in terms]\n",
    "matcher.add(\"IPAddresses\", patterns)\n",
    "\n",
    "# Find matches to the given patterns and print start and end characters and matches texts\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Start token: \", start, \" | End token: \", end, \"| Matched text: \", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fbeaf3-d202-4292-80ed-6ecc0f33a5b0",
   "metadata": {},
   "source": [
    "## Matching with extended syntax in spaCy\n",
    "\n",
    "### Rule-based information extraction is essential for any NLP pipeline. The Matcher class allows patterns to be more expressive by allowing some operators inside the curly brackets. These operators are for extended comparison and look similar to Python's in, not in and comparison operators. In this exercise, you will practice with spaCy's matching functionality, Matcher, to find matches for given terms from an example text.\n",
    "\n",
    "### Matcher class is already imported from spacy.matcher library. You will use a Doc container of an example text in this exercise by calling doc. A pre-loaded spaCy model is also accessible at nlp.\n",
    "\n",
    "### Instructions\n",
    "-    Define a matcher object using Matcher and nlp.\n",
    "-    Use the IN operator to define a pattern to match tiny squares and tiny mouthful.\n",
    "-    Use this pattern to find matches for doc.\n",
    "-    Print start and end token indices and text span of the matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbc33378-ac1a-4baa-aeb5-13794c13f8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"It is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f3ec9f1-5a3b-4bc0-9e55-eb05e4b949de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start token:  4  | End token:  6 | Matched text:  tiny squares\n",
      "Start token:  19  | End token:  21 | Matched text:  tiny mouthful\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(example_text)\n",
    "\n",
    "# Define a matcher object\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Define a pattern to match tiny squares and tiny mouthful\n",
    "pattern = [{\"lower\": \"tiny\"}, {\"lower\": {\"in\": [\"squares\", \"mouthful\"]}}]\n",
    "\n",
    "# Add the pattern to matcher object and find matches\n",
    "matcher.add(\"CustomMatcher\", [pattern])\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Print out start and end token indices and the matched text span per match\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Start token: \", start, \" | End token: \", end, \"| Matched text: \", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c32f077-722f-4b8b-8250-0f83f9e7ab91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
