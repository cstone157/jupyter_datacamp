{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb55cb70-09f2-42d0-b257-d94994405109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#!python3 -m spacy download en_core_web_sm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed03e8c5-dc95-442c-8bb1-04af7a18ae52",
   "metadata": {},
   "source": [
    "## Word-sense disambiguation with spaCy\n",
    "### WSD is a classical problem of deciding in which sense a word is used in a sentence. Determining the sense of the word can be crucial in search engines, machine translation, and question-answering systems. In this exercise, you will practice using POS tagging for word-sense disambiguation.\n",
    "\n",
    "### There are two sentences containing the word jam, with two different senses and you are tasked to identify the POS tags to help you determine the corresponding sense of the word in a given sentence.\n",
    "\n",
    "### The two sentences are available in the texts list. The en_core_web_sm model is already loaded and available for your use as nlp.\n",
    "\n",
    "### Instructions 1/2\n",
    "-    Create a documents list containing the Doc containers of each element in the texts list.\n",
    "-    Print a tuple of the token's text and POS tags per each Doc container only if the word jam is in the token text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e13b111f-70ee-481d-93b0-d2a5864e596c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:  [('jam', 'VERB')] \n",
      "\n",
      "Sentence 2:  [('jam', 'NOUN')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts = [\"This device is used to jam the signal.\",\n",
    "         \"I am stuck in a traffic jam\"]\n",
    "\n",
    "# Create a list of Doc containers in the texts list\n",
    "documents = [nlp(t) for t in texts]\n",
    "\n",
    "# Print a token's text and POS tag if the word jam is in the token's text\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Sentence {i+1}: \", [(token.text, token.pos_) for token in doc if \"jam\" in token.text], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1701a7eb-f806-4b9a-a790-ffc9f3559a21",
   "metadata": {},
   "source": [
    "## Dependency parsing with spaCy\n",
    "### Dependency parsing analyzes the grammatical structure in a sentence and finds out related words as well as the type of relationship between them. An application of dependency parsing is to identify a sentence object and subject. In this exercise, you will practice extracting dependency labels for given texts.\n",
    "\n",
    "### Three comments from the Airline Travel Information System (ATIS) dataset have been provided for you in a list called texts. en_core_web_sm model is already loaded and available for your use as nlp.\n",
    "\n",
    "### Instructions\n",
    "-    Create a documents list containing the doc containers of each element in the texts list.\n",
    "-    Print a tuple of (the token's text, dependency label, and label's explanation) per each doc container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "296ea04e-229e-4aa5-93ea-7c60f50cc57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['I want to fly from Boston at 8:38 am and arrive in Denver at 11:10 in the morning',\n",
    " 'What flights are available from Pittsburgh to Baltimore on Thursday morning?',\n",
    " 'What is the arrival time in San francisco for the 7:55 AM flight leaving Washington?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0fa1933-bca9-453a-aabb-42c4d6c5d80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'nsubj', 'nominal subject'), ('want', 'ROOT', 'root'), ('to', 'aux', 'auxiliary'), ('fly', 'xcomp', 'open clausal complement'), ('from', 'prep', 'prepositional modifier'), ('Boston', 'pobj', 'object of preposition'), ('at', 'prep', 'prepositional modifier'), ('8:38', 'nummod', 'numeric modifier'), ('am', 'pobj', 'object of preposition'), ('and', 'cc', 'coordinating conjunction'), ('arrive', 'conj', 'conjunct'), ('in', 'prep', 'prepositional modifier'), ('Denver', 'pobj', 'object of preposition'), ('at', 'prep', 'prepositional modifier'), ('11:10', 'pobj', 'object of preposition'), ('in', 'prep', 'prepositional modifier'), ('the', 'det', 'determiner'), ('morning', 'pobj', 'object of preposition')] \n",
      "\n",
      "[('What', 'det', 'determiner'), ('flights', 'nsubj', 'nominal subject'), ('are', 'ROOT', 'root'), ('available', 'acomp', 'adjectival complement'), ('from', 'prep', 'prepositional modifier'), ('Pittsburgh', 'pobj', 'object of preposition'), ('to', 'prep', 'prepositional modifier'), ('Baltimore', 'pobj', 'object of preposition'), ('on', 'prep', 'prepositional modifier'), ('Thursday', 'compound', 'compound'), ('morning', 'pobj', 'object of preposition'), ('?', 'punct', 'punctuation')] \n",
      "\n",
      "[('What', 'attr', 'attribute'), ('is', 'ROOT', 'root'), ('the', 'det', 'determiner'), ('arrival', 'compound', 'compound'), ('time', 'nsubj', 'nominal subject'), ('in', 'prep', 'prepositional modifier'), ('San', 'compound', 'compound'), ('francisco', 'pobj', 'object of preposition'), ('for', 'prep', 'prepositional modifier'), ('the', 'det', 'determiner'), ('7:55', 'nummod', 'numeric modifier'), ('AM', 'compound', 'compound'), ('flight', 'pobj', 'object of preposition'), ('leaving', 'acl', 'clausal modifier of noun (adjectival clause)'), ('Washington', 'dobj', 'direct object'), ('?', 'punct', 'punctuation')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a list of Doc containts of texts list\n",
    "documents = [nlp(t) for t in texts]\n",
    "\n",
    "# Print each token's text, dependency label and its explanation\n",
    "for doc in documents:\n",
    "    print([(token.text, token.dep_, spacy.explain(token.dep_)) for token in doc], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e356d4-9a30-4eb5-af5e-7038ed758da0",
   "metadata": {},
   "source": [
    "## spaCy vocabulary\n",
    "### Word vectors, or word embeddings, are numerical representations of words that allow computers to perform complex tasks using text data. Word vectors are a part of many spaCy models, however, a few of the models do not have word vectors.\n",
    "\n",
    "### In this exercise, you will practice accessing spaCy vocabulary information. Some meta information about word vectors are stored in each spaCy model. You can access this information to learn more about the vocabulary size, word vectors dimensions, etc.\n",
    "\n",
    "### The spaCy package is already imported for your use. In a spaCy model's metadata, the number of words is stored as an element with the \"vectors\" key and the dimension of word vectors is stored as an element with the \"width\" key.\n",
    "\n",
    "### Instructions\n",
    "-    Load the en_core_web_md model.\n",
    "-    Print the number of words in the en_core_web_md model's vocabulary.\n",
    "-    Print the dimensions of word vectors in the en_core_web_md model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a061c5c5-05f5-456a-b7f2-577d2abf6c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:  20000 \n",
      "\n",
      "Dimension of word vectors:  300\n"
     ]
    }
   ],
   "source": [
    "# Load the en_core_web_md model\n",
    "md_nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Print the number of words in the model's vocabulary\n",
    "print(\"Number of words: \", md_nlp.meta[\"vectors\"][\"vectors\"], \"\\n\")\n",
    "\n",
    "# Print the dimensions of word vectors in en_core_web_md model\n",
    "print(\"Dimension of word vectors: \", md_nlp.meta[\"vectors\"][\"width\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403f9e9e-cda1-4de9-9aa7-9ba910ab6a43",
   "metadata": {},
   "source": [
    "## Word vectors in spaCy vocabulary\n",
    "### The purpose of word vectors is to allow a computer to understand words. In this exercise, you will practice extracting word vectors for a given list of words.\n",
    "\n",
    "### A list of words is compiled as words. The en_core_web_md model is already imported and available as nlp.\n",
    "\n",
    "### The vocabulary of en_core_web_md model contains 20,000 words. If a word does not exist in the vocabulary, you will not be able to extract its corresponding word vector. In this exercise, for simplicity, it is ensured that all the given words exist in this model's vocabulary.\n",
    "\n",
    "### Instructions\n",
    "-    Extract the IDs of all the given words and store them in an ids list.\n",
    "-    For each ID from ids, store the first ten elements of the word vector in the word_vectors list.\n",
    "-    Print the first ten elements of the first word vector from word_vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be74d08c-6168-44bb-8e0f-653295ad16e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffcdb74f-ba5a-4267-a3b9-2cc7e06715b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.3334  -1.3695  -1.133   -0.68461 -1.8482  -0.63712  2.6791   4.1433\n",
      " -2.5616  -1.8061 ]\n"
     ]
    }
   ],
   "source": [
    "words = [\"like\", \"love\"]\n",
    "\n",
    "# IDs of all the given words\n",
    "ids = [nlp.vocab.strings[w] for w in words]\n",
    "\n",
    "# Store the first ten elements of the word vectors for each word\n",
    "word_vectors = [nlp.vocab.vectors[i][:10] for i in ids]\n",
    "\n",
    "# Print the first ten elements of the first word vector\n",
    "print(word_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0d5ddd-9632-4ce6-a5cf-559e56198fdf",
   "metadata": {},
   "source": [
    "## Word vectors projection\n",
    "\n",
    "### You can visualize word vectors in a scatter plot to help you understand how the vocabulary words are grouped. In order to visualize word vectors, you need to project them into a two-dimensional space. You can project vectors by extracting the two principal components via Principal Component Analysis (PCA).\n",
    "\n",
    "### In this exercise, you will practice how to extract word vectors and project them into two-dimensional space using the PCA library from sklearn.\n",
    "\n",
    "### A short list of words that are stored in the words list and the en_core_web_md model are available for use. The model is loaded as nlp. All necessary libraries and packages are already imported for your use (PCA, numpy as np).\n",
    "\n",
    "### Instructions\n",
    "-    Extract the word IDs from the given words and store them in the word_ids list.\n",
    "-    Extract the first five elements of the word vectors of the words and then stack them vertically using np.vstack() in word_vectors.\n",
    "-    Given a pca object, calculate the transformed word vectors using the .fit_transform() function of the pca class.\n",
    "-    Print the first component of the transformed word vectors using [:, 0] indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de754d32-aacb-45a4-b8a5-6ca079ebafa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbabb37e-2c4e-4756-9fb4-fd21a5a99e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.4203901 -3.420389 ]\n"
     ]
    }
   ],
   "source": [
    "words = [\"tiger\", \"bird\"]\n",
    "\n",
    "# Extract word IDs of given words\n",
    "word_ids = [nlp.vocab.strings[w] for w in words]\n",
    "\n",
    "# Extract word vectors and stack the first five elements vertically\n",
    "word_vectors = np.vstack([nlp.vocab.vectors[i][:5] for i in word_ids])\n",
    "\n",
    "# Calculate the transformed word vectors using the pca object\n",
    "pca = PCA(n_components=2)\n",
    "word_vectors_transformed = pca.fit_transform(word_vectors)\n",
    "\n",
    "# Print the first component of the transformed word vectors\n",
    "print(word_vectors_transformed[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ca29c3-15f9-423d-863b-f68b45bc5ef4",
   "metadata": {},
   "source": [
    "## Similar words in a vocabulary\n",
    "\n",
    "### Finding semantically similar terms has various applications in information retrieval. In this exercise, you will practice finding the most semantically similar term to the word computer from the en_core_web_md model's vocabulary.\n",
    "\n",
    "### The computer word vector is already extracted and stored as word_vector. The en_core_web_md model is already loaded as nlp, and NumPy package is loaded as np.\n",
    "\n",
    "### You can use the .most_similar() function of the nlp.vocab.vectors object to find the most semantically similar terms. Using [0][0] to index the output of this function will return the word IDs of the semantically similar terms. nlp.vocab.strings[<a given word>] can be used to find the word ID of a given word and it can similarly return the word associated with a given word ID.\n",
    "\n",
    "### Instructions\n",
    "-    Find the most semantically similar term from the en_core_web_md vocabulary.\n",
    "-    Find the list of similar words given the word IDs of the similar terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b85ef45-f239-4fdc-85b2-399c83d60300",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = [nlp.vocab.strings[\"computer\"]]\n",
    "word_vector = np.vstack([nlp.vocab.vectors[i][:5] for i in word_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "595acdeb-0fc3-4b48-93e9-e564e088a510",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,1,5) and (300,20000) not aligned: 5 (dim 2) != 300 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Find the most similar word to the word computer\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m most_similar_words \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword_vector\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Find the list of similar words given the word IDs\u001b[39;00m\n\u001b[1;32m      5\u001b[0m words \u001b[38;5;241m=\u001b[39m [nlp\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mstrings[w] \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m most_similar_words[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/spacy/vectors.pyx:590\u001b[0m, in \u001b[0;36mspacy.vectors.Vectors.most_similar\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,1,5) and (300,20000) not aligned: 5 (dim 2) != 300 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Find the most similar word to the word computer\n",
    "most_similar_words = nlp.vocab.vectors.most_similar(np.asarray([word_vector]), n = 1)\n",
    "\n",
    "# Find the list of similar words given the word IDs\n",
    "words = [nlp.vocab.strings[w] for w in most_similar_words[0][0]]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e956585-55b5-4816-8675-6791669a16c1",
   "metadata": {},
   "source": [
    "## Doc similarity with spaCy\n",
    "\n",
    "### Semantic similarity is the process of analyzing multiple sentences to identify similarities between them. In this exercise, you will practice calculating semantic similarities of documents to a given document. The goal is to categorize a list of given reviews that are relevant to canned dog food.\n",
    "\n",
    "### The canned dog food category is stored at category. A sample of five food reviews has been provided for you in a list called texts. en_core_web_md is loaded as nlp.\n",
    "\n",
    "### Instructions\n",
    "-    Create a documents list containing Doc containers of all texts.\n",
    "-    Create a Doc container of the category and store it as category_document.\n",
    "-    Iterate through documents and print the similarity scores of each Doc container and the category_document, rounded to three digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86b90378-7835-42cf-adec-60e7310da685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic similarity with document 1: 0.231\n",
      "Semantic similarity with document 2: 0.329\n",
      "Semantic similarity with document 3: 0.227\n"
     ]
    }
   ],
   "source": [
    "# Create a documents list containing Doc containers\n",
    "documents = [nlp(t) for t in texts]\n",
    "\n",
    "# Create a Doc container of the category\n",
    "category = \"canned dog food\"\n",
    "category_document = nlp(category)\n",
    "\n",
    "# Print similarity scores of each Doc container and the category_document\n",
    "for i, doc in enumerate(documents):\n",
    "  print(f\"Semantic similarity with document {i+1}:\", round(doc.similarity(category_document), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7303ba7b-a91b-4dd7-9c62-f07089d49f6d",
   "metadata": {},
   "source": [
    "## Span similarity with spaCy\n",
    "\n",
    "### Determining semantic similarity can help you to categorize texts into predefined categories or detect relevant texts, or to flag duplicate content. In this exercise, you will practice calculating the semantic similarities of spans of a document to a given document. The goal is to find the most relevant Span of three tokens that are relevant to canned dog food.\n",
    "\n",
    "### The given category of canned dog food is stored at category. A text string is already stored in the text object and the en_core_web_md is loaded as nlp. The Doc container of the text is also already created and stored at document.\n",
    "\n",
    "### Instructions\n",
    "-    Create a Doc container for the category and store at category_document.\n",
    "-    Print similarity score of a given Span and the category_document, rounded to three digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e3c6e1d-59c9-4cae-a702-ddbd1649b958",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = [nlp(\"canned\"), nlp(\"food\"),  nlp(\"products.\")]\n",
    "document = nlp(\"canned food products.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e77767aa-4a58-477f-9191-5f9a1aed9b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic similarity with canned food products : 0.795\n"
     ]
    }
   ],
   "source": [
    "# Create a Doc container for the category\n",
    "category = \"canned dog food\"\n",
    "category_document = nlp(category)\n",
    "\n",
    "# Print similarity score of a given Span and category_document\n",
    "document_span = document[0:3]\n",
    "print(f\"Semantic similarity with\", document_span.text, \":\", round(document_span.similarity(category_document), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064a89a6-231b-4311-b2b3-3e76b9379e12",
   "metadata": {},
   "source": [
    "## Semantic similarity for categorizing text\n",
    "\n",
    "### The main objective of semantic similarity is to measure the distance between the semantic meanings of a pair of words, phrases, sentences, or documents. For example, the word “car” is more similar to “bus” than it is to “cat”. In this exercise, you will find similar sentences to the word sauce from an example text in Amazon Fine Food Reviews. You can use spacy to calculate the similarity score of the word sauce and any of the sentences in a given texts string and report the most similar sentence's score.\n",
    "\n",
    "### A texts string is pre-loaded that contains all reviews' Text data. You'll use en_core_web_md English model for this exercise which is already available as nlp.\n",
    "\n",
    "### Instructions\n",
    "-    Use nlp to generate Doc containers for the word sauce and for texts and store them at key and sentences respectively.\n",
    "-    Calculate similarity scores of the word sauce with each sentence in the texts string (rounded to two digits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e2b7d1e-cfd8-4ad3-bfd4-898256b5600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = 'This hot sauce is amazing! We picked up a bottle on a trip! '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "83f7e74f-d8ee-48ac-87d9-1a816bf45979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'score': 0.5}, {'score': 0.17}]\n"
     ]
    }
   ],
   "source": [
    "# Populate Doc containers for the word \"sauce\" and for \"texts\" string \n",
    "key = nlp(\"sauce\")\n",
    "sentences = nlp(texts)\n",
    "\n",
    "# Calculate similarity score of each sentence and a Doc container for the word sauce\n",
    "semantic_scores = []\n",
    "for sent in sentences.sents:\n",
    "\tsemantic_scores.append({\"score\": round(sent.similarity(key), 2)})\n",
    "print(semantic_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffb674b-8eaa-4ed2-8af8-e2e223e861e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
