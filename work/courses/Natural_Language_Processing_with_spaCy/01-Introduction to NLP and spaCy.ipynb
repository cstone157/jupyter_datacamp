{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a3853e4-f575-4c1c-81b3-91477df1599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749dcfb3-8019-4d16-b54e-123cc03444d5",
   "metadata": {},
   "source": [
    "## Doc container in spaCy\n",
    "### The first step of a spaCy text processing pipeline is to convert a given text string into a Doc container, which stores the processed text. In this exercise, you'll practice loading a spaCy model, creating an nlp() object, creating a Doc container and processing a text string that is available for you.\n",
    "\n",
    "### en_core_web_sm model is already downloaded.\n",
    "\n",
    "### Instructions\n",
    "-    Load en_core_web_sm and create an nlp object.\n",
    "-    Create a doc container of the text string.\n",
    "-    Create a list containing the text of each tokens in the doc container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d93abe81-4930-4b0a-b763-5fe018a20d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'NLP is becoming increasingly popular for providing business solutions.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ec43efc-071e-458d-9be4-77303f1c9e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', 'is', 'becoming', 'increasingly', 'popular', 'for', 'providing', 'business', 'solutions', '.']\n"
     ]
    }
   ],
   "source": [
    "# Load en_core_web_sm and create an nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create a Doc container for the text object\n",
    "doc = nlp(text)\n",
    "\n",
    "# Create a list containing the text of each token in the Doc container\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50979ab7-275c-44fb-b1c9-7ae70f481e95",
   "metadata": {},
   "source": [
    "## Tokenization with spaCy\n",
    "### In this exercise, you'll practice tokenizing text. You'll use the first review from the Amazon Fine Food Reviews dataset for this exercise. You can access this review by using the text object provided.\n",
    "\n",
    "### The en_core_web_sm model is already loaded for you. You can access it by calling nlp(). You can use list comprehension to compile output lists.\n",
    "\n",
    "### Instructions\n",
    "-    Store Doc container for the pre-loaded review in a document object.\n",
    "-    Store and review texts of all the tokens of the document in the variable first_text_tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4d8b1a7-7f08-4e1d-8002-120eb067d853",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6351a24f-15f7-4d65-9ca0-3797f1e50a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First text tokens:\n",
      " ['I', 'have', 'bought', 'several', 'of', 'the', 'Vitality', 'canned', 'dog', 'food', 'products', 'and', 'have', 'found', 'them', 'all', 'to', 'be', 'of', 'good', 'quality', '.', 'The', 'product', 'looks', 'more', 'like', 'a', 'stew', 'than', 'a', 'processed', 'meat', 'and', 'it', 'smells', 'better', '.', 'My', 'Labrador', 'is', 'finicky', 'and', 'she', 'appreciates', 'this', 'product', 'better', 'than', ' ', 'most', '.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Doc container of the given text\n",
    "document = nlp(text)\n",
    "    \n",
    "# Store and review the token text values of tokens for the Doc container\n",
    "first_text_tokens = [token.text for token in document]\n",
    "print(\"First text tokens:\\n\", first_text_tokens, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90da677-278f-41a2-a710-36d39eea5c71",
   "metadata": {},
   "source": [
    "## Running a spaCy pipeline\n",
    "### You've already run a spaCy NLP pipeline on a single piece of text and also extracted tokens of a given list of Doc containers. In this exercise, you'll practice the initial steps of running a spaCy pipeline on texts, which is a list of text strings.\n",
    "\n",
    "### You will use the en_core_web_sm model for this purpose. The spaCy package has already been imported for you.\n",
    "\n",
    "### Instructions\n",
    "-    Load the en_core_web_sm model as nlp.\n",
    "-    Run an nlp() model on each item of texts, and append each corresponding Doc container to a documents list.\n",
    "-    Print the token texts for each Doc container of the documents list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88c32a02-3ec5-4fdf-a9ec-ba438e9cc143",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['A loaded spaCy model can be used to compile documents list!',\n",
    " 'Tokenization is the first step in any spacy pipeline.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9753341-efac-41dd-ace7-9eaa368cbed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'loaded', 'spaCy', 'model', 'can', 'be', 'used', 'to', 'compile', 'documents', 'list', '!']\n",
      "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'any', 'spacy', 'pipeline', '.']\n"
     ]
    }
   ],
   "source": [
    "# Load en_core_web_sm model as nlp\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Run an nlp model on each item of texts and append the Doc container to documents\n",
    "documents = []\n",
    "for text in texts:\n",
    "  documents.append(nlp(text))\n",
    "  \n",
    "# Print the token texts for each Doc container\n",
    "for doc in documents:\n",
    "  print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b234af0-f5d9-4b21-8d95-95b25ca192ba",
   "metadata": {},
   "source": [
    "## Lemmatization with spaCy\n",
    "### In this exercise, you will practice lemmatization. Lemmatization can be helpful to generate the root form of derived words. This means that given any sentence, we expect the number of lemmas to be less than or equal to the number of tokens.\n",
    "\n",
    "### The first Amazon food review is provided for you in a string called text. en_core_web_sm is loaded as nlp, and has been run on the text to compile document, a Doc container for the text string.\n",
    "\n",
    "### tokens, a list containing tokens for the text is also already loaded for your use.\n",
    "\n",
    "### Instructions\n",
    "-    Append the lemma for all tokens in the document, then print the list of lemmas.\n",
    "-    Print tokens list and observe the differences between tokens and lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54f391f3-e105-47b1-9f58-798bcb2801b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "172916e1-f9d6-4b85-8086-3af1514c9245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmas:\n",
      " ['I', 'have', 'buy', 'several', 'of', 'the', 'Vitality', 'can', 'dog', 'food', 'product', 'and', 'have', 'find', 'they', 'all', 'to', 'be', 'of', 'good', 'quality', '.', 'the', 'product', 'look', 'more', 'like', 'a', 'stew', 'than', 'a', 'process', 'meat', 'and', 'it', 'smell', 'well', '.', 'my', 'Labrador', 'be', 'finicky', 'and', 'she', 'appreciate', 'this', 'product', 'well', 'than', ' ', 'most', '.'] \n",
      "\n",
      "Tokens:\n",
      " ['I', 'have', 'bought', 'several', 'of', 'the', 'Vitality', 'canned', 'dog', 'food', 'products', 'and', 'have', 'found', 'them', 'all', 'to', 'be', 'of', 'good', 'quality', '.', 'The', 'product', 'looks', 'more', 'like', 'a', 'stew', 'than', 'a', 'processed', 'meat', 'and', 'it', 'smells', 'better', '.', 'My', 'Labrador', 'is', 'finicky', 'and', 'she', 'appreciates', 'this', 'product', 'better', 'than', ' ', 'most', '.']\n"
     ]
    }
   ],
   "source": [
    "document = nlp(text)\n",
    "tokens = [token.text for token in document]\n",
    "\n",
    "# Append the lemma for all tokens in the document\n",
    "lemmas = [token.lemma_ for token in document]\n",
    "print(\"Lemmas:\\n\", lemmas, \"\\n\")\n",
    "\n",
    "# Print tokens and compare with lemmas list\n",
    "print(\"Tokens:\\n\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a093d0bd-e4c3-44e7-9d5c-02dd7fb8fd3d",
   "metadata": {},
   "source": [
    "## Sentence segmentation with spaCy\n",
    "### In this exercise, you will practice sentence segmentation. In NLP, segmenting a document into its sentences is a useful basic operation. It is one of the first steps in many NLP tasks that are more elaborate, such as detecting named entities. Additionally, capturing the number of sentences may provide some insight into the amount of information provided by the text.\n",
    "\n",
    "### You can access ten food reviews in the list called texts.\n",
    "\n",
    "### The en_core_web_sm model has already been loaded for you as nlp and .\n",
    "\n",
    "### Instructions\n",
    "-    Run the spaCy model on each item in the texts list to compile documents, a list of all Doc containers.\n",
    "-    Extract sentences of each doc container by iterating through documents list and append them to a list called sentences.\n",
    "-    Count the number of sentences in each doc container using the sentences list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "334fe461-b50f-46d7-bb28-ab9742630c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.',\n",
    " 'Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".',\n",
    " 'This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis\\' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.',\n",
    " 'If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.',\n",
    " 'Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.',\n",
    " 'I got a wild hair for taffy and ordered this five pound bag. The taffy was all very enjoyable with many flavors: watermelon, root beer, melon, peppermint, grape, etc. My only complaint is there was a bit too much red/black licorice-flavored pieces (just not my particular favorites). Between me, my kids, and my husband, this lasted only two weeks! I would recommend this brand of taffy -- it was a delightful treat.',\n",
    " \"This saltwater taffy had great flavors and was very soft and chewy.  Each candy was individually wrapped well.  None of the candies were stuck together, which did happen in the expensive version, Fralinger's.  Would highly recommend this candy!  I served it at a beach-themed party and everyone loved it!\",\n",
    " 'This taffy is so good.  It is very soft and chewy.  The flavors are amazing.  I would definitely recommend you buying it.  Very satisfying!!',\n",
    " \"Right now I'm mostly just sprouting this so my cats can eat the grass. They love it. I rotate it around with Wheatgrass and Rye too\",\n",
    " 'This is a very healthy dog food. Good for their digestion. Also good for small puppies. My dog eats her required amount at every feeding.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8baf152a-39f9-4cdf-b693-a9fc75bf8256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 2, 7, 3, 4, 5, 5, 5, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# Generating a documents list of all Doc containers\n",
    "documents = [nlp(text) for text in texts]\n",
    "\n",
    "# Iterate through documents and append sentences in each doc to the sentences list\n",
    "sentences = []\n",
    "for doc in documents:\n",
    "  sentences.append([s for s in doc.sents])\n",
    "  \n",
    "# Find number of sentences per each doc container\n",
    "print([len(s) for s in sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02dd745-72a8-47ae-b282-fa2c6b729985",
   "metadata": {},
   "source": [
    "## POS tagging with spaCy\n",
    "### In this exercise, you will practice POS tagging. POS tagging is a useful tool in NLP as it allows algorithms to understand the grammatical structure of a sentence and to confirm words that have multiple meanings such as watch and play.\n",
    "\n",
    "### For this exercise, en_core_web_sm has been loaded for you as nlp. Three comments from the Airline Travel Information System (ATIS) dataset have been provided for you in a list called texts.\n",
    "\n",
    "### Instructions\n",
    "-    Compile documents, a list of all doc containers for each text in texts list using list comprehension.\n",
    "-    For each doc container, print each token's text and its corresponding POS tag by iterating through documents and tokens of each doc container using a nested for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e12c005-5ebd-4d59-82f0-f7d7de9e530c",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['What is the arrival time in San francisco for the 7:55 AM flight leaving Washington?',\n",
    " 'Cheapest airfare from Tacoma to Orlando is 650 dollars.',\n",
    " 'Round trip fares from Pittsburgh to Philadelphia are under 1000 dollars!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ce5d1d3-bca0-4fe8-8609-f73a7bac9f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  What | POS tag:  PRON\n",
      "Text:  is | POS tag:  AUX\n",
      "Text:  the | POS tag:  DET\n",
      "Text:  arrival | POS tag:  NOUN\n",
      "Text:  time | POS tag:  NOUN\n",
      "Text:  in | POS tag:  ADP\n",
      "Text:  San | POS tag:  PROPN\n",
      "Text:  francisco | POS tag:  PROPN\n",
      "Text:  for | POS tag:  ADP\n",
      "Text:  the | POS tag:  DET\n",
      "Text:  7:55 | POS tag:  NUM\n",
      "Text:  AM | POS tag:  PROPN\n",
      "Text:  flight | POS tag:  NOUN\n",
      "Text:  leaving | POS tag:  VERB\n",
      "Text:  Washington | POS tag:  PROPN\n",
      "Text:  ? | POS tag:  PUNCT\n",
      "\n",
      "\n",
      "Text:  Cheapest | POS tag:  ADJ\n",
      "Text:  airfare | POS tag:  NOUN\n",
      "Text:  from | POS tag:  ADP\n",
      "Text:  Tacoma | POS tag:  PROPN\n",
      "Text:  to | POS tag:  ADP\n",
      "Text:  Orlando | POS tag:  PROPN\n",
      "Text:  is | POS tag:  AUX\n",
      "Text:  650 | POS tag:  NUM\n",
      "Text:  dollars | POS tag:  NOUN\n",
      "Text:  . | POS tag:  PUNCT\n",
      "\n",
      "\n",
      "Text:  Round | POS tag:  ADJ\n",
      "Text:  trip | POS tag:  NOUN\n",
      "Text:  fares | POS tag:  NOUN\n",
      "Text:  from | POS tag:  ADP\n",
      "Text:  Pittsburgh | POS tag:  PROPN\n",
      "Text:  to | POS tag:  ADP\n",
      "Text:  Philadelphia | POS tag:  PROPN\n",
      "Text:  are | POS tag:  AUX\n",
      "Text:  under | POS tag:  ADP\n",
      "Text:  1000 | POS tag:  NUM\n",
      "Text:  dollars | POS tag:  NOUN\n",
      "Text:  ! | POS tag:  PUNCT\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile a list of all Doc containers of texts\n",
    "documents = [nlp(text) for text in texts]\n",
    "\n",
    "# Print token texts and POS tags for each Doc container\n",
    "for doc in documents:\n",
    "    for token in doc:\n",
    "        print(\"Text: \", token.text, \"| POS tag: \", token.pos_)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15391c35-ad02-4a6b-b661-a34ce5f63695",
   "metadata": {},
   "source": [
    "## NER with spaCy\n",
    "### Named entity recognition (NER) helps you to easily identify key elements of a given document, like names of people and places. It helps sort unstructured data and detect important information, which is crucial if you are dealing with large datasets. In this exercise, you will practice Named Entity Recognition.\n",
    "\n",
    "### en_core_web_sm has been loaded for you as nlp. Three comments from the Airline Travel Information System (ATIS) dataset have been provided for you in a list called texts.\n",
    "\n",
    "### Instructions\n",
    "-    Compile documents, a list of all Doc containers for each text in the texts using list comprehension.\n",
    "-    For each doc container, print each entity's text and corresponding label by iterating through doc.ents.\n",
    "-    Print the sixth token's text, and the entity type of the second Doc container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00f25141-9383-4070-adae-7d44b668f4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['I want to fly from Boston at 8:38 am and arrive in Denver at 11:10 in the morning',\n",
    " 'What flights are available from Pittsburgh to Baltimore on Thursday morning?',\n",
    " 'What is the arrival time in San francisco for the 7:55 AM flight leaving Washington?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a60ed0b-5345-4602-b3ef-cb5190cf2864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Boston', 'GPE'), ('8:38 am', 'TIME'), ('Denver', 'GPE'), ('11:10 in the morning', 'TIME')]\n",
      "[('Pittsburgh', 'GPE'), ('Baltimore', 'GPE'), ('Thursday', 'DATE'), ('morning', 'TIME')]\n",
      "[('San francisco', 'GPE'), ('7:55 AM', 'TIME'), ('Washington', 'GPE')]\n",
      "\n",
      "Text: Pittsburgh | Entity type:  GPE\n"
     ]
    }
   ],
   "source": [
    "# Compile a list of all Doc containers of texts\n",
    "documents = [nlp(text) for text in texts]\n",
    "\n",
    "# Print the entity text and label for the entities in each document\n",
    "for doc in documents:\n",
    "    print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "    \n",
    "# Print the 6th token's text and entity type of the second document\n",
    "print(\"\\nText:\", documents[1][5].text, \"| Entity type: \", documents[1][5].ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde58e8f-cbda-4a0e-a887-84b8d8c7aca7",
   "metadata": {},
   "source": [
    "## Text processing with spaCy\n",
    "### Every NLP application consists of several text processing steps. You have already learned some of these steps, including tokenization, lemmatization, sentence segmentation and named entity recognition.\n",
    "\n",
    "### spaCy NLP Pipeline \n",
    "<img src='./images/spacy_convention.jpeg' />\n",
    "\n",
    "### In this exercise, you'll continue to practice with text processing steps in spaCy, such as breaking the text into sentences and extracting named entities. You will use the first five reviews from the Amazon Fine Food Reviews dataset for this exercise. You can access these reviews by using the texts object.\n",
    "\n",
    "### The en_core_web_sm model has already been loaded for you to use, and you can access it by using nlp. The list of Doc containers for each item in texts is also pre-loaded and accessible at documents.\n",
    "\n",
    "### Instructions 1/2\n",
    "-    Create sentences, a list of list of all sentences in each doc container in documents using list comprehension.\n",
    "-    Print num_sentences, a list containing the number of sentences for each doc container by using the len() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00dd73d9-70d4-4b43-b961-7ab8a6989431",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [nlp(\"I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\"),\n",
    " nlp(\"Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \\\"Jumbo\\\".\"),\n",
    " nlp(\"This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \\\"The Lion, The Witch, and The Wardrobe\\\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\"),\n",
    " nlp(\"If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.\"),\n",
    " nlp(\"Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "889f3e2d-389b-404c-9d11-8d4d549227dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in documents:\n",
      " [3, 2, 7, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# Create a list to store sentences of each Doc container in documents\n",
    "sentences = [[sent for sent in doc.sents] for doc in documents]\n",
    "\n",
    "# Print number of sentences in each Doc container in documents\n",
    "num_sentences = [len(s) for s in sentences]\n",
    "print(\"Number of sentences in documents:\\n\", num_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2c43e0-2bb0-472e-8b50-5832ea0b835d",
   "metadata": {},
   "source": [
    "### Instructions 2/2\n",
    "-    Create a list of tuples of format (entity text, entity label) for the third doc container in third_text_entities.\n",
    "-    Create a list of tuples of format (token text, POS tag) of first ten tokens of third doc container at third_text_10_pos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92d4f52c-5189-4559-aad1-637053188239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in documents:\n",
      " [3, 2, 7, 3, 4] \n",
      "\n",
      "Third text entities:\n",
      " [('citrus gelatin', 'PERSON'), ('Filberts', 'PERSON'), (\"C.S. Lewis'\", 'ORG'), ('The Lion, The Witch', 'WORK_OF_ART'), ('The Wardrobe', 'WORK_OF_ART'), ('Edmund', 'GPE'), ('Sisters', 'PERSON'), ('Witch', 'LOC')] \n",
      "\n",
      "First ten tokens of third text:\n",
      " [('This', 'PRON'), ('is', 'AUX'), ('a', 'DET'), ('confection', 'NOUN'), ('that', 'PRON'), ('has', 'AUX'), ('been', 'AUX'), ('around', 'ADP'), ('a', 'DET'), ('few', 'ADJ')]\n"
     ]
    }
   ],
   "source": [
    "# Create a list to store sentences of each Doc container in documents\n",
    "sentences = [[sent for sent in doc.sents] for doc in documents]\n",
    "\n",
    "# Create a list to track number of sentences per Doc container in documents\n",
    "num_sentences = [len([sent for sent in doc.sents]) for doc in documents]\n",
    "print(\"Number of sentences in documents:\\n\", num_sentences, \"\\n\")\n",
    "\n",
    "# Record entities text and corresponding label of the third Doc container\n",
    "third_text_entities = [(ent.text, ent.label_) for ent in documents[2].ents]\n",
    "print(\"Third text entities:\\n\", third_text_entities, \"\\n\")\n",
    "\n",
    "# Record first ten tokens and corresponding POS tag for the third Doc container\n",
    "third_text_10_pos = [(token.text, token.pos_) for token in documents[2]][:10]\n",
    "print(\"First ten tokens of third text:\\n\", third_text_10_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32a0550-cefd-44e4-b9d6-d2a69d747535",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
