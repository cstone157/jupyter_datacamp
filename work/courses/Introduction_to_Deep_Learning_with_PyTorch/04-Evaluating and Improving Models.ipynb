{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b06e7c2-5a79-4ca6-a711-61cc238c9b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33d8241-aed4-4bf5-ba1a-0f0e0e2ada2d",
   "metadata": {},
   "source": [
    "## Using the TensorDataset class\n",
    "\n",
    "#### In practice, loading your data into a PyTorch dataset will be one of the first steps you take in order to create and train a neural network with PyTorch.\n",
    "\n",
    "#### The TensorDataset class is very helpful when your dataset can be loaded directly as a NumPy array. Recall that TensorDataset() can take one or more NumPy arrays as input.\n",
    "\n",
    "#### In this exercise, you'll practice creating a PyTorch dataset using the TensorDataset class.\n",
    "\n",
    "#### torch and numpy have already been imported for you, along with the TensorDataset class.\n",
    "\n",
    "## Instructions\n",
    "-    Convert the NumPy arrays provided to PyTorch tensors.\n",
    "-    Create a TensorDataset using the torch_features and the torch_target tensors provided (in this order).\n",
    "-    Return the last element of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "400466af-8965-42ff-ad0a-5fda724cdcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.2950, 0.5588, 0.1077, 0.5836, 0.4986, 0.8434, 0.6060, 0.4132],\n",
      "       dtype=torch.float64), tensor([0.6191], dtype=torch.float64))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "np_features = np.array(np.random.rand(12, 8))\n",
    "np_target = np.array(np.random.rand(12, 1))\n",
    "\n",
    "# Convert arrays to PyTorch tensors\n",
    "torch_features = torch.tensor(np_features)\n",
    "torch_target = torch.tensor(np_target)\n",
    "\n",
    "# Create a TensorDataset from two tensors\n",
    "dataset = TensorDataset(torch_features, torch_target)\n",
    "\n",
    "# Return the last element of this dataset\n",
    "print(dataset[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843797da-162e-4ed3-add0-50e8d8f90423",
   "metadata": {},
   "source": [
    "## From data loading to running a forward pass\n",
    "\n",
    "#### In this exercise, you'll create a PyTorch DataLoader from a pandas DataFrame and call a model on this dataset. Specifically, you'll run a forward pass on a neural network. You'll continue working with fully connected neural networks, as you have done so far.\n",
    "\n",
    "#### You'll begin by subsetting a loaded DataFrame called dataframe, converting features and targets NumPy arrays, and converting to PyTorch tensors in order to create a PyTorch dataset.\n",
    "\n",
    "#### This dataset can be loaded into a PyTorch DataLoader, batched, shuffled, and used to run a forward pass on a custom fully connected neural network.\n",
    "\n",
    "#### NumPy as np, pandas as pd, torch, TensorDataset(), and DataLoader() have been imported for you.\n",
    "\n",
    "## Instructions 1/3\n",
    "-    Extract the features (ph, Sulfate, Conductivity, Organic_carbon) and target (Potability) values and load them into the appropriate tensors to represent features and targets.\n",
    "-    Use both tensors to create a PyTorch dataset using the dataset class that's quickest to use when tensors don't require any additional preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b66bda97-ae56-4f3f-96fb-02a21ea78f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"../../data/water_potability.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f576114c-e74f-41fb-a04a-39a79bd6d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the different columns into two PyTorch tensors\n",
    "features = torch.tensor(dataframe[['ph', 'Sulfate', 'Conductivity', 'Organic_carbon']].to_numpy()).float()\n",
    "target = torch.tensor(dataframe['Potability'].to_numpy()).float()\n",
    "\n",
    "# Create a dataset from the two generated tensors\n",
    "dataset = TensorDataset(features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480b8726-228c-46f2-8fed-2bacdc169454",
   "metadata": {},
   "source": [
    "## Instructions 2/3\n",
    "-    Create a PyTorch DataLoader from the created TensorDataset; this DataLoader should use a batch_size of two and shuffle the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00a4f16-44da-4afa-a772-01d4a11d4459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the different columns into two PyTorch tensors\n",
    "features = torch.tensor(dataframe[['ph', 'Sulfate', 'Conductivity', 'Organic_carbon']].to_numpy()).float()\n",
    "target = torch.tensor(dataframe['Potability'].to_numpy()).float()\n",
    "\n",
    "# Create a dataset from the two generated tensors\n",
    "dataset = TensorDataset(features, target)\n",
    "\n",
    "# Create a dataloader using the above dataset\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=2)\n",
    "x, y = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3e4cc6-fdda-474f-a513-1419fb887e6c",
   "metadata": {},
   "source": [
    "## Instructions 3/3\n",
    "-    Implement a small, fully connected neural network using exactly two linear layers and the nn.Sequential() API, where the final output size is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ec5c2-4c36-4aec-8af2-456c7008b0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the different columns into two PyTorch tensors\n",
    "features = torch.tensor(dataframe[['ph', 'Sulfate', 'Conductivity', 'Organic_carbon']].to_numpy()).float()\n",
    "target = torch.tensor(dataframe['Potability'].to_numpy()).float()\n",
    "\n",
    "# Create a dataset from the two generated tensors\n",
    "dataset = TensorDataset(features, target)\n",
    "\n",
    "# Create a dataloader using the above dataset\n",
    "dataloader = DataLoader(dataset, shuffle=True, batch_size=2)\n",
    "x, y = next(iter(dataloader))\n",
    "\n",
    "# Create a model using the nn.Sequential API\n",
    "model = nn.Sequential(\n",
    "  nn.Linear(4, 16), \n",
    "  nn.Linear(16, 1)\n",
    ")\n",
    "\n",
    "output = model(features)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b48072b-5b53-4c36-aac2-dd0f58002b21",
   "metadata": {},
   "source": [
    "## Writing the evaluation loop\n",
    "\n",
    "#### In this exercise, you will practice writing the evaluation loop. Recall that the evaluation loop is similar to the training loop, except that you will not perform the gradient calculation and the optimizer step.\n",
    "\n",
    "#### The model has already been defined for you, along with the object validationloader, which is a dataset.\n",
    "\n",
    "## Instructions 1/2\n",
    "-    Set the model to evaluation mode.\n",
    "-    Sum the current batch loss to the validation_loss variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c3899b-7019-4a7f-ac33-b5b56a37e373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "validation_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "  \n",
    "  for data in validationloader:\n",
    "    \n",
    "      outputs = model(data[0])\n",
    "      loss = criterion(outputs, data[1])\n",
    "      \n",
    "      # Sum the current loss to the validation_loss variable\n",
    "      validation_loss += loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cef125-fbc1-4899-91b9-60ac1c8a5dc6",
   "metadata": {},
   "source": [
    "## Instructions 2/2\n",
    "-    Calculate the mean loss value for the epoch.\n",
    "-    Set the model back to training mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bdeafd-2299-463e-9b05-fe987bab51ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "validation_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "  \n",
    "  for data in validationloader:\n",
    "    \n",
    "      outputs = model(data[0])\n",
    "      loss = criterion(outputs, data[1])\n",
    "      \n",
    "      # Sum the current loss to the validation_loss variable\n",
    "      validation_loss += loss.item()\n",
    "      \n",
    "# Calculate the mean loss value\n",
    "validation_loss_epoch = validation_loss / len(validationloader)\n",
    "print(validation_loss_epoch)\n",
    "\n",
    "# Set the model back to training mode\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675f0163-49ba-412d-b414-ea8ca7df4eee",
   "metadata": {},
   "source": [
    "## Calculating accuracy using torchmetrics\n",
    "\n",
    "#### In addition to the losses, you should also be keeping track of the accuracy during training. By doing so, you will be able to select the epoch when the model performed the best.\n",
    "\n",
    "#### In this exercise, you will practice using the torchmetrics package to calculate the accuracy. You will be using a sample of the facemask dataset. This dataset contains three different classes. The plot_errors function will display samples where the model predictions do not match the ground truth. Performing such error analysis will help you understand your model failure modes.\n",
    "\n",
    "#### The torchmetrics package is already imported. The model outputs are the probabilities returned by a softmax as the last step of the model. The labels tensor contains the labels as one-hot encoded vectors.\n",
    "\n",
    "## Instructions 1/2\n",
    "-    Create an accuracy metric for a \"multiclass\" problem with three classes.\n",
    "-    Calculate the accuracy for each batch of the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56516297-5acb-4de9-ad16-c86bc6ef07ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create accuracy metric using torch metrics\n",
    "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3)\n",
    "for data in dataloader:\n",
    "    features, labels = data\n",
    "    outputs = model(features)\n",
    "    \n",
    "    # Calculate accuracy over the batch\n",
    "    acc = metric(outputs, labels.argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a563c8-c2f1-4d77-a879-77843231d260",
   "metadata": {},
   "source": [
    "## # Create accuracy metric using torch metrics\n",
    "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3)\n",
    "for data in dataloader:\n",
    "    features, labels = data\n",
    "    outputs = model(features)\n",
    "    \n",
    "    # Calculate accuracy over the batch\n",
    "    acc = metric(outputs.softmax(dim=-1), labels.argmax(dim=-1))\n",
    "    \n",
    "# Calculate accuracy over the whole epoch\n",
    "acc = metric.compute()\n",
    "\n",
    "# Reset the metric for the next epoch (training or validation)\n",
    "metric.reset()\n",
    "plot_errors(model, dataloader)Instructions 2/2\n",
    "-    Calculate accuracy for the epoch.\n",
    "-    Reset the metric for the next epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c93b200-c2a1-43f4-a283-917328900ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_errors(model, dataloader): \n",
    "    # find mismatches\n",
    "    mismatches = []\n",
    "    for data in dataloader:\n",
    "        if len(mismatches) > 8:\n",
    "            break\n",
    "        features, labels = data\n",
    "        outputs = model(features)\n",
    "        gt = labels.argmax(-1)\n",
    "        pred = outputs.argmax(-1)\n",
    "        for f, g, p in zip(features, gt, pred):\n",
    "            if g != p:\n",
    "                mismatches.append((f, g, p))\n",
    "    \n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                     nrows_ncols=(2, 4),  # creates 2x2 grid of axes\n",
    "                     axes_pad=0.5,  # pad between axes in inch.\n",
    "                     )\n",
    "    mapping = {0: 'No mask', 1: 'Mask', 2: 'Incorrect'}\n",
    "    for idx, ax in enumerate(grid):\n",
    "        ax.imshow(mismatches[idx][0].permute(1, 2, 0))\n",
    "        ax.set_title(f'GT: {mapping[mismatches[idx][1].item()]} \\n PRED: {mapping[mismatches[idx][2].item()]}')\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81460ba3-65c8-4659-9873-9f6c6410a4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create accuracy metric using torch metrics\n",
    "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3)\n",
    "for data in dataloader:\n",
    "    features, labels = data\n",
    "    outputs = model(features)\n",
    "    \n",
    "    # Calculate accuracy over the batch\n",
    "    acc = metric(outputs.softmax(dim=-1), labels.argmax(dim=-1))\n",
    "    \n",
    "# Calculate accuracy over the whole epoch\n",
    "acc = metric.compute()\n",
    "\n",
    "# Reset the metric for the next epoch (training or validation)\n",
    "metric.reset()\n",
    "plot_errors(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5732a4-8d84-4847-b8b0-4a79586b75b1",
   "metadata": {},
   "source": [
    "# Create a small neural network\n",
    "model = nn.Sequential(nn.Linear(3072, 16),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout())\n",
    "model(input_tensor)## Experimenting with dropout\n",
    "\n",
    "#### The dropout layer randomly zeroes out elements of the input tensor. Doing so helps fight overfitting. In this exercise, you'll create a small neural network with at least two linear layers, two dropout layers, and two activation functions.\n",
    "\n",
    "#### The torch.nn package has already been imported as nn. An input_tensor of dimensions 1*3072 has been created for you.\n",
    "\n",
    "## Instructions 1/2\n",
    "-        Create a small neural network with one linear layer, one ReLU function, and one dropout layer, in that order.\n",
    "-        The model should take input_tensor as input and return an output of size 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9873690-6785-4bc9-873a-c7c1691de0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small neural network\n",
    "model = nn.Sequential(nn.Linear(3072, 16),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout())\n",
    "model(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d0ba2-819c-4654-a8ef-cee60b933d5b",
   "metadata": {},
   "source": [
    "## Instructions 2/2\n",
    "-        Using the same neural network, set the probability of zeroing out elements in the dropout layer to 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab6fa18-433e-4342-9b3d-bfc14ad3ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the same model, set the dropout probability to 0.8\n",
    "model = nn.Sequential(nn.Linear(3072, 16),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(p=0.8))\n",
    "model(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd548d32-b303-4f4a-9cd5-54ae130b5d0d",
   "metadata": {},
   "source": [
    "## Implementing random search\n",
    "\n",
    "#### Hyperparameter search is a computationally costly approach to experiment with different hyperparameter values. However, it can lead to performance improvements. In this exercise, you will implement a random search algorithm.\n",
    "\n",
    "#### You will randomly sample 10 values of the learning rate and momentum from the uniform distribution. To do so, you will use the np.random.uniform() function.\n",
    "\n",
    "#### The numpy package has already been imported as np.\n",
    "\n",
    "## Instructions\n",
    "-    Randomly sample a learning rate factor between 2 and 4 so that the learning rate (lr) is bounded between 10^-2 and 10^-4.\n",
    "-    Randomly sample a momentum between 0.85 and 0.99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af938813-2351-46d3-abae-f3b2c8b68592",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "for idx in range(10):\n",
    "    # Randomly sample a learning rate factor 2 and 4\n",
    "    factor = np.random.uniform(2, 4)\n",
    "    lr = 10 ** -factor\n",
    "    \n",
    "    # Randomly sample a momentum between 0.85 and 0.99\n",
    "    momentum = np.random.uniform(0.85, 0.99)\n",
    "    \n",
    "    values.append((lr, momentum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0afcf79-41a8-47b0-9b2e-60045b17ed49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
